{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koheikobayashi/machine-learning/blob/main/%E5%8A%B9%E6%9E%9C%E6%A4%9C%E8%A8%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "692cdbcf",
      "metadata": {
        "id": "692cdbcf"
      },
      "source": [
        "## 目次\n",
        "- [A/B テスト](#scrollTo=A%2FB+%E3%83%86%E3%82%B9%E3%83%88)\n",
        "- [差分の差法（DID）](#scrollTo=%E5%B7%AE%E5%88%86%E3%81%AE%E5%B7%AE%E6%B3%95%EF%BC%88DID%EF%BC%89)\n",
        "- [回帰不連続デザイン（RDD）](#scrollTo=%E5%9B%9E%E5%B8%B0%E4%B8%8D%E9%80%A3%E7%B6%9A%E3%83%87%E3%82%B6%E3%82%A4%E3%83%B3%EF%BC%88RDD%EF%BC%89)\n",
        "- [反実仮想](#scrollTo=%E5%8F%8D%E5%AE%9F%E4%BB%AE%E6%83%B3)\n",
        "- [Feedback Loop](#scrollTo=Feedback+Loop)\n",
        "- [平均処置効果](#scrollTo=%E5%B9%B3%E5%9D%87%E5%87%A6%E7%BD%AE%E5%8A%B9%E6%9E%9C)\n",
        "- [期待値](#scrollTo=%E6%9C%9F%E5%BE%85%E5%80%A4)\n",
        "- [条件付き期待値](#scrollTo=%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E6%9C%9F%E5%BE%85%E5%80%A4)\n",
        "- [A/A テスト](#scrollTo=A%2FA+%E3%83%86%E3%82%B9%E3%83%88)\n",
        "- [統計的仮説検証](#scrollTo=%E7%B5%B1%E8%A8%88%E7%9A%84%E4%BB%AE%E8%AA%AC%E6%A4%9C%E8%A8%BC)\n",
        "- [回帰分析](#scrollTo=%E5%9B%9E%E5%B8%B0%E5%88%86%E6%9E%90)\n",
        "- [誤差項](#scrollTo=%E8%AA%A4%E5%B7%AE%E9%A0%85)\n",
        "- [標準誤差](#scrollTo=%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE)\n",
        "- [帰無仮説](#scrollTo=%E5%B8%B0%E7%84%A1%E4%BB%AE%E8%AA%AC)\n",
        "- [仮説検定](#scrollTo=%E4%BB%AE%E8%AA%AC%E6%A4%9C%E5%AE%9A)\n",
        "- [p値](#scrollTo=p%E5%80%A4)\n",
        "- [統計的に非有意](#scrollTo=%E7%B5%B1%E8%A8%88%E7%9A%84%E3%81%AB%E9%9D%9E%E6%9C%89%E6%84%8F)\n",
        "- [有意水準](#scrollTo=%E6%9C%89%E6%84%8F%E6%B0%B4%E6%BA%96)\n",
        "- [信頼区間](#scrollTo=%E4%BF%A1%E9%A0%BC%E5%8C%BA%E9%96%93)\n",
        "- [共変量](#scrollTo=%E5%85%B1%E5%A4%89%E9%87%8F)\n",
        "- [バランステスト](#scrollTo=%E3%83%90%E3%83%A9%E3%83%B3%E3%82%B9%E3%83%86%E3%82%B9%E3%83%88)\n",
        "- [t値](#scrollTo=t%E5%80%A4)\n",
        "- [p-hacking](#scrollTo=p-hacking)\n",
        "- [クラスターA/Bテスト](#scrollTo=%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BCA%2FB%E3%83%86%E3%82%B9%E3%83%88)\n",
        "- [チェリーピッキング](#scrollTo=%E3%83%81%E3%82%A7%E3%83%AA%E3%83%BC%E3%83%94%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0)\n",
        "- [コルモゴロフ-スミルノフ検定](#scrollTo=%E3%82%B3%E3%83%AB%E3%83%A2%E3%82%B4%E3%83%AD%E3%83%95-%E3%82%B9%E3%83%9F%E3%83%AB%E3%83%8E%E3%83%95%E6%A4%9C%E5%AE%9A)\n",
        "- [層化 A/B テスト](#scrollTo=%E5%B1%A4%E5%8C%96+A%2FB+%E3%83%86%E3%82%B9%E3%83%88)\n",
        "- [クラスター頑健標準誤差](#scrollTo=%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BC%E9%A0%91%E5%81%A5%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE)\n",
        "- [局所平均処置効果（LATE）](#scrollTo=%E5%B1%80%E6%89%80%E5%B9%B3%E5%9D%87%E5%87%A6%E7%BD%AE%E5%8A%B9%E6%9E%9C%EF%BC%88LATE%EF%BC%89)\n",
        "- [交差項](#scrollTo=%E4%BA%A4%E5%B7%AE%E9%A0%85)\n",
        "- [rdrobustライブラリ](#scrollTo=rdrobust%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA)\n",
        "- [アウトカム](#scrollTo=%E3%82%A2%E3%82%A6%E3%83%88%E3%82%AB%E3%83%A0)\n",
        "- [固定効果モデル](#scrollTo=%E5%9B%BA%E5%AE%9A%E5%8A%B9%E6%9E%9C%E3%83%A2%E3%83%87%E3%83%AB)\n",
        "- [パラレルトレンド仮定](#scrollTo=%E3%83%91%E3%83%A9%E3%83%AC%E3%83%AB%E3%83%88%E3%83%AC%E3%83%B3%E3%83%89%E4%BB%AE%E5%AE%9A)\n",
        "- [McCrary の検定](#scrollTo=McCrary+%E3%81%AE%E6%A4%9C%E5%AE%9A)\n",
        "- [Sharp RDD](#scrollTo=Sharp+RDD)\n",
        "- [Fuzzy RDD](#scrollTo=Fuzzy+RDD)\n",
        "- [as-if 条件](#scrollTo=as-if+%E6%9D%A1%E4%BB%B6)\n",
        "- [密度関数](#scrollTo=%E5%AF%86%E5%BA%A6%E9%96%A2%E6%95%B0)\n",
        "- [diagnostic tests](#scrollTo=diagnostic+tests)\n",
        "- [処置確率](#scrollTo=%E5%87%A6%E7%BD%AE%E7%A2%BA%E7%8E%87)\n",
        "- [右側極限](#scrollTo=%E5%8F%B3%E5%81%B4%E6%A5%B5%E9%99%90)\n",
        "- [左側極限](#scrollTo=%E5%B7%A6%E5%81%B4%E6%A5%B5%E9%99%90)\n",
        "- [局所多項式回帰](#scrollTo=%E5%B1%80%E6%89%80%E5%A4%9A%E9%A0%85%E5%BC%8F%E5%9B%9E%E5%B8%B0)\n",
        "- [局所線形回帰](#scrollTo=%E5%B1%80%E6%89%80%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0)\n",
        "- [線形モデル](#scrollTo=%E7%B7%9A%E5%BD%A2%E3%83%A2%E3%83%87%E3%83%AB)\n",
        "- [バリアンス](#scrollTo=%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9)\n",
        "- [潜在スコア](#scrollTo=%E6%BD%9C%E5%9C%A8%E3%82%B9%E3%82%B3%E3%82%A2)\n",
        "- [共変量のバランステスト](#scrollTo=%E5%85%B1%E5%A4%89%E9%87%8F%E3%81%AE%E3%83%90%E3%83%A9%E3%83%B3%E3%82%B9%E3%83%86%E3%82%B9%E3%83%88)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f62c9946",
      "metadata": {
        "id": "f62c9946"
      },
      "source": [
        "<h1 id=\"A%2FB+%E3%83%86%E3%82%B9%E3%83%88\">A/B テスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945124dc",
      "metadata": {
        "id": "945124dc"
      },
      "source": [
        "### A/Bテストの解説\n",
        "\n",
        "\n",
        " A/Bテストは、2つのグループ（A, B）間での効果を比較するための統計的手法です。ウェブサイトのデザイン変更や広告キャンペーンの効果を測定する際によく使用されます。一般的に、片方のグループ（Aグループ）がコントロール群、もう片方のグループ（Bグループ）がテスト群です。\n",
        "\n",
        "\n",
        " #### A/Bテストと数式\n",
        "\n",
        "\n",
        " A/Bテストの効果を数式で表現する方法の一つは、2つのグループの平均の差を比較することです。ここで使用するのが**差の推定**です。数式で表現すると以下のようになります：\n",
        "\n",
        "\n",
        " $$ D = ar{X}_B - ar{X}_A $$\n",
        "\n",
        "\n",
        " - $\\bar{X}_A$: Aグループの平均値\n",
        " - $\\bar{X}_B$: Bグループの平均値\n",
        "\n",
        "\n",
        " また、この差が統計的に有意であるかを確認するために、**標準誤差**と**t検定**を使用します。\n",
        "\n",
        "\n",
        " $$ SE = \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}} $$\n",
        "\n",
        "\n",
        " - $SE$: 標準誤差\n",
        " - $s_A$: Aグループの標準偏差\n",
        " - $s_B$: Bグループの標準偏差\n",
        " - $n_A$: Aグループのサンプルサイズ\n",
        " - $n_B$: Bグループのサンプルサイズ\n",
        "\n",
        "\n",
        " そしてt値は次のように計算されます。\n",
        "\n",
        "\n",
        " $$ t = \\frac{D}{SE} $$\n",
        "\n",
        "\n",
        " #### Pythonでの効果検証\n",
        "\n",
        "\n",
        " Pythonを用いたA/Bテストの解析は『Pythonで学ぶ効果検証入門』で詳しく説明されています。この書籍では、上記で述べた統計的手法をPythonを用いて実装する過程が紹介されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb1bcd8",
      "metadata": {
        "id": "ceb1bcd8"
      },
      "outputs": [],
      "source": [
        "# PythonでA/Bテストを実施するコード\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # サンプルデータとしてA, Bグループの観測データ\n",
        " A_data = np.array([14, 15, 15, 13, 12, 14, 16])\n",
        " B_data = np.array([16, 18, 19, 17, 18, 16, 20])\n",
        "\n",
        "\n",
        " # Aグループの平均値を計算\n",
        " mean_A = np.mean(A_data)\n",
        " print('Aグループの平均値:', mean_A)\n",
        "\n",
        "\n",
        " # Bグループの平均値を計算\n",
        " mean_B = np.mean(B_data)\n",
        " print('Bグループの平均値:', mean_B)\n",
        "\n",
        "\n",
        " # AグループとBグループの平均の差\n",
        " D = mean_B - mean_A\n",
        " print('平均の差:', D)\n",
        "\n",
        "\n",
        " # Aグループの標準偏差を計算\n",
        " std_A = np.std(A_data, ddof=1)\n",
        " print('Aグループの標準偏差:', std_A)\n",
        "\n",
        "\n",
        " # Bグループの標準偏差を計算\n",
        " std_B = np.std(B_data, ddof=1)\n",
        " print('Bグループの標準偏差:', std_B)\n",
        "\n",
        "\n",
        " # A/Bテストの標準誤差を計算\n",
        " SE = np.sqrt(std_A**2/len(A_data) + std_B**2/len(B_data))\n",
        " print('標準誤差:', SE)\n",
        "\n",
        "\n",
        " # t値を計算\n",
        " t_value = D / SE\n",
        " print('t値:', t_value)\n",
        "\n",
        "\n",
        " # t検定を実行しp値を得る\n",
        " _, p_value = stats.ttest_ind(A_data, B_data)\n",
        " print('p値:', p_value)\n",
        "\n",
        "\n",
        " # p値が0.05未満であれば有意差があると判断\n",
        " if p_value < 0.05:\n",
        "  print('結果: 有意差あり')\n",
        " else:\n",
        "  print('結果: 有意差なし')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aaf468",
      "metadata": {
        "id": "98aaf468"
      },
      "source": [
        "<h1 id=\"%E5%B7%AE%E5%88%86%E3%81%AE%E5%B7%AE%E6%B3%95%EF%BC%88DID%EF%BC%89\">差分の差法（DID）</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89824646",
      "metadata": {
        "id": "89824646"
      },
      "source": [
        "差分の差法（Difference-in-Differences, DID）は、処置群と対照群の時間的変化を比較することで、処置の効果を推定する手法です。この手法は、特定の政策や介入が導入される前後での処置群と対照群の平均的な変化を比較することで、その介入の平均的な効果を推定します。\n",
        "\n",
        "\n",
        " DIDの数式は以下のように表現されます：\n",
        "\n",
        "\n",
        " $$ \\text{DID} = (\\bar{Y}_{T1} - \\bar{Y}_{T0}) - (\\bar{Y}_{C1} - \\bar{Y}_{C0}) $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $\\bar{Y}_{T1}$ は、処置群の介入後の平均的なアウトカム。\n",
        " - $\\bar{Y}_{T0}$ は、処置群の介入前の平均的なアウトカム。\n",
        " - $\\bar{Y}_{C1}$ は、対照群の介入後の平均的なアウトカム。\n",
        " - $\\bar{Y}_{C0}$ は、対照群の介入前の平均的なアウトカム。\n",
        "\n",
        "\n",
        " この数式の意味するところは、まず処置群での変化を計算し、次に対照群での変化を計算し、その差を取ることで介入の純粋な効果を求めるというものです。これにより、共通の時間的な影響を取り除きます。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、このDIDを計算する方法と考え方を詳細に解説しています。この手法は、政策評価や社会科学の実証研究で広く使用されています。特に、ランダム化実験が難しい場合や、自然実験のデータを用いる場合に有効です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51426f24",
      "metadata": {
        "id": "51426f24"
      },
      "outputs": [],
      "source": [
        "# データを仮定（実際のデータはサンプルやデータセットとして用意されるべき）\n",
        " import pandas as pd\n",
        "\n",
        "\n",
        " data = pd.DataFrame({\n",
        "  'group': ['treatment'] * 4 + ['control'] * 4,\n",
        "  'time': ['pre', 'post', 'pre', 'post', 'pre', 'post', 'pre', 'post'],\n",
        "  'outcome': [5, 9, 7, 12, 4, 5, 6, 6]\n",
        " })\n",
        "\n",
        "\n",
        " # データを表示して確認\n",
        " print(data)\n",
        "\n",
        "\n",
        " # 処置群と対照群ごとの平均を計算して表示\n",
        " pre_treatment_mean = data[(data['group'] == 'treatment') & (data['time'] == 'pre')]['outcome'].mean()\n",
        " print('Pre-treatment mean:', pre_treatment_mean)\n",
        "\n",
        "\n",
        " post_treatment_mean = data[(data['group'] == 'treatment') & (data['time'] == 'post')]['outcome'].mean()\n",
        " print('Post-treatment mean:', post_treatment_mean)\n",
        "\n",
        "\n",
        " pre_control_mean = data[(data['group'] == 'control') & (data['time'] == 'pre')]['outcome'].mean()\n",
        " print('Pre-control mean:', pre_control_mean)\n",
        "\n",
        "\n",
        " post_control_mean = data[(data['group'] == 'control') & (data['time'] == 'post')]['outcome'].mean()\n",
        " print('Post-control mean:', post_control_mean)\n",
        "\n",
        "\n",
        " # DIDの計算\n",
        " DID = (post_treatment_mean - pre_treatment_mean) - (post_control_mean - pre_control_mean)\n",
        " print('Difference-in-Differences:', DID)\n",
        "\n",
        "\n",
        " # 各部分を計算する目的と役割を示すコメント\n",
        " # 処置群の事前と事後の平均差を取り、処置の変化を測定\n",
        " # 対照群の事前と事後の平均差を取り、他の要因での自然な変化を測定\n",
        " # 両者の差を取ることで、処置の純粋な効果を推定"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93a418f",
      "metadata": {
        "id": "f93a418f"
      },
      "source": [
        "<h1 id=\"%E5%9B%9E%E5%B8%B0%E4%B8%8D%E9%80%A3%E7%B6%9A%E3%83%87%E3%82%B6%E3%82%A4%E3%83%B3%EF%BC%88RDD%EF%BC%89\">回帰不連続デザイン（RDD）</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c026b78",
      "metadata": {
        "id": "7c026b78"
      },
      "source": [
        "回帰不連続デザイン（RDD）は、切断値を境にして回帰線が不連続に変化する状況を分析するための手法です。特に、政策変更やプログラムの影響を評価するときに有効です。数式で表現すると、次のようになります。\n",
        "\n",
        "\n",
        " $$\n",
        " Y_i = \\alpha + \\tau D_i + f(X_i) + \\epsilon_i\n",
        " $$\n",
        "\n",
        "\n",
        " この数式について解説します。\n",
        "\n",
        "\n",
        " - $Y_i$: 被説明変数。例えば、テストの結果や健康指標など、我々が興味を持っているアウトカムです。\n",
        " - $\\alpha$: 切片項。モデルの基礎的な値。\n",
        " - $\\tau$: 介入の効果を示すパラメータ。具体的に知りたいポリシーや措置の効果を表します。\n",
        " - $D_i$: ダミー変数。ある基準点（例えば、テストの点数が通過不通過の境界を超えたかどうか）を超えたかどうかを示します。\n",
        " - $f(X_i)$: 説明変数$X_i$に基づく回帰関数。通常は切断点の前後で異なる傾きを持つ線形関数が用いられます。\n",
        " - $\\epsilon_i$: 誤差項。モデルが説明できないその他の要因を含みます。\n",
        "\n",
        "\n",
        " 'Pythonで学ぶ効果検証入門'は、このような数式をPythonを使って具体的にどのように計算・分析できるかを例示しています。RDDは、治療割り当てがランダムではない場合でも因果関係の推定を可能にし、特に教育、政治、経済などの社会科学の分野で広く使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84b94ed",
      "metadata": {
        "id": "f84b94ed"
      },
      "outputs": [],
      "source": [
        "# Pythonで回帰不連続デザインを実装する例です。\n",
        "\n",
        "\n",
        " import numpy as np # 数値計算用のライブラリをインポート\n",
        " import statsmodels.api as sm # 回帰分析用のstatsmodelsライブラリをインポート\n",
        "\n",
        "\n",
        " # ダミーデータの生成\n",
        " np.random.seed(0) # 乱数の種を固定して毎回同じ結果を得る\n",
        " X = np.linspace(-10, 10, 100) # 介入変数の生成\n",
        " D = (X >= 0).astype(float) # 切断点でのダミー変数生成\n",
        "\n",
        "\n",
        " # 真の効果(τ)を設定しデータ生成\n",
        " tau = 5 # 真の効果\n",
        " Y = 2 + tau * D + 0.5 * X + np.random.normal(size=X.shape) # 被説明変数の生成\n",
        "\n",
        "\n",
        " # データの確認\n",
        " print('X:', X) # 介入変数の確認\n",
        " print('D:', D) # ダミー変数の確認\n",
        " print('Y:', Y) # 被説明変数の確認\n",
        "\n",
        "\n",
        " # 回帰モデルの設定\n",
        " X_design = sm.add_constant(np.column_stack((D, X))) # 切片項と説明変数を含むデザイン行列を作成\n",
        " model = sm.OLS(Y, X_design) # OLS(最小二乗法)を使ってモデルを指定\n",
        " results = model.fit() # モデルのフィッティング（学習）\n",
        "\n",
        "\n",
        " # 結果の表示\n",
        " print(results.summary()) # 結果概要を出力し回帰係数等を確認"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50a904c",
      "metadata": {
        "id": "e50a904c"
      },
      "source": [
        "<h1 id=\"%E5%8F%8D%E5%AE%9F%E4%BB%AE%E6%83%B3\">反実仮想</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96558dac",
      "metadata": {
        "id": "96558dac"
      },
      "source": [
        "### 反実仮想とは\n",
        "\n",
        "\n",
        " 反実仮想（Counterfactual）は、実際に起こった出来事とは異なる状況を考える概念です。この概念は主に因果推論や経済学、社会科学の分野で使用され、特定の介入や処置がなかった場合の結果を比較分析するために用いられます。\n",
        "\n",
        "\n",
        " 例えば、ある薬を投与された患者の回復状況が得られたとします。反実仮想分析は、その薬が投与されなかったとしたらその患者がどうなっていたかを推測することに関心があります。\n",
        "\n",
        "\n",
        " #### 数式による表現\n",
        " 反実仮想は数学的には以下のように表現されます。\n",
        "\n",
        "\n",
        " $$Y_i(1) - Y_i(0)$$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $Y_i(1)$：個体 $i$ が介入を受けた場合の結果\n",
        " - $Y_i(0)$：個体 $i$ が介入を受けなかった場合の結果\n",
        "\n",
        "\n",
        " つまり、上記の数式は介入の効果を計るための基本的な式で、実際に介入を受けた結果と受けなかった場合の仮想的な結果との差を示しています。\n",
        "\n",
        "\n",
        " #### Pythonで学ぶ効果検証入門との関係性\n",
        " 『Pythonで学ぶ効果検証入門』では、反実仮想を用いた効果検証手法について具体的にPythonを用いて学ぶことができます。因果推論の基礎をPythonのライブラリを使いながら検証するプロセスが詳しく解説されているため、教材として非常に有用です。\n",
        "\n",
        "\n",
        " #### 使用用途\n",
        " 反実仮想はビジネスにおいて政策の効果を測定したり、マーケティング施策の評価を行う際に重要な役割を果たします。現実に観測できない結果を推測することで、最適な意思決定を支援するための強力なツールとなります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d3622d",
      "metadata": {
        "id": "00d3622d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        " # 実際の結果と反実仮想の結果を生成\n",
        " actual_outcomes = np.array([1, 0, 1, 1, 0]) # 例として介入を受けた結果\n",
        " counterfactual_outcomes = np.array([0, 0, 0, 1, 0]) # 例として介入を受けなかった場合の仮定結果\n",
        "\n",
        "\n",
        " # 介入の効果を計算\n",
        " intervention_effects = actual_outcomes - counterfactual_outcomes\n",
        "\n",
        "\n",
        " # 介入の効果の配列を出力\n",
        " print(\"Intervention Effects:\", intervention_effects)\n",
        "\n",
        "\n",
        " # 解説:\n",
        " # 実際の結果と反実仮想の結果の計算\n",
        " # 指定した患者ごとに介入の効果を計算\n",
        " # 各患者について、実際の結果 - 反実仮想の結果 を計算\n",
        " # 結果を出力するために print を使用"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e945bd0",
      "metadata": {
        "id": "4e945bd0"
      },
      "source": [
        "<h1 id=\"Feedback+Loop\">Feedback Loop</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a192fd0",
      "metadata": {
        "id": "4a192fd0"
      },
      "source": [
        "フィードバックループ（Feedback Loop）は、システムやプロセスにおける出力が再び入力としてフィードバックされるメカニズムを指します。これにより、システムは外部からの影響に応じて動的に応答し、適応することが可能になります。フィードバックループは、特に制御システムや機械学習のトレーニングにおいて重要な役割を果たします。\n",
        "\n",
        "\n",
        " フィードバックループが数式で表現される場合、以下のような全体的な形で示されます：\n",
        "\n",
        "\n",
        " $$ y(t) = G(u(t) + H(y(t))) $$\n",
        "\n",
        "\n",
        " この数式では、\n",
        " - $y(t)$ はフィードバックシステムの出力を示します。\n",
        " - $u(t)$ はシステムへの入力を表します。\n",
        " - $G$ はシステムの伝達関数や応答関数です。\n",
        " - $H$ は出力から入力へのフィードバックの効果をモデル化します。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門においては、このようなフィードバックループを使って、データの自己改善や制御理論の適用を試みます。実際の応用例としては、モデルの自動調整や最適化、システム制御の最適化があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7af05a4",
      "metadata": {
        "id": "b7af05a4"
      },
      "outputs": [],
      "source": [
        "def feedback_loop(initial_input, G, H, iterations=10):\n",
        "  \"\"\"\n",
        "  フィードバックループをシミュレートする関数。\n",
        "\n",
        "\n",
        "  initial_input: 初期入力値\n",
        "  G: システムの伝達関数\n",
        "  H: フィードバックの影響を示す関数\n",
        "  iterations: フィードバックの繰り返し回数\n",
        "  \"\"\"\n",
        "  y = initial_input # 初期状態としての出力（初期入力を使用）\n",
        "  print(f'初期入力 y: {y}') # 初期入力の確認\n",
        "\n",
        "\n",
        "  for i in range(iterations):\n",
        "  feedback = H(y) # 現在の出力に基づくフィードバックを計算\n",
        "  print(f'フィードバック feedback: {feedback}') # フィードバックの確認\n",
        "\n",
        "\n",
        "  y = G(initial_input + feedback) # フィードバックを反映した新しい出力を計算\n",
        "  print(f'新しい出力 y: {y}') # 各ステップの出力の確認\n",
        "\n",
        "\n",
        "  return y\n",
        "\n",
        "\n",
        " # シンプルな例として、GとHを線形関数として定義\n",
        " G = lambda x: 2 * x # 伝達関数 G\n",
        " H = lambda y: 0.5 * y # フィードバックの関数 H\n",
        "\n",
        "\n",
        " # 初期入力\n",
        " initial_input = 1\n",
        "\n",
        "\n",
        " # フィードバックループの実行\n",
        " final_output = feedback_loop(initial_input, G, H, iterations=5)\n",
        " print(f'最終出力 final_output: {final_output}') # 最終出力の確認"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83da69d6",
      "metadata": {
        "id": "83da69d6"
      },
      "source": [
        "<h1 id=\"%E5%B9%B3%E5%9D%87%E5%87%A6%E7%BD%AE%E5%8A%B9%E6%9E%9C\">平均処置効果</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c14441",
      "metadata": {
        "id": "76c14441"
      },
      "source": [
        "### 平均処置効果（ATE: Average Treatment Effect）の解説\n",
        "\n",
        "\n",
        " 平均処置効果は、集団における処置（例えば、薬や政策）の平均的な効果を評価するための指標です。平均処置効果は、数式で以下のように表現されます：\n",
        "\n",
        "\n",
        " $$ ATE = E[Y(1)] - E[Y(0)] $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $E[Y(1)]$ は処置を受けた場合の期待結果（平均的な結果）です。\n",
        " - $E[Y(0)]$ は処置を受けなかった場合の期待結果です。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、この数式を基に処置の効果をシミュレーションや実データを用いて実証的に分析する方法が紹介されています。平均処置効果は、医薬品の評価から経済政策の効果評価まで、幅広い分野で施策の有効性を評価するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2aacd5",
      "metadata": {
        "id": "ff2aacd5"
      },
      "outputs": [],
      "source": [
        "# 平均処置効果（ATE）を計算するPythonコード\n",
        "\n",
        "\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 処置を受けた群と受けなかった群のアウトカム（結果）を示すデータを用意\n",
        " # 例として、処置を受けた群の平均と処置を受けなかった群の平均を用いる\n",
        " outcomes_treated = [5, 6, 7, 8, 9] # 処置を受けたケースのアウトカム\n",
        " outcomes_control = [3, 4, 5, 3, 4] # 処置を受けていないケースのアウトカム\n",
        "\n",
        "\n",
        " # 処置を受けた場合の平均アウトカムを計算\n",
        " mean_treated = np.mean(outcomes_treated)\n",
        " print(f\"Mean outcome for treated group: {mean_treated}\") # 期待結果E[Y(1)]の計算結果を表示\n",
        "\n",
        "\n",
        " # 処置を受けなかった場合の平均アウトカムを計算\n",
        " mean_control = np.mean(outcomes_control)\n",
        " print(f\"Mean outcome for control group: {mean_control}\") # 期待結果E[Y(0)]の計算結果を表示\n",
        "\n",
        "\n",
        " # 平均処置効果を計算\n",
        " ATE = mean_treated - mean_control\n",
        " print(f\"Average Treatment Effect (ATE): {ATE}\") # 平均処置効果ATEの計算結果を表示"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8765cc8c",
      "metadata": {
        "id": "8765cc8c"
      },
      "source": [
        "<h1 id=\"%E6%9C%9F%E5%BE%85%E5%80%A4\">期待値</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529691f0",
      "metadata": {
        "id": "529691f0"
      },
      "source": [
        "期待値（Expected Value）とは、ランダム変数が取る値の加重平均を表します。数学的には、期待値は多くの場合、未来の平均的な結果を予測するために使われます。例えば、サイコロを振る場合の期待値は、すべての出る目の平均値を表します。期待値は次のような数式で表すことができます：\n",
        "\n",
        "\n",
        " $$E(X) = \\sum_{i=1}^{n} x_i \\cdot P(x_i)$$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $E(X)$ は変数 $X$ の期待値を表します。\n",
        " - $x_i$ はサンプル空間内の値であり、具体的な結果を指します。\n",
        " - $P(x_i)$ は $x_i$ が発生する確率を示します。\n",
        "\n",
        "\n",
        " この式は、各結果 $x_i$ にその発生確率 $P(x_i)$ を掛け、その総和を取ることで期待値を計算します。\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』では、この期待値を用いて、さまざまな状況下での予測や評価、比較を行います。期待値はプロダクトやサービス、実験などにおける効果を検証するために使われ、ビジネス決定やデータ分析の重要な要素です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf6e60f",
      "metadata": {
        "id": "4bf6e60f"
      },
      "outputs": [],
      "source": [
        "# 期待値を計算するためのPythonコード例\n",
        " # サイコロを振る例を想定します\n",
        "\n",
        "\n",
        " # サイコロの出る目（1から6まで）\n",
        " values = [1, 2, 3, 4, 5, 6]\n",
        " # 各目の出る確率（公平なサイコロの場合、それぞれの目が出る確率は1/6）\n",
        " probabilities = [1/6] * 6\n",
        "\n",
        "\n",
        " # 期待値を計算する\n",
        " expected_value = sum(value * probability for value, probability in zip(values, probabilities))\n",
        "\n",
        "\n",
        " # 計算された期待値を出力\n",
        " print(expected_value) # 出力は3.5、サイコロの期待値は平均して3.5\n",
        "\n",
        "\n",
        " # このコードでは、目とその出る確率を掛け合わせたものを全て足し合わせ (合計) 期待値を求めます。\n",
        " # 公平なサイコロを前提としているため、すべての目の出る確率は1/6です。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104b843c",
      "metadata": {
        "id": "104b843c"
      },
      "source": [
        "<h1 id=\"%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E6%9C%9F%E5%BE%85%E5%80%A4\">条件付き期待値</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3995b251",
      "metadata": {
        "id": "3995b251"
      },
      "source": [
        "### 条件付き期待値についての解説\n",
        "\n",
        "\n",
        " 条件付き期待値は、ある事象が既に起きているという条件のもとでの期待値を求める概念です。例えば、確率変数 $X$ が与えられるときに、 $Y$ の期待値を条件付き期待値 $E[Y | X]$ で表します。条件付き期待値は次のようなブロック数式で表すことができます：\n",
        "\n",
        "\n",
        " $$E[Y | X] = \\int y \\, f_{Y|X}(y|x) \\, dy$$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $E[Y | X]$ は、確率変数 $Y$ の $X$ の条件下での期待値です。\n",
        " - $\\int$ は積分記号で、積分を表します。\n",
        " - $y$ は、$Y$ の実現値を表します。\n",
        " - $f_{Y|X}(y|x)$ は $X=x$ の条件下での $Y$ の条件付き確率密度関数です。\n",
        " - $dy$ は $y$ に関する微小な変化を表し、積分の変数です。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』では、この条件付き期待値を利用して、介入効果の推定や因果推論を行います。特に、条件付き期待値を用いて治療効果の評価やデータの傾向を分析するためのツールとして紹介されます。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        "\n",
        "\n",
        " 条件付き期待値は、様々な分野で幅広く使用されます。例えば、統計学、機械学習、経済学などで、異なる状況下での分布や期待値を理解するための基礎です。特に、予測モデルの性能評価や不確実さを考慮した意思決定において重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae9ca01",
      "metadata": {
        "id": "bae9ca01"
      },
      "outputs": [],
      "source": [
        "# Pythonで条件付き期待値を計算する例\n",
        " # 必要なライブラリをインポートします\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # XとYの条件付き分布を模擬するデータを生成します\n",
        " # ここでは、正規分布 N(0, 1) からのランダムサンプルを使用します\n",
        " X = np.random.normal(0, 1, 1000)\n",
        " Y = 2 * X + np.random.normal(0, 1, 1000)\n",
        "\n",
        "\n",
        " # XとYのデータを出力します\n",
        " print(f'Xのデータ: {X[:5]}') # Xの最初の5つのデータを表示\n",
        " print(f'Yのデータ: {Y[:5]}') # Yの最初の5つのデータを表示\n",
        "\n",
        "\n",
        " # 条件付き期待値を計算する関数を定義します\n",
        " def conditional_expectation(X, Y):\n",
        "  # Xの一意の値のリストを作成します\n",
        "  unique_X = np.unique(X)\n",
        "\n",
        "  # 結果を保存するための辞書を作成します\n",
        "  cond_exp = {}\n",
        "\n",
        "  # unique_Xそれぞれについて条件付き期待値を計算します\n",
        "  for x in unique_X:\n",
        "  # Xがxに等しいインデックスを取得します\n",
        "  indices = (X == x)\n",
        "\n",
        "  # XがxのときのYのサンプルに対して平均を取ります\n",
        "  cond_exp[x] = np.mean(Y[indices])\n",
        "\n",
        "  return cond_exp\n",
        "\n",
        "\n",
        " # 条件付き期待値を計算し、結果を出力します\n",
        " cond_exp_result = conditional_expectation(np.round(X), Y)\n",
        " print(f'条件付き期待値: {cond_exp_result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897c2e64",
      "metadata": {
        "id": "897c2e64"
      },
      "source": [
        "<h1 id=\"A%2FA+%E3%83%86%E3%82%B9%E3%83%88\">A/A テスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91b8278",
      "metadata": {
        "id": "f91b8278"
      },
      "source": [
        "### A/Aテストとは\n",
        "\n",
        "\n",
        " A/Aテストは通常のA/Bテストと異なり、2つのグループ（例えばWebページのバージョン）に全く同じ条件を与え、データを取得するテスト手法です。これにより、システムやツールの動作確認、外部要因による影響確認、統計的な検定方法の整合性を検証することができます。\n",
        "\n",
        "\n",
        " #### A/Aテストの数式\n",
        " A/Aテストにおいて重要なのは、得られたデータが偶然によるものか否かを検定することです。これは通常、$z$検定を使って行います。\n",
        "\n",
        "\n",
        " $$\n",
        " Z = \\frac{\\overline{X_1} - \\overline{X_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n",
        " $$\n",
        "\n",
        "\n",
        " - $Z$: 標準正規分布に従うZ値。\n",
        " - $\\overline{X_1}, \\overline{X_2}$: それぞれのグループの平均値。\n",
        " - $\\sigma_1^2, \\sigma_2^2$: それぞれのグループの分散。\n",
        " - $n_1, n_2$: それぞれのグループのサンプルサイズ。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " この書籍では、A/Bテストと共にA/Aテストも扱うことで、統計分析の基礎と、母集団からサンプルを取る際の課題を学びます。A/Aテストの理論を実際にコードで実装することで、理論と実践の両方を理解することができます。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " - システム実装のテストを行うことができる。\n",
        " - サンプルサイズ不足などの問題が存在しないか確認できる。\n",
        " - 外部要因の影響を確認できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811aa09a",
      "metadata": {
        "id": "811aa09a"
      },
      "outputs": [],
      "source": [
        "# PythonでA/Aテストのシミュレーションを行ってZ検定を適用します\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # グループごとのデータ生成（平均50、標準偏差10の正規分布）\n",
        " group1 = np.random.normal(50, 10, 1000) # グループ1のデータ\n",
        " print('Group 1 data:', group1[:5]) # 初めの5つのデータポイントを表示\n",
        " group2 = np.random.normal(50, 10, 1000) # グループ2のデータ\n",
        " print('Group 2 data:', group2[:5]) # 初めの5つのデータポイントを表示\n",
        "\n",
        "\n",
        " # グループごとの平均を計算\n",
        " mean1 = np.mean(group1) # グループ1の平均\n",
        " print('Mean of Group 1:', mean1)\n",
        " mean2 = np.mean(group2) # グループ2の平均\n",
        " print('Mean of Group 2:', mean2)\n",
        "\n",
        "\n",
        " # グループごとの分散を計算\n",
        " var1 = np.var(group1, ddof=1) # グループ1の分散（不偏分散）\n",
        " print('Variance of Group 1:', var1)\n",
        " var2 = np.var(group2, ddof=1) # グループ2の分散（不偏分散）\n",
        " print('Variance of Group 2:', var2)\n",
        "\n",
        "\n",
        " # Z検定の計算\n",
        " n1 = len(group1) # グループ1のサンプルサイズ\n",
        " n2 = len(group2) # グループ2のサンプルサイズ\n",
        "\n",
        "\n",
        " # Z値の計算に使用する分母\n",
        " pooled_se = np.sqrt(var1/n1 + var2/n2)\n",
        " print('Pooled Standard Error:', pooled_se)\n",
        "\n",
        "\n",
        " # Z値の計算\n",
        " z = (mean1 - mean2) / pooled_se\n",
        " print('Z-value:', z)\n",
        "\n",
        "\n",
        " # p値の計算（両側検定）\n",
        " p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        " print('P-value:', p_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e352ad1c",
      "metadata": {
        "id": "e352ad1c"
      },
      "source": [
        "<h1 id=\"%E7%B5%B1%E8%A8%88%E7%9A%84%E4%BB%AE%E8%AA%AC%E6%A4%9C%E8%A8%BC\">統計的仮説検証</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc660f56",
      "metadata": {
        "id": "dc660f56"
      },
      "source": [
        "統計的仮説検証は、ある仮説が正しいかどうかを統計的に検証するための手法です。こちらの理論は、まず帰無仮説（$H_0$）と対立仮説（$H_1$）を設定します。次に、データを用いて仮説がどの程度支持されるかを評価し、$p$値などを用いて結論を下します。帰無仮説 $H_0$ は通常、変化がない、あるいは効果がないという仮説で、対立仮説 $H_1$ は対照的に、効果があるとする仮説です。数式で表すと次の通りです。\n",
        "\n",
        "\n",
        " $$H_0: \\mu = \\mu_0$$\n",
        "\n",
        "\n",
        " $$H_1: \\mu \\neq \\mu_0$$\n",
        "\n",
        "\n",
        " ここで、$\\mu$ は母集団の平均を表し、$\\mu_0$ は帰無仮説における平均の仮定値を表します。\n",
        "\n",
        "\n",
        " 仮説検証によく使われる手法の一つに $t$-検定があります。これは主に2つのデータセットの平均が同じかどうかを検証するために使われます。これを用いることにより、例えば新しい治療法が従来のものよりも効果があるかどうかを判断できます。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、コードを通じてそれらの理論を試すことができる環境を提供します。具体的なコード例を以下に示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b10111",
      "metadata": {
        "id": "07b10111"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # データセットの生成: 2つのグループのデータ平均を比較する\n",
        " np.random.seed(0) # 再現可能性のためランダムシードを設定\n",
        "\n",
        "\n",
        " group1 = np.random.normal(5, 1, 30) # 平均5、標準偏差1の正規分布に従うデータ30個\n",
        " print('Group 1 Data:', group1)\n",
        "\n",
        "\n",
        " group2 = np.random.normal(6, 1, 30) # 平均6、標準偏差1の正規分布に従うデータ30個\n",
        " print('Group 2 Data:', group2)\n",
        "\n",
        "\n",
        " # 2つのグループの平均を比較するためのt-検定\n",
        " # t統計量とp値を計算する\n",
        " statistic, p_value = stats.ttest_ind(group1, group2)\n",
        "\n",
        "\n",
        " # t統計量とp値を出力\n",
        " print('T-statistic:', statistic)\n",
        " print('P-value:', p_value)\n",
        "\n",
        "\n",
        " # p値が0.05未満であれば、対立仮説を採択（平均に差がある）\n",
        " if p_value < 0.05:\n",
        "  print('Reject the null hypothesis: There is a significant difference between the groups.')\n",
        " else:\n",
        "  print('Fail to reject the null hypothesis: No significant difference found.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d99f7d",
      "metadata": {
        "id": "d8d99f7d"
      },
      "source": [
        "<h1 id=\"%E5%9B%9E%E5%B8%B0%E5%88%86%E6%9E%90\">回帰分析</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bd92fd",
      "metadata": {
        "id": "34bd92fd"
      },
      "source": [
        "### 回帰分析の解説\n",
        "\n",
        "\n",
        " 回帰分析は、与えられたデータに基づいて、目的変数（結果）と説明変数（入力）との関係をモデル化する手法です。最も単純な形である単回帰分析の数式は次のように表されます。\n",
        "\n",
        "\n",
        " $$\n",
        " y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        " $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $y$ は目的変数（予測したい結果）\n",
        " - $x$ は説明変数（入力データ）\n",
        " - $\\beta_0$ は切片（回帰直線とy軸との交点）\n",
        " - $\\beta_1$ は傾き（説明変数と目的変数の関係の強さと方向）\n",
        " - $\\epsilon$ は誤差（モデルで説明できないバラツキ）\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門においては、回帰分析を用いることで、政策や施策の影響をデータを通して測定する方法を学びます。例えば、新商品の導入が売上に与える影響を評価する際に利用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9349e04d",
      "metadata": {
        "id": "9349e04d"
      },
      "source": [
        "<h1 id=\"%E8%AA%A4%E5%B7%AE%E9%A0%85\">誤差項</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2912652c",
      "metadata": {
        "id": "2912652c"
      },
      "source": [
        "誤差項 (error term) は、統計モデルや回帰分析において観測される値とモデルが予測する値との差を表す要素です。数式としては以下のように表現されます。\n",
        "\n",
        "\n",
        " $$ Y = eta_0 + eta_1 X + \\epsilon $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $Y$ は従属変数（観測される値）です。\n",
        " - $eta_0$ は切片です。\n",
        " - $eta_1$ は独立変数 $X$ の係数です。\n",
        " - $X$ は独立変数です。\n",
        " - $\\epsilon$ は誤差項で、観測される値 $Y$ とモデルが予測する値との違いを表します。\n",
        "\n",
        "\n",
        " 誤差項は、モデルがすべての変動を説明できないために残るランダムな部分を示しています。誤差の正確な理解はモデルの健全性、予測精度の向上、および因果関係の解釈のために重要です。\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』における誤差項は、観測データとモデル予測との差を探る過程でしばしば言及され、実際の分析ケーススタディにおいてその理解が求められるでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee75cc54",
      "metadata": {
        "id": "ee75cc54"
      },
      "outputs": [],
      "source": [
        "# 誤差項を含む線形モデルをPythonでシミュレーションする\n",
        " import numpy as np # 数値計算のためのライブラリ\n",
        "\n",
        "\n",
        " # データ数\n",
        " n = 100\n",
        "\n",
        "\n",
        " # 独立変数Xを平均0，標準偏差1の正規乱数で生成\n",
        " X = np.random.normal(0, 1, n)\n",
        " print('X:', X)\n",
        "\n",
        "\n",
        " # 真のモデルのパラメータ\n",
        " beta_0 = 2.0 # 切片\n",
        " beta_1 = 3.0 # 係数\n",
        "\n",
        "\n",
        " # 誤差項epsilonを平均0，標準偏差1の正規乱数で生成\n",
        " epsilon = np.random.normal(0, 1, n)\n",
        " print('epsilon:', epsilon)\n",
        "\n",
        "\n",
        " # 従属変数Yを計算（真のモデル: Y = beta_0 + beta_1*X + epsilon）\n",
        " Y = beta_0 + beta_1 * X + epsilon\n",
        " print('Y:', Y)\n",
        "\n",
        "\n",
        " # 回帰分析を行うためにstatsmodelsライブラリを使用\n",
        " import statsmodels.api as sm\n",
        "\n",
        "\n",
        " # 内部で行列計算をする関係で、定数項を追加\n",
        " X = sm.add_constant(X)\n",
        "\n",
        "\n",
        " # 回帰モデルのフィッティング\n",
        " model = sm.OLS(Y, X).fit()\n",
        "\n",
        "\n",
        " # フィッティング結果の概要をプリント\n",
        " print(model.summary())\n",
        "\n",
        "\n",
        " # 誤差項とは、観測値Yと予測値(model.fittedvalues)の差です\n",
        " error_terms = Y - model.fittedvalues\n",
        " print('error_terms:', error_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493c3486",
      "metadata": {
        "id": "493c3486"
      },
      "source": [
        "<h1 id=\"%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE\">標準誤差</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4016ba3c",
      "metadata": {
        "id": "4016ba3c"
      },
      "source": [
        "# 標準誤差\n",
        "\n",
        "\n",
        " 標準誤差（Standard Error）は、サンプル平均の標準偏差を表す統計量です。特定の推定値が母集団平均にどれほど近いかを示します。これはサンプルの変動性を示しており、大きなサンプルサイズでは通常、標準誤差は小さくなります。\n",
        "\n",
        "\n",
        " 数式で表現すると、標準誤差は以下のようになります：\n",
        "\n",
        "\n",
        " $$SE = \\frac{s}{\\sqrt{n}}$$\n",
        "\n",
        "\n",
        " ここで：\n",
        " - $SE$ は標準誤差（Standard Error）を表します。\n",
        " - $s$ はサンプル標準偏差を表します。\n",
        " - $n$ はサンプルサイズを表します。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Pythonで標準誤差を計算するためには、通常サンプルデータの標準偏差を計算し、それをサンプルサイズの平方根で割ります。'Pythonで学ぶ効果検証入門'では、統計的推測や必要な統計量の推定において標準誤差がどのように役立つかを説明しています。使用用途としては、平均値の信頼区間の計算やt検定の導出などがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d925e4a",
      "metadata": {
        "id": "6d925e4a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        " # サンプルデータを用意する\n",
        " sample_data = [10, 20, 30, 40, 50]\n",
        "\n",
        "\n",
        " # サンプルサイズを取得する\n",
        " n = len(sample_data) # サンプルデータの長さからサンプルサイズを計算\n",
        " print('サンプルサイズ n:', n) # 計算されたサンプルサイズを出力\n",
        "\n",
        "\n",
        " # サンプル標準偏差を計算する\n",
        " s = np.std(sample_data, ddof=1) # Numpyライブラリを使ってサンプル標準偏差を計算\n",
        " ddof=1とすることで不偏標準偏差を計算\n",
        " print('サンプル標準偏差 s:', s) # 計算された標準偏差を出力\n",
        "\n",
        "\n",
        " # 標準誤差を計算する\n",
        " SE = s / np.sqrt(n) # 標準誤差の計算\n",
        " print('標準誤差 SE:', SE) # 計算された標準誤差を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a91b4ec",
      "metadata": {
        "id": "2a91b4ec"
      },
      "source": [
        "<h1 id=\"%E5%B8%B0%E7%84%A1%E4%BB%AE%E8%AA%AC\">帰無仮説</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb863bb",
      "metadata": {
        "id": "9fb863bb"
      },
      "source": [
        "### 帰無仮説の解説\n",
        "\n",
        "\n",
        " 帰無仮説（null hypothesis）とは、統計的検定において、特定の効果や差異が無いと仮定する仮説です。これに対して、検定することで効果や差異の存在を確認しようとする仮説を対立仮説（alternative hypothesis）と言います。\n",
        "\n",
        "\n",
        " 一般的な帰無仮説の表現としては、例えば平均 $\\mu$ に関して次のような形式があります。\n",
        "\n",
        "\n",
        " $$ H_0: \\mu = \\mu_0 $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $H_0$: 帰無仮説を表す記号。\n",
        " - $\\mu$: 母集団の平均を表す。\n",
        " - $\\mu_0$: 比較対象となる既知の平均値。\n",
        "\n",
        "\n",
        " 帰無仮説は、統計的検定の基礎となり、それが棄却されるかどうかに基づいて対立仮説の妥当性を推定します。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門との関係としては、帰無仮説を理解し、検定を行うことでデータ分析や実験結果の有意性の判断に役立ちます。また、帰無仮説を立てることで実験結果が偶然でないという証明を行い、データに基づいた意思決定を支えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1c282a",
      "metadata": {
        "id": "3d1c282a"
      },
      "outputs": [],
      "source": [
        "# 平均に対するt検定を使って帰無仮説を検定してみます。\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # データの例: サンプルデータです\n",
        " # 実際のデータに基づいて解析を行います\n",
        " sample_data = [10, 12, 9, 11, 13, 10, 10, 11, 12, 9]\n",
        " #比較対象の平均値（帰無仮説の平均）\n",
        " mu_0 = 11\n",
        "\n",
        "\n",
        " # データのサンプルサイズ\n",
        " n = len(sample_data)\n",
        " print('サンプルサイズ n:', n) # サンプルサイズを出力\n",
        "\n",
        "\n",
        " # サンプルの平均を計算\n",
        " sample_mean = np.mean(sample_data)\n",
        " print('サンプルの平均:', sample_mean) # サンプルの平均を出力\n",
        "\n",
        "\n",
        " # サンプルの標準偏差を計算（n-1で割って不偏推定量を出す）\n",
        " sample_std = np.std(sample_data, ddof=1)\n",
        " print('サンプルの標準偏差:', sample_std) # サンプルの標準偏差を出力\n",
        "\n",
        "\n",
        " # t-statistic の計算\n",
        " t_statistic = (sample_mean - mu_0) / (sample_std / np.sqrt(n))\n",
        " print('t-統計量:', t_statistic) # t-statistic を出力\n",
        "\n",
        "\n",
        " # t検定の p-value を計算\n",
        " p_value = stats.t.sf(np.abs(t_statistic), df=n-1) * 2 # 両側検定のため、p値を2倍する\n",
        " print('p値:', p_value) # p-value を出力\n",
        "\n",
        "\n",
        " # p値が有意水準より小さいかどうかを確認して、帰無仮説を棄却するか決定\n",
        " alpha = 0.05\n",
        " if p_value < alpha:\n",
        "  print('帰無仮説を棄却します。有意な差があると考えられます。')\n",
        " else:\n",
        "  print('帰無仮説を棄却できません。有意な差があるとは言えません。')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027675b1",
      "metadata": {
        "id": "027675b1"
      },
      "source": [
        "<h1 id=\"%E4%BB%AE%E8%AA%AC%E6%A4%9C%E5%AE%9A\">仮説検定</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b231b82",
      "metadata": {
        "id": "3b231b82"
      },
      "source": [
        "仮説検定は、統計的手法を用いて特定の仮説がデータによって支持されるかどうかを判断する方法です。仮説検定には帰無仮説と対立仮説の設定が必要です。帰無仮説とは従来の理論や主張などを表し、通常『差がない』という立場を取ります。一方、対立仮説は帰無仮説に反する仮説を示します。\n",
        "\n",
        "\n",
        " 仮説検定は一般に、統計量と呼ばれる値を計算し、その分布をもとに仮説の検定を行います。例えば、平均値の比較に基づくt検定などが挙げられます。t検定の数式は次のように表されます。\n",
        "\n",
        "\n",
        " $$ t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} $$\n",
        "\n",
        "\n",
        " ここで、$\\bar{x}$はサンプルの平均、$\\mu$は母平均、$s$はサンプルの標準偏差、$n$はサンプルサイズを意味します。\n",
        "\n",
        "\n",
        " この数式をPythonで学ぶ効果検証入門で扱う際、Pythonのライブラリを用いて統計検定を行うことができ、Pythonのコードを通じ理論的背景を理解するのに役立てます。使用用途としては、実験データの分析やビジネスにおける意思決定に応用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7070e03f",
      "metadata": {
        "id": "7070e03f"
      },
      "outputs": [],
      "source": [
        "# 仮説検定のPythonコード実装例\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # 仮のデータセットを生成\n",
        " np.random.seed(0)\n",
        " data = np.random.normal(50, 10, 30) # 平均50、標準偏差10、サンプルサイズ30の正規分布に従うデータ\n",
        "\n",
        "\n",
        " # サンプルの平均を計算\n",
        " sample_mean = np.mean(data)\n",
        " print('サンプルの平均:', sample_mean) # サンプル平均を出力\n",
        "\n",
        "\n",
        " # サンプルの標準偏差を計算\n",
        " sample_std = np.std(data, ddof=1)\n",
        " print('サンプルの標準偏差:', sample_std) # サンプル標準偏差を出力\n",
        "\n",
        "\n",
        " # サンプルサイズを取得\n",
        " n = len(data)\n",
        " print('サンプルサイズ:', n) # サンプルサイズを出力\n",
        "\n",
        "\n",
        " # 帰無仮説の下の母平均（例として50）\n",
        " mu = 50\n",
        "\n",
        "\n",
        " # t検定の統計量を計算\n",
        " t_statistic = (sample_mean - mu) / (sample_std / np.sqrt(n))\n",
        " print('t検定の統計量:', t_statistic) # t検定の統計量を出力\n",
        "\n",
        "\n",
        " # 採択域を決めるためのp値を計算\n",
        " t_critical = stats.t.ppf(0.975, df=n-1) # 95%信頼区間のt値\n",
        " print('t検定の臨界値:', t_critical) # t検定の臨界値を出力\n",
        "\n",
        "\n",
        " # t検定のp値を計算\n",
        " p_value = stats.ttest_1samp(data, mu)[1]\n",
        " print('p値:', p_value) # p値を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19eeaed6",
      "metadata": {
        "id": "19eeaed6"
      },
      "source": [
        "<h1 id=\"p%E5%80%A4\">p値</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b1aea1",
      "metadata": {
        "id": "79b1aea1"
      },
      "source": [
        "p値(またはp-value)は、統計学で利用される概念であり、観測されたデータが帰無仮説の下で得られるかどうかの確率を表します。帰無仮説は通常、何も起こらないと仮定した状態です。p値が小さい場合、帰無仮説が正しい状態でこのデータが観測される可能性が低いことを意味し、帰無仮説を棄却する証拠となります。\n",
        "\n",
        "\n",
        " p値は次のように数式で表現されます:\n",
        "\n",
        "\n",
        " $$ p = P(T \\geq t | H_0) $$\n",
        "\n",
        "\n",
        " ここで、\n",
        "\n",
        "\n",
        " - $P$: 確率を示します。\n",
        " - $T$: 標本からの統計量です。\n",
        " - $t$: 実際に観測された統計量の値です。\n",
        " - $H_0$: 帰無仮説です。\n",
        "\n",
        "\n",
        " つまり、帰無仮説が正しいと仮定した時に、観測データ以上に極端な値を得る確率を示します。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、統計的手法とPythonによる実装が説明されており、p値の解釈と利用方法をプログラム上で確認することができるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79eb356b",
      "metadata": {
        "id": "79eb356b"
      },
      "outputs": [],
      "source": [
        "from scipy import stats # SciPyライブラリからstatsモジュールをインポート\n",
        "\n",
        "\n",
        " # サンプルデータ - 観測されたデータ\n",
        " observed_data = [2, 4, 6, 8, 10] # 観測データのリストを設定\n",
        " print('Observed data:', observed_data) # 観測データの内容を出力\n",
        "\n",
        "\n",
        " # 仮の平均値 - 帰無仮説の仮定としての平均値\n",
        " null_hypothesis_mean = 5 # 帰無仮説として仮定する平均値を設定\n",
        " print('Null hypothesis mean:', null_hypothesis_mean) # 仮定する平均値を出力\n",
        "\n",
        "\n",
        " # t検定の実行\n",
        " # 帰無仮説に対するt検定を行い、p値を計算する\n",
        " statistic, p_value = stats.ttest_1samp(observed_data, null_hypothesis_mean)\n",
        " # t統計量とp値を取得\n",
        " print('T-statistic:', statistic) # t統計量を出力\n",
        " print('P-value:', p_value) # p値を出力\n",
        "\n",
        "\n",
        " # コードでは、観測されたデータリストを使い、平均が帰無仮説と同じかどうかをt検定で確認しています。\n",
        " # 結果としてt統計量と対応するp値を表示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c98e7f5",
      "metadata": {
        "id": "6c98e7f5"
      },
      "source": [
        "<h1 id=\"%E7%B5%B1%E8%A8%88%E7%9A%84%E3%81%AB%E9%9D%9E%E6%9C%89%E6%84%8F\">統計的に非有意</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87860694",
      "metadata": {
        "id": "87860694"
      },
      "source": [
        "統計的に非有意（statistical insignificance）とは、得られたデータから得ることのできる結果が、統計学的に有意ではないことを指します。これは通常、帰無仮説が棄却できない状態を意味します。帰無仮説は『効果がない』または『グループ間の差がない』ことを示す仮説です。統計的に非有意を示す数式は、通常$p$値を用いて表されます。\\n\\n$$ p > \\alpha $$\\n\\nここで、$p$は得られた$p$値を表し、$\\alpha$は通常の有意水準（一般的には0.05）を表します。\\n\\nこの$p$値は、観測されたデータが帰無仮説のもとで得られる確率を指します。$p$値が$\\alpha$より大きい場合、結果は統計的に非有意とされ、帰無仮説は棄却されません。\\n\\n統計的に非有意な結果が意味することとしては、検出可能な差や効果がないか、サンプルサイズが小さすぎる可能性が考えられます。この概念は実験や調査の結果の信頼性を評価する上で重要です。例えば、医療研究では、新薬が治療効果を持たないと結論づける場合に用いられます。\\n\\nPythonで学ぶ効果検証入門では、統計的な検定を行い、結果が有意であるか非有意であるかを判断するために、p値を計算する手法が説明されています。この書籍を通じて、統計的な結果に対する解釈を学ぶことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75928dda",
      "metadata": {
        "id": "75928dda"
      },
      "outputs": [],
      "source": [
        "# 統計的に非有意の例をPythonで示します\n",
        " # 必要なライブラリをインポート\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # サンプルデータの作成\n",
        " control = [20, 23, 21, 22, 19, 22, 21] # コントロールグループのデータ\n",
        " experiment = [21, 22, 20, 19, 23, 21, 22] # 実験グループのデータ\n",
        "\n",
        "\n",
        " # t検定の実施\n",
        " # t検定は、2つのグループの平均に有意差があるかを判定するためのもの。\n",
        " t_statistic, p_value = stats.ttest_ind(control, experiment)\n",
        "\n",
        "\n",
        " # 結果の出力\n",
        " print('t-statistic:', t_statistic) # t値を出力し、どの程度差があるか見る\n",
        " print('p-value:', p_value) # p値を出力し、非有意か有意か確認\n",
        "\n",
        "\n",
        " # p値の評価\n",
        " def evaluate_p_value(p_value, alpha=0.05):\n",
        "  \"\"\"\n",
        "  p値が有意水準alphaを超える場合、統計的に非有意と判断する。\n",
        "  \"\"\"\n",
        "  if p_value > alpha:\n",
        "  print('Result is statistically insignificant (p > alpha)')\n",
        "  else:\n",
        "  print('Result is statistically significant (p <= alpha)')\n",
        "\n",
        "\n",
        " # p値の評価実施\n",
        " evaluate_p_value(p_value) # p値を基に、結果が統計的に非有意か有意かを評価"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9123b81",
      "metadata": {
        "id": "d9123b81"
      },
      "source": [
        "<h1 id=\"%E6%9C%89%E6%84%8F%E6%B0%B4%E6%BA%96\">有意水準</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ba44a7",
      "metadata": {
        "id": "72ba44a7"
      },
      "source": [
        "有意水準とは、統計的仮説検定において、帰無仮説を棄却するための基準となる確率のことです。数式で有意水準 \\( \\alpha \\) は次のように表されます： $$ P(\\text{Type I error}) = \\alpha $$\n",
        " ここで、$\\alpha$ は有意水準を表す記号で、通常は0.05や0.01などの値が設定されます。これは、帰無仮説が真である場合に、それを誤って棄却する確率（第一種の誤り）を示します。Pythonで学ぶ効果検証入門においては、統計的仮説を検証するための基礎概念として有意水準が用いられ、実験や観察の結果が統計的に有意であるかどうかを判断するために使用されます。\n",
        " 一般的な用途として、A/Bテストのような実験の結果の判断を行う際に、仮説検定と組み合わせて使用されることがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64d1702",
      "metadata": {
        "id": "e64d1702"
      },
      "outputs": [],
      "source": [
        "# 有意水準を設定して仮説検定のためのサンプルを生成\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 有意水準alphaを設定（通常は0.05などを使う）\n",
        " alpha = 0.05\n",
        " print('有意水準alpha:', alpha)\n",
        "\n",
        "\n",
        " # サンプルデータのサイズ\n",
        " sample_size = 1000\n",
        "\n",
        "\n",
        " # 帰無仮説の下でのデータ生成（平均0, 標準偏差1の正規分布）\n",
        " data = np.random.normal(0, 1, sample_size)\n",
        " print('サンプルデータの一部:', data[:10]) # データの最初の10個を表示\n",
        "\n",
        "\n",
        " # 仮説検定のための統計量を計算\n",
        " # ここでは、母平均が0であるかの仮説をt検定で確認\n",
        " from scipy.stats import ttest_1samp\n",
        "\n",
        "\n",
        " t_statistic, p_value = ttest_1samp(data, 0)\n",
        " print('t統計量:', t_statistic)\n",
        " print('p値:', p_value)\n",
        "\n",
        "\n",
        " # p値と有意水準の比較による仮説検定の結果\n",
        " if p_value < alpha:\n",
        "  print('帰無仮説は棄却される。データは有意である。')\n",
        " else:\n",
        "  print('帰無仮説は棄却されない。データは有意ではない。')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fda0243",
      "metadata": {
        "id": "1fda0243"
      },
      "source": [
        "<h1 id=\"%E4%BF%A1%E9%A0%BC%E5%8C%BA%E9%96%93\">信頼区間</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03536fc",
      "metadata": {
        "id": "d03536fc"
      },
      "source": [
        "### 信頼区間の解説\n",
        "\n",
        "\n",
        " 信頼区間は、母集団のパラメータが特定の範囲内にある確率を示す範囲を指します。通常、95%信頼区間や99%信頼区間などが使われ、これは\"母集団のパラメータがこの区間に含まれる確率が95%または99%である\"という意味です。\n",
        "\n",
        "\n",
        " 信頼区間は以下の一般式で表されます：\n",
        "\n",
        "\n",
        " $$ \\bar{x} \\pm Z \\frac{\\sigma}{\\sqrt{n}} $$\n",
        "\n",
        "\n",
        " この数式において、\n",
        " - $\\bar{x}$ はサンプルの平均。\n",
        " - $Z$ は標準正規分布における信頼係数（95%信頼区間の場合は約1.96）。\n",
        " - $\\sigma$ はサンプルの標準偏差。\n",
        " - $n$ はサンプルサイズ。すなわちデータの個数。\n",
        "\n",
        "\n",
        " 信頼区間の計算はデータのばらつきを考慮しつつ、母集団の真の平均を推定するための手法として、『Pythonで学ぶ効果検証入門』においても利用されることが多いです。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " 信頼区間は、統計データの分析において、得られたサンプルデータから予測や推測を行う際に、その予測がどれほどの確からしさを持つかを表すために用いられます。例えば、薬の効果を検証する実験でその結果がどれほど信頼できるかを示すために使われます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1c705b",
      "metadata": {
        "id": "4f1c705b"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        " import numpy as np\n",
        " import scipy.stats as stats\n",
        "\n",
        "\n",
        " # サンプルデータを生成\n",
        " # サンプルデータとしてランダムな整数を生成します\n",
        " np.random.seed(0) # 再現性を持たせるためのシード値設定\n",
        " sample_data = np.random.randint(100, size=50)\n",
        " print('Sample data:', sample_data) # サンプルデータの確認\n",
        "\n",
        "\n",
        " # サンプル平均を計算\n",
        " sample_mean = np.mean(sample_data)\n",
        " print('Sample Mean:', sample_mean) # サンプル平均の確認\n",
        "\n",
        "\n",
        " # サンプル標準偏差を計算\n",
        " sample_std = np.std(sample_data, ddof=1) # 不偏分散を用いるためにddof=1を指定\n",
        " print('Sample Standard Deviation:', sample_std) # サンプル標準偏差の確認\n",
        "\n",
        "\n",
        " # サンプルサイズを取得\n",
        " n = len(sample_data)\n",
        " print('Sample Size:', n) # サンプルサイズの確認\n",
        "\n",
        "\n",
        " # 信頼係数を設定\n",
        " confidence_level = 0.95\n",
        " z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
        " print('Z-score:', z_score) # 信頼係数に対応するZスコアの確認\n",
        "\n",
        "\n",
        " # 信頼区間の計算\n",
        " margin_of_error = z_score * (sample_std / np.sqrt(n))\n",
        " confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n",
        " print('Confidence Interval:', confidence_interval) # 信頼区間の確認"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5d31db",
      "metadata": {
        "id": "ca5d31db"
      },
      "source": [
        "<h1 id=\"%E5%85%B1%E5%A4%89%E9%87%8F\">共変量</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2865dce2",
      "metadata": {
        "id": "2865dce2"
      },
      "source": [
        "共変量（Covariate）とは、統計学やデータ解析において、実験結果や研究結果に何らかの影響を与える可能性がある変数のことを指します。効果検証においては、共変量を考慮することで、原因と結果の関係をより明確に理解することが可能となります。例えば、医療試験において、患者の回復における治療効果を評価したい場合、年齢や性別などが共変量として考えられることがあります。数式で共変量は以下のように表現される場合があります。 $$Y = eta_0 + eta_1X_1 + eta_2X_2 + \\cdots + eta_nX_n +\n",
        " arepsilon$$ ここで、\n",
        " - $Y$ は目的変数（結果変数）。\n",
        " - $X_1, X_2, \\ldots, X_n$ は共変量。\n",
        " - $eta_0, eta_1, \\ldots, eta_n$ は各変数に対応する係数。\n",
        " - $\n",
        " arepsilon$ は誤差項。共変量とは、説明変数としてモデルに取り入れ、目的変数に対する影響を調整し、ノイズを取り除くことができるため、より精度の高い予測や解釈が可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "419f904e",
      "metadata": {
        "id": "419f904e"
      },
      "outputs": [],
      "source": [
        "# Pythonでの共変量を考慮した線形回帰モデルの実装例\n",
        " import numpy as np # 数値計算を効率よく行うためのライブラリ\n",
        " from sklearn.linear_model import LinearRegression # 線形回帰モデルを提供するライブラリ\n",
        "\n",
        "\n",
        " # ダミーデータの作成\n",
        " # 目的変数Yを生成\n",
        " Y = np.array([5, 7, 9, 11, 13]) # 結果として得られる数値データ\n",
        " print('目的変数 Y:', Y)\n",
        "\n",
        "\n",
        " # 共変量X1とX2を生成\n",
        " X1 = np.array([1, 2, 3, 4, 5]) # 共変量となるデータ1\n",
        " X2 = np.array([2, 3, 4, 5, 6]) # 共変量となるデータ2\n",
        " print('共変量 X1:', X1)\n",
        " print('共変量 X2:', X2)\n",
        "\n",
        "\n",
        " # 共変量を組み合わせて説明変数として扱う（行列として結合）\n",
        " X = np.column_stack((X1, X2)) # column_stackを使って共変量X1とX2を列単位で結合\n",
        " print('結合した共変量 X:', X)\n",
        "\n",
        "\n",
        " # 線形回帰モデルの初期化\n",
        " model = LinearRegression() # LinearRegressionクラスを使ってモデルを生成\n",
        "\n",
        "\n",
        " # モデルをトレーニングデータに適合させる（フィッティング）\n",
        " model.fit(X, Y) # fit関数を使ってモデルにデータを学習させる\n",
        "\n",
        "\n",
        " # 回帰係数を出力\n",
        " coef = model.coef_ # 学習したモデルの各説明変数の回帰係数を取得\n",
        " print('回帰係数:', coef)\n",
        "\n",
        "\n",
        " # 切片を出力\n",
        " intercept = model.intercept_ # 学習したモデルの切片を取得\n",
        " print('切片:', intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3e5af9",
      "metadata": {
        "id": "4a3e5af9"
      },
      "source": [
        "<h1 id=\"%E3%83%90%E3%83%A9%E3%83%B3%E3%82%B9%E3%83%86%E3%82%B9%E3%83%88\">バランステスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117b06fd",
      "metadata": {
        "id": "117b06fd"
      },
      "source": [
        "バランステストは、実験や観察研究で治療群と対照群が統計的に同等であるかを検証するための手法です。これは、群間の交絡因子の偏りを確認するために行われます。\n",
        "\n",
        "\n",
        " 数式で表現すると次のようになります。\n",
        "\n",
        "\n",
        " $$ H_0: \\mu_1 = \\mu_2 $$\n",
        "\n",
        "\n",
        " $$ H_1: \\mu_1 \\neq \\mu_2 $$\n",
        "\n",
        "\n",
        " ここで、$\\mu_1$ は治療群の平均、$\\mu_2$ は対照群の平均を意味しています。$H_0$ は帰無仮説で、両群の平均が等しいと仮定します。$H_1$ は対立仮説で、両群の平均が異なると仮定します。\n",
        "\n",
        "\n",
        " バランステストは主に、研究においてランダム化が適切に行われたかどうかを確認するために使用されます。特に、サンプルのランダム化が成功した場合、治療群と対照群で共変量が同様に分布するはずです。\n",
        "\n",
        "\n",
        " 'Pythonで学ぶ効果検証入門' との関係性については、この書籍が効果検証を実施するための実践的な方法を解説しており、バランステストを使うことで、その検証の正確さを保証するための基礎を築くことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59689090",
      "metadata": {
        "id": "59689090"
      },
      "outputs": [],
      "source": [
        "# バランステストをPythonで実装する例として、t検定を用いて治療群と対照群が同等かどうかを確認します。\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # 群のデータを生成\n",
        " control_group = np.random.normal(0, 1, 100) # 対照群のデータ\n",
        " print(\"Control Group Data:\", control_group)\n",
        " treatment_group = np.random.normal(0.1, 1, 100) # 治療群のデータ (平均が少し異なる)\n",
        " print(\"Treatment Group Data:\", treatment_group)\n",
        "\n",
        "\n",
        " # t検定を実施\n",
        " # 群間で平均に差があるかどうかを検定します。\n",
        " t_stat, p_value = stats.ttest_ind(treatment_group, control_group)\n",
        " print(\"T-statistic:\", t_stat)\n",
        " print(\"P-value:\", p_value)\n",
        "\n",
        "\n",
        " # 結果を解釈\n",
        " # p値が0.05未満であれば、帰無仮説を棄却し、両群の平均に統計的に有意な差があると判断します。\n",
        " if p_value < 0.05:\n",
        "  print(\"有意な差があります。治療群と対照群は統計的に異なります。\")\n",
        " else:\n",
        "  print(\"有意な差はありません。治療群と対照群は統計的に等しいです。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b4834f",
      "metadata": {
        "id": "11b4834f"
      },
      "source": [
        "<h1 id=\"t%E5%80%A4\">t値</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f63e67",
      "metadata": {
        "id": "76f63e67"
      },
      "source": [
        "t値（t-statistic）は、統計的仮説検定で利用される指標で、特にt検定と呼ばれる検定法で頻繁に使用されます。t値は、二つの平均値がどれほど異なるかを示し、その差がランダムな変動によるものか統計的に有意かを判断します。数式で表現されるt値は以下の通りです。\n",
        "\n",
        "\n",
        " $$ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\n",
        "\n",
        "\n",
        " ここで、$\\bar{x}_1$と$\\bar{x}_2$はそれぞれのサンプルの平均値、$s_p$はプールされた標準偏差、$n_1$と$n_2$はそれぞれのサンプルサイズです。具体的には、\n",
        "\n",
        "\n",
        " - $\\bar{x}_1$: 第一のサンプルの平均。\n",
        " - $\\bar{x}_2$: 第二のサンプルの平均。\n",
        " - $s_p$: プールされた標準偏差であり、双方のサンプルの標準偏差を考慮に入れて計算されます。\n",
        " - $n_1$: 第一のサンプルサイズ。\n",
        " - $n_2$: 第二のサンプルサイズ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a4e598",
      "metadata": {
        "id": "87a4e598"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # サンプルデータの作成\n",
        " sample1 = [1, 2, 3, 4, 5]\n",
        " sample2 = [2, 3, 4, 5, 6]\n",
        "\n",
        "\n",
        " # サンプル平均の計算\n",
        " mean1 = np.mean(sample1) # 第一のサンプルの平均を計算\n",
        " mean2 = np.mean(sample2) # 第二のサンプルの平均を計算\n",
        " print('mean1:', mean1)\n",
        " print('mean2:', mean2)\n",
        "\n",
        "\n",
        " # サンプルサイズ\n",
        " n1 = len(sample1)\n",
        " n2 = len(sample2)\n",
        " print('n1:', n1)\n",
        " print('n2:', n2)\n",
        "\n",
        "\n",
        " # サンプル分散の計算\n",
        " var1 = np.var(sample1, ddof=1) # 不偏分散を用いるためddof=1を設定\n",
        " var2 = np.var(sample2, ddof=1) # 不偏分散を用いるためddof=1を設定\n",
        " print('var1:', var1)\n",
        " print('var2:', var2)\n",
        "\n",
        "\n",
        " # プールされた標準偏差の計算\n",
        " df = n1 + n2 - 2 # 自由度の計算\n",
        " sp = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / df) # プールされた標準偏差の計算\n",
        " print('sp:', sp)\n",
        "\n",
        "\n",
        " # t値の計算\n",
        " t_value = (mean1 - mean2) / (sp * np.sqrt(1/n1 + 1/n2))\n",
        " print('t_value:', t_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a86b3e",
      "metadata": {
        "id": "79a86b3e"
      },
      "source": [
        "<h1 id=\"p-hacking\">p-hacking</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "166b2c4e",
      "metadata": {
        "id": "166b2c4e"
      },
      "source": [
        "# p-hackingの解説\n",
        "\n",
        "\n",
        " p-hackingとは、統計的な分析において、有意な結果を得るためにデータを繰り返し試行錯誤し、意図的または無意識的に調整を加える手法のことを指します。\n",
        "\n",
        "\n",
        " ## 理論と数式\n",
        "\n",
        "\n",
        " p-hackingを数式で表現するのは難しいですが、関与する概念を理解するのは重要です。典型的なのは、有意水準（$\\alpha$）とp値（$p$）の関係です。有意水準は、帰無仮説が正しい場合に差が偶然に生じる確率の閾値を示します。\n",
        "\n",
        "\n",
        " $$H_0: \\text{帰無仮説} \\\\\n",
        " H_1: \\text{対立仮説} \\\\\n",
        " p < \\alpha \\implies \\text{帰無仮説を棄却}$$\n",
        "\n",
        "\n",
        " ここで\n",
        " - $H_0$ は帰無仮説：通常の状態を示す仮説\n",
        " - $H_1$ は対立仮説：通常との違いを示す仮説\n",
        " - $p$ はp値：データが帰無仮説のもとで得られる確率\n",
        " - $\\alpha$ は有意水準：一般的に0.05や0.01が使用される\n",
        "\n",
        "\n",
        " ## 使用用途\n",
        "\n",
        "\n",
        " p-hackingは研究者が無意識に誤った結論を導く原因となり、結果の信頼性を損ないます。そのため、p-hackingを検出し、排除することが学術研究の信頼性を保つ上で重要です。\n",
        "\n",
        "\n",
        " ## Pythonで学ぶ効果検証入門との関係性\n",
        "\n",
        "\n",
        " 書籍『Pythonで学ぶ効果検証入門』の中で、p-hackingは科学的な分析を正確に行う上で避けるべき行為として説明されている可能性があります。適切な統計解析を実現するため、p-hackingのリスクを理解し、その影響を最小限に抑える手法が重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f650ce9",
      "metadata": {
        "id": "4f650ce9"
      },
      "outputs": [],
      "source": [
        "# Pythonでp-hackingの挙動をシミュレートする\n",
        " import numpy as np # 数値計算用ライブラリ\n",
        " from scipy import stats # 統計関数を使用するためのライブラリ\n",
        "\n",
        "\n",
        " np.random.seed(0) # 再現性のため、乱数シードを設定\n",
        "\n",
        "\n",
        " # データを生成\n",
        " # サイズ1000、平均0、標準偏差1の正規分布に従う乱数を生成\n",
        " data = np.random.normal(0, 1, 1000)\n",
        " print('Generated data:', data)\n",
        "\n",
        "\n",
        " # 仮に元のデータに無作為に付加的な要因を加えたと仮定\n",
        " # モジュール外部からランダムにデータの一部を改変する(p-hacking)\n",
        "\n",
        "\n",
        " p_values = [] # p値を保持するリストを初期化\n",
        " for _ in range(100):\n",
        "  sample_data = data + (np.random.rand() < 0.05) * np.random.normal(0, 1, 1000) # サンプルデータにノイズを追加する\n",
        "  print('Sample data with noise:', sample_data)\n",
        "\n",
        "\n",
        "  # t検定（平均0と比較）を実行\n",
        "  t_statistic, p_value = stats.ttest_1samp(sample_data, 0)\n",
        "  p_values.append(p_value) # 各試行のp値を保存\n",
        "\n",
        " print('P-values from trials:', p_values)\n",
        "\n",
        "\n",
        " # 統計的に有意と見なされるp値を確認\n",
        " significant_p_values = sum(p < 0.05 for p in p_values) # p値が0.05未満のものを集計\n",
        " print(f'Number of significant p-values (p < 0.05): {significant_p_values} out of 100 trials') # 有意なp値を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27915f60",
      "metadata": {
        "id": "27915f60"
      },
      "source": [
        "<h1 id=\"%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BCA%2FB%E3%83%86%E3%82%B9%E3%83%88\">クラスターA/Bテスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9513d3f5",
      "metadata": {
        "id": "9513d3f5"
      },
      "source": [
        "### クラスターA/Bテスト\n",
        "\n",
        "\n",
        " クラスターA/Bテストは、特定の単位（例: 地域、店舗、学校など）ごとにユーザーをグループ化し、そのグループ全体に対してAまたはBの異なる処置を施してテストを行う方法です。個別のユーザーにランダムに対処を割り当てるのではなく、クラスター単位で処置が割り当てられます。このアプローチは、例えば、地域ごとにマーケティングキャンペーンを比較するといった状況において有効です。\n",
        "\n",
        "\n",
        " #### 数式\n",
        " クラスターA/Bテストの結果は、一般的に以下のような数式で表現できます。\n",
        " $$\n",
        " D = \\frac{(Y_{T} - Y_{C})}{n}\n",
        " $$\n",
        " ここで：\n",
        " - $D$ は処置の効果の測定値です\n",
        " - $Y_{T}$ は処置グループの平均結果です\n",
        " - $Y_{C}$ はコントロールグループの平均結果です\n",
        " - $n$ はクラスターの数です\n",
        "\n",
        "\n",
        " #### 使用用途\n",
        " クラスターA/Bテストは、ランダムな個人レベルの割り当てが難しい場合や、同じ出力を共有する自然発生的なグループがある場合に最適です。例えば、医療の飲み薬のテストを行う際、同じ病院の患者間のトリートメントの交差を防ぐために病院単位でテストしたり、学校単位で教育手法をテストする場合に使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02116fac",
      "metadata": {
        "id": "02116fac"
      },
      "outputs": [],
      "source": [
        "# クラスターA/BテストのPython実装\n",
        "\n",
        "\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 処置グループとコントロールグループのデータを仮定\n",
        " # 各行はクラスターに対応し、各クラスターの平均結果を格納\n",
        " Y_T = np.array([30, 45, 25, 40, 55]) # 処置グループの各クラスターの平均結果\n",
        " Y_C = np.array([20, 35, 20, 30, 45]) # コントロールグループの各クラスターの平均結果\n",
        "\n",
        "\n",
        " # クラスターの数\n",
        " n = len(Y_T)\n",
        " print('クラスターの数:', n) # クラスターの数を出力\n",
        "\n",
        "\n",
        " # 処置の効果の測定値を計算\n",
        " D = (Y_T - Y_C).sum() / n\n",
        " print('処置の効果 (D):', D) # 処置の効果を出力\n",
        "\n",
        "\n",
        " # 上記のコードは以下のことを行います：\n",
        " # 1. 各グループのクラスター平均結果を保持する配列 Y_T と Y_C を定義\n",
        " # 2. クラスター数 n を計算し出力\n",
        " # 3. 各クラスターごとの差分を計算し、全てのクラスターの差分を合計してクラスター数で割ることで\n",
        " # 平均の差分を求め、処置の効果 D を計算し出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e4142a",
      "metadata": {
        "id": "07e4142a"
      },
      "source": [
        "<h1 id=\"%E3%83%81%E3%82%A7%E3%83%AA%E3%83%BC%E3%83%94%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0\">チェリーピッキング</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd70451",
      "metadata": {
        "id": "1bd70451"
      },
      "source": [
        "### チェリーピッキングとは\n",
        "\n",
        "\n",
        " チェリーピッキング（Cherry Picking）は、データや結果から自分の都合の良いものだけを選び出して提示することを指します。これにより、誤った結論を導いたり、特定の意図的な見解を強調することができます。数値解析においてチェリーピッキングは、データが持つ本来の意味を歪める可能性があるため、注意が必要です。\n",
        "\n",
        "\n",
        " #### チェリーピッキングに関連する数式\n",
        " チェリーピッキングは、その行為自体がデータの選択に関するものなので、特定の数式を持つものではありません。ただし、データの平均値や中央値を取る際に、チェリーピッキングが行われることが多いです。\n",
        "\n",
        "\n",
        " 例として、データセット$D$の中から、特定の条件を満たす要素だけを選び、その平均を取ることを考えます。\n",
        "\n",
        "\n",
        " $$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\quad \\text{ただし、} x_i \\in D \\text{かつ条件を満たす} $$\n",
        "\n",
        "\n",
        " - $\\bar{x}$: 条件を満たす要素の平均値\n",
        " - $n$: 条件を満たす要素の個数\n",
        " - $x_i$: 選ばれたデータセットの要素\n",
        "\n",
        "\n",
        " この数式は、特定の条件に合致したデータだけを用いて平均を求めており、チェリーピッキングの例となります。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " 『Pythonで学ぶ効果検証入門』という書籍は、統計学や実験結果の分析手法について学ぶことを目的としており、その中でチェリーピッキングを理解することは、偏りのない正確な解析を行うために重要です。チェリーピッキングが生じる場面やその影響を理解し、適切な方法でデータ分析を行うための知識を得ることができます。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " チェリーピッキングは意図せず発生することもあるため、通常は避けられるべきものです。しかし、特定のターゲットグループに対するマーケティングや特異なケーススタディを行う際に、意図して行う場合もあります。その際は、選択されたデータの偏りを認識した上で、適切に結論を導く工夫が求められます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0063c0",
      "metadata": {
        "id": "7d0063c0"
      },
      "outputs": [],
      "source": [
        "# サンプルデータセットを定義\n",
        " # データは平均を取る際に任意の条件でフィルタリングされる\n",
        " データセット = [12, 15, 14, 10, 9, 18, 56, 23, 45]\n",
        "\n",
        "\n",
        " # フィルタリング条件を設定（例えば、20未満のデータだけを選択）\n",
        " フィルタリングされたデータセット = [x for x in データセット if x < 20]\n",
        " print('フィルタリングされたデータセット:', フィルタリングされたデータセット) # フィルタリングされた結果を出力\n",
        "\n",
        "\n",
        " # フィルタリングされたデータセットの平均を計算\n",
        " 平均値 = sum(フィルタリングされたデータセット) / len(フィルタリングされたデータセット)\n",
        " print('平均値:', 平均値) # 計算された平均値を出力\n",
        "\n",
        "\n",
        " # このコードブロックは、データセットから条件に合致したデータを抽出し、その平均を求める処理を示している。\n",
        " # チェリーピッキングの一例として、条件によるデータの選択がどのように影響するかを示している。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a06bccb",
      "metadata": {
        "id": "2a06bccb"
      },
      "source": [
        "<h1 id=\"%E3%82%B3%E3%83%AB%E3%83%A2%E3%82%B4%E3%83%AD%E3%83%95-%E3%82%B9%E3%83%9F%E3%83%AB%E3%83%8E%E3%83%95%E6%A4%9C%E5%AE%9A\">コルモゴロフ-スミルノフ検定</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b020e6be",
      "metadata": {
        "id": "b020e6be"
      },
      "source": [
        "### コルモゴロフ-スミルノフ検定の解説\n",
        "\n",
        "\n",
        " コルモゴロフ-スミルノフ検定（Kolmogorov-Smirnov test, KS検定）は、2つの標本が同じ確率分布に従うかどうか、あるいはある標本が理論上の分布に従うかを調べる非パラメトリックな統計的検定です。特に、2標本問題や適合度検定に用いられます。\n",
        "\n",
        "\n",
        " この検定は、次のような統計量 $D$ を用います：\n",
        "\n",
        "\n",
        " $$D = \\max_x \\left| F_n(x) - F(x) \\right|$$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $F_n(x)$ は経験分布関数（Empirical Distribution Function, EDF）です。これは、データサンプルに基づいて得られる累積分布関数です。\n",
        " - $F(x)$ は理論上の累積分布関数で、通常は標準正規分布や他の仮定された分布です。\n",
        " - $\\max_x$ は、すべてのサンプルにわたっての最大値を意味します。\n",
        "\n",
        "\n",
        " この統計量 $D$ は、2つの累積分布関数の間の最大偏差を示します。統計量の値が小さいほど、2つの分布が近似的に等しいと考えられます。\n",
        "\n",
        "\n",
        " ### コルモゴロフ-スミルノフ検定の使用用途\n",
        " - **適合度の検定**：データが特定の分布（例えば、正規分布）に従っているかどうかを検証します。\n",
        " - **分布間の差**：2つの独立したグループ間の分布の差を検証します。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " この検定は、効果検証において統計的な仮説を検証する方法の一つです。Pythonを用いることで、このような検定を簡単に実行でき、データに基づく意思決定の精度を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aafead07",
      "metadata": {
        "id": "aafead07"
      },
      "outputs": [],
      "source": [
        "# KS検定をPythonで実行\n",
        " from scipy import stats\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # サンプルデータを生成\n",
        " np.random.seed(0) # 乱数の再現性を確保\n",
        " sample1 = np.random.normal(loc=0.0, scale=1.0, size=100) # 標準正規分布から抽出\n",
        " sample2 = np.random.normal(loc=0.5, scale=1.0, size=100) # 平均が0.5の正規分布から抽出\n",
        "\n",
        "\n",
        " # サンプルデータを出力して確認\n",
        " print('Sample 1:', sample1)\n",
        " print('Sample 2:', sample2)\n",
        "\n",
        "\n",
        " # KS検定の実行\n",
        " ks_statistic, p_value = stats.ks_2samp(sample1, sample2)\n",
        "\n",
        "\n",
        " # 結果を出力\n",
        " print('KS Statistic:', ks_statistic)\n",
        " print('P-value:', p_value)\n",
        "\n",
        "\n",
        " # コードの解説\n",
        " # from scipy import stats\n",
        " # - KS検定を含む様々な統計的検定を提供するSciPyライブラリをインポートしています。\n",
        "\n",
        "\n",
        " # import numpy as np\n",
        " # - データの生成や操作を行うためにNumPyライブラリをインポートしています。\n",
        "\n",
        "\n",
        " # np.random.seed(0)\n",
        " # - ランダムな数を生成する際に、結果を再現可能にするためにシードを設定しています。\n",
        "\n",
        "\n",
        " # sample1 = np.random.normal(loc=0.0, scale=1.0, size=100)\n",
        " # - 平均0.0、標準偏差1.0の正規分布に従う100個のデータを生成しています。\n",
        "\n",
        "\n",
        " # sample2 = np.random.normal(loc=0.5, scale=1.0, size=100)\n",
        " # - 平均0.5、標準偏差1.0の正規分布に従う100個のデータを生成しています。\n",
        "\n",
        "\n",
        " # print('Sample 1:', sample1)\n",
        " # print('Sample 2:', sample2)\n",
        " # - 生成したサンプル1とサンプル2を出力して確認しています。\n",
        "\n",
        "\n",
        " # ks_statistic, p_value = stats.ks_2samp(sample1, sample2)\n",
        " # - 2つのサンプルが同じ分布に従うかどうかを判断するためにKS検定を実行し、\n",
        " #  統計量ks_statisticとp値を取得しています。\n",
        "\n",
        "\n",
        " # print('KS Statistic:', ks_statistic)\n",
        " # print('P-value:', p_value)\n",
        " # - 検定の結果であるKS統計量とp値を出力しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f982049",
      "metadata": {
        "id": "3f982049"
      },
      "source": [
        "<h1 id=\"%E5%B1%A4%E5%8C%96+A%2FB+%E3%83%86%E3%82%B9%E3%83%88\">層化 A/B テスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa491a4",
      "metadata": {
        "id": "faa491a4"
      },
      "source": [
        "### 層化 A/B テスト\n",
        "\n",
        "\n",
        " 層化 A/B テストは、A/B テスト（スプリットテスト）の一種であり、実験を行う際に母集団を層（ストラタム）に分割して行います。これは、層内の変動を抑えつつ、層間の比較を可能にすることが目的です。\n",
        "\n",
        "\n",
        " 層化 A/B テストは特に、母集団がある種の特性によって異なるパターンを持つ場合に有効です。そのような特性には、地域や年齢、収入、過去の行動などがあります。これを考慮することで、各層のバイアスを取り除き、合計の変動を減少させ、テストの精度を向上させることができます。\n",
        "\n",
        "\n",
        " 層化 A/B テストの数式的表現は、各層における効果の平均を用いて、全体の効果を推定するものです。数式を以下に示します：\n",
        "\n",
        "\n",
        " $$ E_{total} = \\sum_{i=1}^{k} w_i \\cdot E_i $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $E_{total}$ は全体の効果の推定値です。\n",
        " - $\\sum$ は総和を表します。\n",
        " - $k$ は層の数を表します。\n",
        " - $w_i$ は層 $i$ の重み（通常、層 $i$ のサンプルサイズの全体サンプルサイズに対する割合）です。\n",
        " - $E_i$ は層 $i$ における効果の推定値です。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " 書籍『Pythonで学ぶ効果検証入門』では、効果検証の基本手法として層化 A/B テストが紹介されており、統計的な理論の背景とそのPythonによる実装を学ぶことができます。層化 A/B テストは、その精度の向上と正確な効果測定を目的として数式に基づいたアプローチを紹介しています。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " 層化 A/B テストは、以下のような場面で使用されます：\n",
        " - 受け手が多数の異なるグループに分かれている状況でのテスト。\n",
        " - 変数が多く、各グループの異なる応答を試験する場合。\n",
        " - 環境や人口統計に基づく偏りを排除する必要がある場合。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2281a5",
      "metadata": {
        "id": "4a2281a5"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 各層のサンプルサイズと効果のリストを定義\n",
        " sample_sizes = np.array([50, 30, 20]) # 各層のサンプルサイズ\n",
        " layer_effects = np.array([1.5, 0.5, -0.5]) # 各層の効果推定値\n",
        "\n",
        "\n",
        " # 各層の重みをサンプルサイズから計算\n",
        " weights = sample_sizes / sample_sizes.sum() # 各層のサンプルサイズを全体のサンプルサイズで割る\n",
        " print('Weights:', weights) # 各層の重みを出力\n",
        "\n",
        "\n",
        " # 全体の効果の推定値を計算\n",
        " E_total = np.sum(weights * layer_effects) # 各層の効果にその重みを掛けて総和をとる\n",
        " print('Total estimated effect:', E_total) # 全体の効果の推定値を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee80af5",
      "metadata": {
        "id": "1ee80af5"
      },
      "source": [
        "<h1 id=\"%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BC%E9%A0%91%E5%81%A5%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE\">クラスター頑健標準誤差</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb6e353e",
      "metadata": {
        "id": "cb6e353e"
      },
      "source": [
        "### クラスター頑健標準誤差について\n",
        "\n",
        "\n",
        " クラスター頑健標準誤差（Cluster-Robust Standard Errors）は、観測値が独立していない場合、特にデータがクラスターと呼ばれるグループに分けられる場合に適している標準誤差の推定方法です。この手法は、クラスター内での誤差項が相関している可能性がある状況でより正確な結果を提供するために使用されます。\n",
        "\n",
        "\n",
        " 通常の標準誤差の計算が観測値の独立性を仮定しているのに対し、クラスター頑健標準誤差はクラスター内相関を考慮します。これにより、より信頼性の高い推定が可能となります。\n",
        "\n",
        "\n",
        " #### 数式\n",
        " クラスター頑健標準誤差は、次の数式で表現されます。\n",
        "\n",
        "\n",
        " $$V_{CRSE} = (X'X)^{-1} \\left( \\sum_{c=1}^{C} X_c' \\hat{u}_c \\hat{u}_c' X_c \\right) (X'X)^{-1}$$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $X$ は設計行列（独立変数の行列）、\n",
        " - $X_c$ はクラスター$c$に属する観測値に対する行列、\n",
        " - $\\hat{u}_c$ は回帰モデルの残差ベクトルでクラスター$c$に属するもの、\n",
        " - $C$ はクラスターの総数。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " クラスター頑健標準誤差は、特にデータがクラスターに分けられ、それぞれのクラスター内で相関がある場合に使われます。例えば、教育データでは、同じクラスに属する学生のデータがクラスターを形成し、同じ教師の影響を受けるなどのケースに適用されます。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " 「Pythonで学ぶ効果検証入門」では、政策評価や様々な統計モデルの有効性を検証する際に、このような頑健な推定方法を用いることで、より信頼性の高い結論を得ることができることが示されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae868b1",
      "metadata": {
        "id": "dae868b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        " # データの設定\n",
        " # Xは設計行列で、各列が1つの独立変数に対応します\n",
        " # 例としてランダムに生成しますが、実世界のデータに置き換える必要があります\n",
        " X = np.random.rand(100, 3)\n",
        "\n",
        "\n",
        " # 残差ベクトル（例としてランダムに生成）\n",
        " # 実データでは回帰モデルの予測から計算した残差を使います\n",
        " residuals = np.random.rand(100)\n",
        "\n",
        "\n",
        " # クラスター情報（例として10個のクラスターをランダムに割り当て）\n",
        " clusters = np.random.randint(0, 10, size=100)\n",
        "\n",
        "\n",
        " # 設計行列のXの転置を計算\n",
        " X_t = X.T\n",
        "\n",
        "\n",
        " # 行列積を計算(X'X)\n",
        " XtX = X_t @ X\n",
        " print('XtX:', XtX)\n",
        "\n",
        "\n",
        " # クラスター内の計算を蓄積する変数\n",
        " cluster_sums = np.zeros_like(XtX)\n",
        " print('Initial cluster_sums:', cluster_sums)\n",
        "\n",
        "\n",
        " # クラスターごとの計算\n",
        " for cluster in np.unique(clusters):\n",
        "  # 同一クラスターに属する観測値をフィルタリング\n",
        "  cluster_indices = np.where(clusters == cluster)\n",
        "  X_c = X[cluster_indices]\n",
        "  residuals_c = residuals[cluster_indices]\n",
        "\n",
        "  # 各クラスターごとにX_c' * (residuals_c * residuals_c') * X_cを計算して蓄積\n",
        "  cluster_sums += X_c.T @ np.outer(residuals_c, residuals_c) @ X_c\n",
        "  print(f'Cluster {cluster} sums:', cluster_sums)\n",
        "\n",
        "\n",
        " # 最終的なクラスター頑健標準誤差を計算\n",
        " V_CRSE = np.linalg.inv(XtX) @ cluster_sums @ np.linalg.inv(XtX)\n",
        " print('V_CRSE:', V_CRSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecd0f77",
      "metadata": {
        "id": "fecd0f77"
      },
      "source": [
        "<h1 id=\"%E5%B1%80%E6%89%80%E5%B9%B3%E5%9D%87%E5%87%A6%E7%BD%AE%E5%8A%B9%E6%9E%9C%EF%BC%88LATE%EF%BC%89\">局所平均処置効果（LATE）</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72404e30",
      "metadata": {
        "id": "72404e30"
      },
      "source": [
        "局所平均処置効果（LATE: Local Average Treatment Effect）は、統計学や因果推論において、特に実験や観察データから因果効果を識別するために使用される概念です。LATEは、処置割付（treatment assignment）がランダム化されているが完全な遵守がない場合、例えば治療プロトコルが厳密に守られないケースで有用です。\n",
        "\n",
        "\n",
        " LATEの数式表現は次のように表されます：\n",
        " $$ LATE = \\frac{E[Y_i|Z_i=1] - E[Y_i|Z_i=0]}{E[D_i|Z_i=1] - E[D_i|Z_i=0]} $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $E[Y_i|Z_i=1]$ は割当変数 $Z_i$ が1のときの結果変数 $Y_i$ の期待値を表し、\n",
        " - $E[Y_i|Z_i=0]$ は割当変数 $Z_i$ が0のときの結果変数 $Y_i$ の期待値を表します。\n",
        " - $E[D_i|Z_i=1]$ は割当変数 $Z_i$ が1のときの処置変数 $D_i$ の期待値を表し、\n",
        " - $E[D_i|Z_i=0]$ は割当変数 $Z_i$ が0のときの処置変数 $D_i$ の期待値を表します。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、このような因果推論を統計的手法で学ぶことができ、実際にPythonによる実装も提供されています。LATEの用途としては、プログラムや政策の効果を推定する際に、ランダム化された統制試験（Randomized Controlled Trial: RCT）が完全に遵守されていない場合の効果を適切に捉えるために使われます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0eab7c",
      "metadata": {
        "id": "ce0eab7c"
      },
      "outputs": [],
      "source": [
        "# LATEをPythonで計算するスクリプト例\n",
        "\n",
        "\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # ダミーデータの生成\n",
        " np.random.seed(0)\n",
        " Z = np.random.binomial(1, 0.5, 1000) # 割当変数\n",
        " D = Z * np.random.binomial(1, 0.8, 1000) # 処置変数（部分的遵守）\n",
        " Y = 2 * D + np.random.normal(0, 1, 1000) # 結果変数\n",
        "\n",
        "\n",
        " # データの内容を確認\n",
        " print('Z:', Z[:10])\n",
        " print('D:', D[:10])\n",
        " print('Y:', Y[:10])\n",
        "\n",
        "\n",
        " # 割当変数の違いによる期待値の計算\n",
        " E_Y_Z1 = np.mean(Y[Z == 1]) # Zが1のときのYの期待値\n",
        " E_Y_Z0 = np.mean(Y[Z == 0]) # Zが0のときのYの期待値\n",
        " E_D_Z1 = np.mean(D[Z == 1]) # Zが1のときのDの期待値\n",
        " E_D_Z0 = np.mean(D[Z == 0]) # Zが0のときのDの期待値\n",
        "\n",
        "\n",
        " # 計算された期待値を出力\n",
        " print('E[Y|Z=1]:', E_Y_Z1)\n",
        " print('E[Y|Z=0]:', E_Y_Z0)\n",
        " print('E[D|Z=1]:', E_D_Z1)\n",
        " print('E[D|Z=0]:', E_D_Z0)\n",
        "\n",
        "\n",
        " # LATEの計算\n",
        " late = (E_Y_Z1 - E_Y_Z0) / (E_D_Z1 - E_D_Z0)\n",
        "\n",
        "\n",
        " # LATEの結果を出力\n",
        " print('LATE:', late)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c9406c8",
      "metadata": {
        "id": "9c9406c8"
      },
      "source": [
        "<h1 id=\"%E4%BA%A4%E5%B7%AE%E9%A0%85\">交差項</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74dba48",
      "metadata": {
        "id": "b74dba48"
      },
      "source": [
        "交差項とは、2つ以上の説明変数の積を新たな説明変数としてモデルに加えることで、モデリングの柔軟性を高めるために使用されます。例えば、変数$X$と$Z$の交差項は、$XZ$として表現され、モデル中に追加されます。$XZ$は、個別の変数$X$および$Z$が取りうる値の組み合わせによって影響を受ける要因を捉えることができるため、相互作用の効果をモデルに組み込むことができます。\n",
        "\n",
        "\n",
        " 基本的な数式は以下のようになります：\n",
        "\n",
        "\n",
        " $$ Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3XZ + \\epsilon $$\n",
        "\n",
        "\n",
        " この式の各記号の意味は以下の通りです：\n",
        " - $Y$：目的変数（従属変数）\n",
        " - $\\beta_0$：切片項（定数項）\n",
        " - $\\beta_1$, $\\beta_2$, $\\beta_3$：説明変数の係数\n",
        " - $X$：説明変数1\n",
        " - $Z$：説明変数2\n",
        " - $XZ$：$X$と$Z$の交差項\n",
        " - $\\epsilon$：誤差項\n",
        "\n",
        "\n",
        " 交差項は、説明変数間の相互作用効果を考慮するために、線形回帰や他の統計モデルで使用されます。\n",
        "\n",
        "\n",
        " 以下に交差項を含む線形回帰モデルをPythonで実装する例を示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689a462c",
      "metadata": {
        "id": "689a462c"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        " import numpy as np\n",
        " import pandas as pd\n",
        " from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        " # データを生成\n",
        " np.random.seed(0) # 乱数シードを設定し、結果の再現性を持たせる\n",
        " X = np.random.rand(100, 1) # 説明変数Xをランダムに生成\n",
        " Z = np.random.rand(100, 1) # 説明変数Zをランダムに生成\n",
        "\n",
        "\n",
        " # 交差項XZを生成\n",
        " XZ = X * Z # XとZの要素ごとの積を計算して交差項XZを作成\n",
        "\n",
        "\n",
        " # 目的変数Yを設定（ここでは乱数を用いて擬似的に生成）\n",
        " # 後ほどこれに交差項の含まれるモデルを適用し回帰する\n",
        " Y = 3 + 2 * X + 4 * Z + 5 * XZ + np.random.rand(100, 1)\n",
        "\n",
        "\n",
        " # データを確認\n",
        " print('X:', X[:5]) # Xの最初の5行を表示\n",
        " print('Z:', Z[:5]) # Zの最初の5行を表示\n",
        " print('XZ:', XZ[:5]) # XZの最初の5行を表示\n",
        " print('Y:', Y[:5]) # Yの最初の5行を表示\n",
        "\n",
        "\n",
        " # フィッティング用データフレームを構築\n",
        " data = pd.DataFrame(np.hstack((X, Z, XZ, Y)), columns=['X', 'Z', 'XZ', 'Y']) # X, Z, XZ, Yを列に持つデータフレームを作成\n",
        "\n",
        "\n",
        " # 線形回帰モデルにデータをフィット\n",
        " model = LinearRegression() # 線形回帰モデルを初期化\n",
        " model.fit(data[['X', 'Z', 'XZ']], data['Y']) # モデルに交差項XZを含むデータを適用しフィット\n",
        "\n",
        "\n",
        " # 回帰係数を出力\n",
        " print('Coefficients:', model.coef_) # モデルの係数を表示\n",
        " print('Intercept:', model.intercept_) # モデルの切片を表示"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c0243c",
      "metadata": {
        "id": "a2c0243c"
      },
      "source": [
        "<h1 id=\"rdrobust%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA\">rdrobustライブラリ</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9908f14",
      "metadata": {
        "id": "e9908f14"
      },
      "source": [
        "rdrobustライブラリは、主に回帰不連続デザイン（Regression Discontinuity Design, RDD）を使用した効果検証のためのツールを提供します。RDDは、それぞれの単位（例: 教育プログラムに登録するかどうか）に利用可能なカットオフを使用して、二つのグループ間の効果を推定します。\n",
        "\n",
        "\n",
        " 数式で表現すると、RDDは次のように表現されます：\n",
        "\n",
        "\n",
        " $$Y_i = \\alpha + \\tau D_i + f(X_i) + \\epsilon_i$$\n",
        "\n",
        "\n",
        " ここで、\n",
        "\n",
        "\n",
        " - $Y_i$ は観測されたアウトカム（結果変数）、\n",
        " - $\\alpha$ は定数項、\n",
        " - $\\tau$ は介入の効果、\n",
        " - $D_i$ は処遇ダミー変数（カットオフを超えたかどうか）、\n",
        " - $f(X_i)$ はスムースな関数であり、$X_i$ はその他の共変量を表す、\n",
        " - $\\epsilon_i$ は誤差項を表します。\n",
        "\n",
        "\n",
        " rdrobustライブラリは、指定されたカットオフにおける介入の局所的なカーネル加重を用いた回帰に基づいて、$\\tau$ を推定します。この数式の理論的背景や詳細は、Pythonで学ぶ効果検証入門においても、効果検証手法の一部として解説されています。\n",
        "\n",
        "\n",
        " 使用用途としては、政策評価や施策の有効性を調べるためのデータ分析に利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e51a6ac",
      "metadata": {
        "id": "5e51a6ac"
      },
      "outputs": [],
      "source": [
        "# rdrobustライブラリを使った効果検証のサンプルコード\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " import pandas as pd\n",
        " from rdrobust import rdrobust\n",
        "\n",
        "\n",
        " # データセットの作成\n",
        " np.random.seed(0)\n",
        " data = pd.DataFrame({\n",
        "  'X': np.random.uniform(-1, 1, 100), # 変数Xはランダムに生成します\n",
        "  'Y': np.random.normal(0, 1, 100) # 変数Yは正規乱数で生成します\n",
        " })\n",
        " data['D'] = (data['X'] >= 0).astype(int) # Xが0以上ならDは1\n",
        "\n",
        "\n",
        " # カットオフに近いデータの観察\n",
        " print(data.head())\n",
        "\n",
        "\n",
        " # rdrobust関数を適用して見積もりを行う\n",
        " rd_result = rdrobust(data['Y'], data['X'], c=0)\n",
        "\n",
        "\n",
        " # 結果の表示\n",
        " print(rd_result)\n",
        "\n",
        "\n",
        " # ここでの目的は、X=0をカットオフとしてYの変化を見積もることです。\n",
        " # rdrobust関数はこの変化を推定し、結果をrd_resultに保存しています。\n",
        "\n",
        "\n",
        " # rdrobustの出力を確認するための処理\n",
        " print(f\"Estimated Effect: {rd_result['coefficients']}\")\n",
        " # 'coefficients'キーには、推定されたτの値が含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e214afd2",
      "metadata": {
        "id": "e214afd2"
      },
      "source": [
        "<h1 id=\"%E3%82%A2%E3%82%A6%E3%83%88%E3%82%AB%E3%83%A0\">アウトカム</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed2fd5a",
      "metadata": {
        "id": "bed2fd5a"
      },
      "source": [
        "アウトカム（Outcome）は因果推論や統計分析において、介入や処置の結果として観測される変数を指します。例えば、医薬品の実験においては、患者の健康状態や治療改善率などがアウトカムとして考えられます。アウトカムは典型的に以下のような数式で表現されます。\n",
        "\n",
        "\n",
        " $$ Y = f(T, X, \\epsilon) $$\n",
        "\n",
        "\n",
        " この数式において、\n",
        " - $Y$ はアウトカム変数（結果変数）を示します。\n",
        " - $T$ は扱われる処置または介入（例: 薬の投与）を示します。\n",
        " - $X$ は制御変数や共変量（例: 年齢、性別、既往歴など）を示します。\n",
        " - $\\epsilon$ は観測されない誤差項を示します。\n",
        "\n",
        "\n",
        " この式は、アウトカムが処置や共変量に依存することを示しており、誤差項はモデル化されていない影響をキャプチャします。\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』では、このような式を用いて、Pythonを使った実証分析を行う手法が解説されています。効果検証のためには、どの変数が因果関係にあるのかを正確に把握し、適切な分析手法を選択することが重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d126f6",
      "metadata": {
        "id": "91d126f6"
      },
      "outputs": [],
      "source": [
        "['# アウトカムを計算するための簡単な例を示します。', 'import numpy as np', '', '# 介入処置を示す変数T（ここではランダムに0か1を与える）', 'T = np.random.randint(0, 2, 100)', \"print('T:', T)\", '', '# 共変量X（ここでは0から1までのランダム値）', 'X = np.random.rand(100)', \"print('X:', X)\", '', '# モデル誤差項ε（ここでも正規分布に従うランダム値）', 'epsilon = np.random.normal(0, 0.1, 100)', \"print('epsilon:', epsilon)\", '', '# アウトカムYを計算する。今回は簡単な線形モデルを使用。', '# YはT, X, epsilonの関数として定義されています。', 'Y = 3 * T + 2 * X + epsilon', \"print('Y:', Y)\", '', '# ここで、Yは与えられた処置 T と共変量 X に基づくアウトカムです。']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c623d2",
      "metadata": {
        "id": "e1c623d2"
      },
      "source": [
        "<h1 id=\"%E5%9B%BA%E5%AE%9A%E5%8A%B9%E6%9E%9C%E3%83%A2%E3%83%87%E3%83%AB\">固定効果モデル</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca096225",
      "metadata": {
        "id": "ca096225"
      },
      "source": [
        "### 固定効果モデルの解説\n",
        "\n",
        "\n",
        " 固定効果モデル（Fixed Effects Model）は、観察データにおける未観測の個人差を考慮し、それが分析結果に影響を与えないように定量化するためのモデルです。一般的にパネルデータ（同一対象から複数時点で観測されたデータ）に対して用いられます。\n",
        "\n",
        "\n",
        " #### 固定効果モデルの数式\n",
        " パネルデータにおける固定効果モデルは、以下のように表現されます：\n",
        "\n",
        "\n",
        " $$\n",
        " y_{it} = \\alpha + \\beta x_{it} + \\gamma_i + \\epsilon_{it}\n",
        " $$\n",
        "\n",
        "\n",
        " ここでの記号の意味を以下に説明します：\n",
        " - $y_{it}$: 個体 $i$ の時点 $t$ における従属変数\n",
        " - $\\alpha$: 共通の切片\n",
        " - $\\beta$: 独立変数 $x_{it}$ に対する係数\n",
        " - $x_{it}$: 個体 $i$ の時点 $t$ における独立変数\n",
        " - $\\gamma_i$: 個体 $i$ に固有の固定効果（時間に依存せず、個体に固有の影響）\n",
        " - $\\epsilon_{it}$: 残差や誤差項（通常は独立で分散が等しいと仮定）\n",
        "\n",
        "\n",
        " 固定効果モデルの主な用途は、各個体の特有の影響を除去した上で、独立変数の影響を捉えることです。これにより、類似の個体間の分析が可能になり、観測不可能な偏りを除去できます。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " 書籍『Pythonで学ぶ効果検証入門』では、Pythonを用いて効果検証を行う方法が説明されています。固定効果モデルは、同書のテーマである因果推論において、内生性の問題を緩和する手法の一つとして取り扱われています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b552b982",
      "metadata": {
        "id": "b552b982"
      },
      "outputs": [],
      "source": [
        "# Pythonで固定効果モデルを実装するためのサンプルコード\n",
        " import pandas as pd\n",
        " import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        " # サンプルデータの作成\n",
        " # ここでは、個体ID、時間、従属変数y、独立変数xを持つデータフレームを仮定\n",
        " np.random.seed(1)\n",
        " data = pd.DataFrame({\n",
        "  'id': np.repeat(range(1, 6), 10), # 5個体, 各10時点\n",
        "  'time': list(range(10)) * 5,\n",
        "  'y': np.random.randn(50), # 従属変数\n",
        "  'x': np.random.randn(50)  # 独立変数\n",
        " })\n",
        "\n",
        "\n",
        " print(data.head()) # データの最初の5行を出力して確認\n",
        "\n",
        "\n",
        " # 固定効果モデルの構築\n",
        " # 'y ~ x' はモデル式で、従属変数をy、独立変数をxと指定\n",
        " # 0 + C(id) は、固定効果を個体IDでコントロールすることを指定\n",
        " model = smf.ols('y ~ x + C(id)', data=data).fit()\n",
        "\n",
        "\n",
        " # モデルの結果を出力\n",
        " print(model.summary()) # 結果の要約を表示\n",
        "\n",
        "\n",
        " # 固定効果モデルの役割は個体間の不変な違い（各idに固有の影響）を制御すること"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c24732b",
      "metadata": {
        "id": "0c24732b"
      },
      "source": [
        "<h1 id=\"%E3%83%91%E3%83%A9%E3%83%AC%E3%83%AB%E3%83%88%E3%83%AC%E3%83%B3%E3%83%89%E4%BB%AE%E5%AE%9A\">パラレルトレンド仮定</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730d91c3",
      "metadata": {
        "id": "730d91c3"
      },
      "source": [
        "## パラレルトレンド仮定の解説\n",
        "\n",
        "\n",
        " パラレルトレンド仮定は、差分の差分法 (Difference in Differences, DiD) を用いるための重要な仮定です。治療群と対照群の間のトレンドが、処置が行われる前に時間とともに均等に変わることを仮定します。\n",
        "\n",
        "\n",
        " この仮定は、処置の有無による違いを正確に測定し、因果関係を特定するのに役立ちます。\n",
        "\n",
        "\n",
        " ### 数式\n",
        "\n",
        "\n",
        " パラレルトレンドを数式で表すと次のようになります：\n",
        "\n",
        "\n",
        " $$ E[Y_{t0}^D - Y_{t0}^N] = E[Y_{t1}^D - Y_{t1}^N] $$\n",
        "\n",
        "\n",
        " - $E$ は期待値を表します。\n",
        " - $Y_{t0}^D$ は時間$t0$での処置群の平均結果です。\n",
        " - $Y_{t0}^N$ は時間$t0$での対照群の平均結果です。\n",
        " - $Y_{t1}^D$ は時間$t1$での処置群の平均結果です。\n",
        " - $Y_{t1}^N$ は時間$t1$での対照群の平均結果です。\n",
        "\n",
        "\n",
        " この仮定により、治療と対照群の結果が事前には平行なトレンドを辿っていると考えられ、その上で処置があった後に結果が異なることを観察します。\n",
        "\n",
        "\n",
        " ### パラレルトレンド仮定のPythonでの学び方\n",
        "\n",
        "\n",
        " 書籍『Pythonで学ぶ効果検証入門』では、因果推論や効果検証の基本的な概念をPyhtonを使って学ぶことができ、パラレルトレンド仮定はその中で差分の差分法を理解するための基盤となる仮定です。\n",
        "\n",
        "\n",
        " 使用用途としては、特に政策評価やマーケティングなどにおいて、治療効果を推定するときに用いられます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd69963c",
      "metadata": {
        "id": "dd69963c"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        " import pandas as pd\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # データフレームを構築します。ここでは仮のデータを使用しています。\n",
        " data = {\n",
        "  'time': ['t0', 't1', 't0', 't1'], # 時間情報\n",
        " t0とt1を使います。\n",
        "  'group': ['treatment', 'treatment', 'control', 'control'], # グループ情報\n",
        "  'outcome': [5, 15, 7, 12] # 各グループとタイムポイントでの結果の値\n",
        " }\n",
        " df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        " # データフレームを表示し、内容を確認します\n",
        " print(df)\n",
        "\n",
        "\n",
        " # ピボットテーブルを作成して、各グループ毎のアウトカムの平均を時間毎に見ます\n",
        " table = df.pivot_table(values='outcome', index='time', columns='group')\n",
        " print(table)\n",
        "\n",
        "\n",
        " # 各時間でのアウトカムの差を計算し、パラレルトレンド仮定を確認します\n",
        " diff_t0 = table.loc['t0', 'treatment'] - table.loc['t0', 'control'] # t0時点の差\n",
        " diff_t1 = table.loc['t1', 'treatment'] - table.loc['t1', 'control'] # t1時点の差\n",
        "\n",
        "\n",
        " # パラレルトレンドの仮定が成り立つかを確認するため、差分を出力します\n",
        " print('Difference at t0:', diff_t0)\n",
        " print('Difference at t1:', diff_t1)\n",
        "\n",
        "\n",
        " # 理想的には、これらの差が政策導入の効果によって異なるかを判断します。\n",
        " # したがって、観測する差分の変化を出力します\n",
        " difference_in_differences = diff_t1 - diff_t0\n",
        " print('Difference in Differences:', difference_in_differences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234e196d",
      "metadata": {
        "id": "234e196d"
      },
      "source": [
        "<h1 id=\"McCrary+%E3%81%AE%E6%A4%9C%E5%AE%9A\">McCrary の検定</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90bbd973",
      "metadata": {
        "id": "90bbd973"
      },
      "source": [
        "McCraryの検定（McCrary density test）は、回帰不連続デザイン（Regression Discontinuity Design, RDD）を利用する際に重要な検定の一つで、処置変数（治療変数）の切れ目（discontinuity）付近での密度が急激に変化していないか検証するために用います。この検定は、RDDにおいてバイアスをチェックし、操作変数が実際には統計的に有意な影響を受けていないかどうかを確認するために使用されます。特に、処置グループと対照グループの境界付近で被験者が不正に割り振られていないかを検出します。検出方法において、密度関数の連続性を視覚化したり、密度の凸型の歪みを示すためにスムーズな推定を行います。### 数式: McCrary検定の数式は、密度の切れ目を検出するための関数として次のように表現されます: $$\\frac{\\hat{f}_+(c) - \\hat{f}_-(c)}{\\sqrt{\\hat{V}_+ + \\hat{V}_-}}$$- $\\hat{f}_+(c)$: 処置境界点の右側での密度の推定値- $\\hat{f}_-(c)$: 処置境界点の左側での密度の推定値- $\\hat{V}_+$: 右側の密度推定値に対応する分散の推定値- $\\hat{V}_-$: 左側の密度推定値に対応する分散の推定値この式の値が有意であれば、密度に不連続があると判断されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31249325",
      "metadata": {
        "id": "31249325"
      },
      "outputs": [],
      "source": [
        "# PythonでMcCraryの検定を実行するには、まずデータを準備します。\n",
        " # 必要なライブラリをインポート\n",
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from statsmodels.nonparametric.kde import KDEUnivariate\n",
        "\n",
        "\n",
        " # デモ用のデータを生成\n",
        " np.random.seed(0)\n",
        " data = np.random.normal(loc=0, scale=1, size=1000)\n",
        " data = np.append(data, np.random.normal(loc=1.5, scale=0.3, size=100)) # 境界の右側で密度を増加させます\n",
        " print('データ:', data)\n",
        "\n",
        "\n",
        " # 境界点を設定\n",
        " cutoff = 0.5\n",
        "\n",
        "\n",
        " # 境界の両側についてカーネル密度推定（KDE）を行う\n",
        " kde_left = KDEUnivariate(data[data < cutoff])\n",
        " kde_right = KDEUnivariate(data[data >= cutoff])\n",
        " kde_left.fit(kernel='gau', bw='silverman', fft=True)\n",
        " kde_right.fit(kernel='gau', bw='silverman', fft=True)\n",
        "\n",
        "\n",
        " # 推定された密度を見てみる\n",
        " print('左側の密度の推定値:', kde_left.density)\n",
        " print('右側の密度の推定値:', kde_right.density)\n",
        "\n",
        "\n",
        " # 境界での密度の差を計算\n",
        " density_difference = kde_right.evaluate(cutoff) - kde_left.evaluate(cutoff)\n",
        " print('境界での密度の差:', density_difference)\n",
        "\n",
        "\n",
        " # 結果のプロット\n",
        " x_grid = np.linspace(-3, 3, 100)\n",
        " plt.plot(x_grid, kde_left.evaluate(x_grid), label='左側の推定密度')\n",
        " plt.plot(x_grid, kde_right.evaluate(x_grid), label='右側の推定密度')\n",
        " plt.axvline(x=cutoff, color='r', linestyle='--', label='カットオフ')\n",
        " plt.legend()\n",
        " plt.title('McCraryの検定における左右の密度推定')\n",
        " plt.xlabel('X')\n",
        " plt.ylabel('密度')\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9195d1f",
      "metadata": {
        "id": "a9195d1f"
      },
      "source": [
        "<h1 id=\"%E3%83%90%E3%83%A9%E3%83%B3%E3%82%B9%E3%83%86%E3%82%B9%E3%83%88\">バランステスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec5c4cfe",
      "metadata": {
        "id": "ec5c4cfe"
      },
      "source": [
        "バランステスト（Balance Test）は、プログラムやデータの統計分析において、ランダム化実験や観測データの処理群と対照群の間に有意差があるかどうかを確認するために使用されます。通常、t-検定などを使用して、処理群と対照群の間の差があるかどうかを検定します。バランステストは、以下のように数式で表されます。\n",
        "\n",
        "\n",
        " $$ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{s^2(\\frac{1}{n_1} + \\frac{1}{n_2})}} $$\n",
        "\n",
        "\n",
        " ここで、$\\bar{X}_1$ は処理群の平均、$\\bar{X}_2$ は対照群の平均、$s^2$ は両群の分散の加重平均、$n_1$ と $n_2$ はそれぞれ処理群と対照群のサンプルサイズです。この数式は、通常、処理群と対照群の平均の差が偶然起こったものかどうかを検定するために使われます。Pythonで学ぶ効果検証入門においては、このバランステストを用いて、実験データの偏りを確認し、適切な分析を行うための手法を学びます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff31e4a6",
      "metadata": {
        "id": "ff31e4a6"
      },
      "outputs": [],
      "source": [
        "# Pythonでバランステストを実装する\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " from scipy import stats\n",
        "\n",
        "\n",
        " # 処理群と対照群のデータ\n",
        " # ここではランダムに生成されたサンプルデータを使います\n",
        "\n",
        "\n",
        " np.random.seed(0)\n",
        "\n",
        "\n",
        " # 処理群のデータの平均（μ1 = 0.5）、標準偏差（σ = 1.0）、サイズ（n1 = 30）\n",
        " data_treatment = np.random.normal(0.5, 1.0, 30)\n",
        "\n",
        "\n",
        " # 対照群のデータの平均（μ2 = 0）、標準偏差（σ = 1.0）、サイズ（n2 = 30）\n",
        " data_control = np.random.normal(0.0, 1.0, 30)\n",
        "\n",
        "\n",
        " # 平均の算出\n",
        " mean_treatment = np.mean(data_treatment) # 処理群の平均を計算\n",
        " mean_control = np.mean(data_control) # 対照群の平均を計算\n",
        "\n",
        "\n",
        " print('処理群の平均:', mean_treatment)\n",
        " print('対照群の平均:', mean_control)\n",
        "\n",
        "\n",
        " # 分散の算出\n",
        " var_treatment = np.var(data_treatment, ddof=1) # 処理群の不偏分散を計算\n",
        " var_control = np.var(data_control, ddof=1) # 対照群の不偏分散を計算\n",
        "\n",
        "\n",
        " print('処理群の分散:', var_treatment)\n",
        " print('対照群の分散:', var_control)\n",
        "\n",
        "\n",
        " # サンプルサイズ\n",
        " n_treatment = len(data_treatment) # 処理群のサンプルサイズ\n",
        " n_control = len(data_control) # 対照群のサンプルサイズ\n",
        "\n",
        "\n",
        " print('処理群のサンプルサイズ:', n_treatment)\n",
        " print('対照群のサンプルサイズ:', n_control)\n",
        "\n",
        "\n",
        " # バランステスト（t検定）の実行\n",
        " # 仮説: 両群の平均に差がない（帰無仮説）\n",
        " # 使用モード: 両側検定\n",
        "\n",
        "\n",
        " # t検定の実行結果を取得\n",
        " # t_statistic: 検定統計量のt値\n",
        " # p_value: p値（帰無仮説が正しいと仮定したときに得られる観測データが得られる確率）\n",
        " t_statistic, p_value = stats.ttest_ind(data_treatment, data_control)\n",
        "\n",
        "\n",
        " print('検定統計量 t:', t_statistic)\n",
        " print('p値:', p_value)\n",
        "\n",
        "\n",
        " # 検定結果の解釈\n",
        " if p_value < 0.05:\n",
        "  print('対照群と処理群の間に統計的に有意な差があります。')\n",
        " else:\n",
        "  print('対照群と処理群の間に統計的に有意な差はありません。')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cf16b1",
      "metadata": {
        "id": "96cf16b1"
      },
      "source": [
        "<h1 id=\"Sharp+RDD\">Sharp RDD</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b3b868",
      "metadata": {
        "id": "52b3b868"
      },
      "source": [
        "Sharp Regression Discontinuity Design（Sharp RDD、シャープRDD）は、特定の治療または介入があるしきい値を超えた結果で一貫して提供される状況で因果効果を推定するための手法です。数式で表すと以下のようになります。\n",
        "\n",
        "\n",
        " $$ Y_i = \\alpha + \\tau D_i + \\beta X_i + \\epsilon_i $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $Y_i$ は被験者 $i$ のアウトカム変数（結果）です。\n",
        " - $\\alpha$ は$Y_i$の切片、基礎的な影響や平均値です。\n",
        " - $\\tau$ は $ au$ で表された介入の効果を示すパラメータです。\n",
        " - $D_i$ はしきい値を超えたときは1、それ以外は0で、介入が受けられたかを示す指示変数（ダミー変数）です。\n",
        " - $X_i$ は被験者 $i$ の共変量（制御変数）です。\n",
        " - $\\epsilon_i$ はランダム誤差項を示します。\n",
        "\n",
        "\n",
        " Sharp RDDの目的は、しきい値を超えるかどうかを利用して因果推論を行うことです。Pythonで学ぶ効果検証入門では、プログラミングを通じてこのような因果推論手法を学びます。この手法は、教育や社会科学の研究における政策評価や介入の影響を評価するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8003b95",
      "metadata": {
        "id": "f8003b95"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        " # サンプルデータの作成\n",
        " np.random.seed(0)\n",
        " N = 1000\n",
        " X = np.random.uniform(0, 100, size=N) # 介入変数Xを一様分布に従い生成\n",
        " print(f'X: {X[:10]}')\n",
        "\n",
        "\n",
        " # しきい値を設定\n",
        " threshold = 50\n",
        "\n",
        "\n",
        " # 介入指示変数Dの生成\n",
        " D = (X >= threshold).astype(int)\n",
        " print(f'D: {D[:10]}')\n",
        "\n",
        "\n",
        " # アウトカム変数Yの生成\n",
        " # $ au$ は介入の平均効果、 $\\epsilon$ は誤差項\n",
        " alpha = 2\n",
        " beta = 0.5\n",
        " tau = 3\n",
        " epsilon = np.random.normal(size=N)\n",
        "\n",
        "\n",
        " Y = alpha + tau * D + beta * X + epsilon # アウトカム変数Yの生成\n",
        " print(f'Y: {Y[:10]}')\n",
        "\n",
        "\n",
        " # リニア回帰モデルで介入効果を推定\n",
        " model = LinearRegression()\n",
        " model.fit(D.reshape(-1, 1), Y)\n",
        "\n",
        "\n",
        " # 介入効果の係数を出力\n",
        " print(f'推定介入効果: {model.coef_[0]}')\n",
        "\n",
        "\n",
        " # データプロット\n",
        " plt.scatter(X, Y, alpha=0.5, c=D)\n",
        " plt.axvline(x=threshold, color='r', linestyle='--')\n",
        " plt.xlabel('Running Variable\n",
        " X')\n",
        " plt.ylabel('Outcome Variable\n",
        " Y')\n",
        " plt.title('Sharp RDD Example')\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815022b7",
      "metadata": {
        "id": "815022b7"
      },
      "source": [
        "<h1 id=\"Fuzzy+RDD\">Fuzzy RDD</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d8e14c",
      "metadata": {
        "id": "90d8e14c"
      },
      "source": [
        "Fuzzy RDD（Fuzzy Regression Discontinuity Design）は、割当変数において明確な境界があるが、その境界付近で処置の導入が確率的であるような状況に対応する手法です。通常のRDD（Regression Discontinuity Design）では、処置が境界において明確に適用されるのに対し、Fuzzy RDDでは、境界での処置の適用が確率的になる点が異なります。\\n\\nこの設計では、処置の確率は割当変数が閾値に達したかどうかによって変動するモデルを考慮します。Fuzzy RDDの数式は以下のように表現されます：\\n\\n$$ E[Y_i | X_i] = \\alpha + \\tau D_i + \\beta (X_i - c) + \\epsilon_i $$\\n\\nここで：\\n\\n- $E[Y_i | X_i]$ は、割当変数 $X_i$ に条件付けた結果 $Y_i$ の期待値です。\\n- $\\alpha$ は切片で、介入効果のない場合の期待値に相当します。\\n- $\\tau$ は介入処置の効果を示します。\\n- $D_i$ は介入処置が実際に適用されたかどうかを示す変数で、$D_i = 1$ の場合は処置が適用されています。\\n- $X_i$ は割当変数で、介入の閾値 $c$ を持ちます。\\n- $\\beta (X_i - c)$ は割当変数の閾値からの距離に基づく調整項です。\\n- $\\epsilon_i$ は誤差項で、通常の誤差項の仮定が適用されます。\\n\\nFuzzy RDDを用いることで、処置のランダム化が完全でない状況でも、処置の効果を推定することが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a4989b",
      "metadata": {
        "id": "b4a4989b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        " # データを生成します\n",
        " # X: 割当変数、実際の観測データ\n",
        " np.random.seed(42) # 乱数の種を固定\n",
        " n = 100 # サンプル数\n",
        " X = np.linspace(0, 20, n)\n",
        " # c: 閾値\n",
        " c = 10\n",
        "\n",
        "\n",
        " # D: 処置の割り当て、閾値より大きければ確率的に処置を受ける\n",
        " D = np.where(X >= c, np.random.binomial(1, 0.8, n), np.random.binomial(1, 0.2, n))\n",
        "\n",
        "\n",
        " # 期待される処置効果\n",
        " true_tau = 5\n",
        "\n",
        "\n",
        " # Y: 結果変数、基礎の線形モデルとノイズで定義\n",
        " Y = 3 + true_tau * D + 0.5 * (X - c) + np.random.normal(0, 1, n)\n",
        "\n",
        "\n",
        " # 変数の出力\n",
        " print(\"X:\", X)\n",
        " print(\"D:\", D)\n",
        " print(\"Y:\", Y)\n",
        "\n",
        "\n",
        " # データのプロット\n",
        " plt.scatter(X, Y, c=D, cmap='bwr', alpha=0.6, label='Data')\n",
        " plt.axvline(c, color='grey', linestyle='--', label='Threshold (c)')\n",
        " plt.xlabel('Assignment Variable X')\n",
        " plt.ylabel('Outcome Variable Y')\n",
        " plt.title('Fuzzy RDD Example')\n",
        " plt.legend()\n",
        " plt.show()\n",
        "\n",
        "\n",
        " # フィッティング\n",
        " # 処置を考慮した線形モデルのフィッティング\n",
        " X_reshaped = X.reshape(-1, 1) # 2次元配列に変形\n",
        " model = LinearRegression().fit(np.hstack((X_reshaped, D.reshape(-1, 1))), Y)\n",
        "\n",
        "\n",
        " # 結果の表示\n",
        " alpha_hat = model.intercept_\n",
        " tau_hat = model.coef_[1]\n",
        " beta_hat = model.coef_[0]\n",
        "\n",
        "\n",
        " print(f\"Estimated alpha (intercept): {alpha_hat}\")\n",
        " print(f\"Estimated tau (treatment effect): {tau_hat}\")\n",
        " print(f\"Estimated beta (slope): {beta_hat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29b5bf4",
      "metadata": {
        "id": "d29b5bf4"
      },
      "source": [
        "<h1 id=\"as-if+%E6%9D%A1%E4%BB%B6\">as-if 条件</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dade32fe",
      "metadata": {
        "id": "dade32fe"
      },
      "source": [
        "As-if条件は効果検証の際に特定の条件下での結果を推定するための仮定です。この仮定は、特定の処置や介入が行われた群と行われていない群において、それらが 'as if' 同じであったかのように扱うことができることを意味します。数学的には、特定の削減効果や処置の無相関性が仮定されます。\n",
        "\n",
        "\n",
        " $$ E[Y(0) | T=1] = E[Y(0) | T=0] $$\n",
        "\n",
        "\n",
        " この式は、処置群 $T=1$ と対照群 $T=0$ の介入なしの潜在結果 $Y(0)$ の平均が等しいことを示します。ここでの $E$ は期待値を表し、$Y(0)$ は介入が行われなかった場合の結果変数を示します。また、$T$ は処置または介入を意味し、$T=1$ は処置が行われたことを、$T=0$ は行われていないことを示します。\n",
        "\n",
        "\n",
        " Pythonで学ぶ効果検証入門では、これらの仮定を用いて、実際のデータで処置の効果を統計的に推定するための方法を学ぶことができます。as-if条件は、このような推定が正確で有効であるための重要な前提条件の一つです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73a6fbb",
      "metadata": {
        "id": "c73a6fbb"
      },
      "outputs": [],
      "source": [
        "# Pythonでは、データを用いてas-if条件を検証するために、回帰やマッチングなどの手法を使います。\n",
        " import numpy as np\n",
        " import pandas as pd\n",
        "\n",
        "\n",
        " # サンプルデータを生成します。\n",
        " data = {\n",
        "  'treatment': np.random.randint(0, 2, 100),  # 0または1の処置変数\n",
        "  'outcome': np.random.normal(0, 1, 100)  # 正規分布に従う結果変数\n",
        " }\n",
        " df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        " # 処置群と対照群の平均を比較してみます。\n",
        " treatment_group = df[df['treatment'] == 1]['outcome']\n",
        " control_group = df[df['treatment'] == 0]['outcome']\n",
        "\n",
        "\n",
        " # 処置群の平均値を出力します。\n",
        " print('Treatment group mean:', treatment_group.mean())\n",
        "\n",
        "\n",
        " # 対照群の平均値を出力します。\n",
        " print('Control group mean:', control_group.mean())\n",
        "\n",
        "\n",
        " # このようにして、実際に得られたデータの処置および対照群の平均結果を比較します。\n",
        " # マッチングや回帰を用いる場合もありますが、基本はサンプル内のas-if条件を見つけることが重要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a70cdb7",
      "metadata": {
        "id": "3a70cdb7"
      },
      "source": [
        "<h1 id=\"%E5%AF%86%E5%BA%A6%E9%96%A2%E6%95%B0\">密度関数</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d3b5ce",
      "metadata": {
        "id": "f6d3b5ce"
      },
      "source": [
        "密度関数は、ある確率分布において特定の値が取られる確率の密度を表す関数です。連続型の確率分布では、確率密度関数 (Probability Density Function, PDF) として表現されます。連続確率変数 $X$ の確率密度関数 $f(x)$ について、任意の $a \\leq x \\leq b$ に対して\n",
        " $$\n",
        " P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\n",
        " $$\n",
        " として定義され、ここで $P$ は確率を表し、$\\int$ は積分記号を表します。$a$ と $b$ は積分の範囲で、$f(x)$ は密度関数そのものです。この関数が示すのは、確率変数がある範囲にある確率です。\n",
        "\n",
        "\n",
        " また、密度関数は常に正で、密度関数全体を積分すると1になります。これが、全体の確率が1であるとする基本ルールに一致します。\n",
        "\n",
        "\n",
        " This is a key component when learning about effect measurement in Python, as described in \"Pythonで学ぶ効果検証入門\". Here, understanding probability density functions can help in modeling real-world data and in performing statistical inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf8eb14",
      "metadata": {
        "id": "abf8eb14"
      },
      "outputs": [],
      "source": [
        "# 密度関数をPythonで実装\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from scipy.stats import norm\n",
        "\n",
        "\n",
        " # 正規分布の平均(mean)と標準偏差(std)を設定\n",
        " mu, sigma = 0, 1\n",
        "\n",
        "\n",
        " # xの範囲を設定。ここでは-3から3までの範囲を0.01刻みで設定\n",
        " x = np.arange(-3, 3, 0.01)\n",
        " print(x)\n",
        "\n",
        "\n",
        " # scipyのnorm.pdfを用いて正規分布の確率密度関数を計算\n",
        " pdf = norm.pdf(x, mu, sigma)\n",
        " print(pdf)\n",
        "\n",
        "\n",
        " # 確率密度関数をプロット\n",
        " gf = plt.figure()\n",
        " plt.plot(x, pdf, label='PDF')\n",
        " plt.title('Normal Distribution PDF')\n",
        " plt.xlabel('x')\n",
        " plt.ylabel('Density')\n",
        " plt.grid(True)\n",
        " plt.show()\n",
        "\n",
        "\n",
        " # このコードスニペットでは、\n",
        " # 1. numpyで0.01間隔のx軸の値を生成し、配列に設定しています。\n",
        " # 2. scipyライブラリのnorm.pdfを使用してこれらの値に対する正規分布の確率密度を計算しています。\n",
        " # 3. matplotlibを用いて、計算した確率密度をプロットし、その分布を視覚化しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384f0a0f",
      "metadata": {
        "id": "384f0a0f"
      },
      "source": [
        "<h1 id=\"diagnostic+tests\">diagnostic tests</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00d145f",
      "metadata": {
        "id": "b00d145f"
      },
      "source": [
        "効果検証における診断テスト（diagnostic tests）は、ある条件が満たされているかどうかを確認するための手法の一つです。これらのテストの主な目的は、データの信頼性とモデルの適切性を確保することです。通常、これには統計モデルの仮定の検証が含まれます。特に、回帰分析やその他のモデリング手法において、残差の正規性、ホモスケダスティシティ、独立性などを検証することが一般的です。\n",
        "\n",
        "\n",
        " 理論的には、diagnostic testsはいくつかの統計的な検定に基づいています。例えば、ホモスケダスティシティ（等分散性）を検証するためのBreusch-Pagan検定や、データの正規性を検証するためのShapiro-Wilk検定などです。\n",
        "\n",
        "\n",
        " 以下にBreusch-Pagan検定の理論を数式で示します。Breusch-Pagan検定は、次のように$\n",
        "\n",
        "\n",
        " R^2\n",
        "\n",
        "\n",
        " $（決定係数）を用いて表現されます：\n",
        "\n",
        "\n",
        " $$\n",
        "\n",
        "\n",
        " BP = n \\cdot R^2\n",
        "\n",
        "\n",
        " $$\n",
        "\n",
        "\n",
        " ここで、$BP$はBreusch-Paganの検定統計量、$n$はサンプルサイズ、$R^2$は余剰分散を独立変数の関数として回帰したときの決定係数です。 $R^2$はモデルがどの程度のデータの変動を説明できるかを示す指標です。\n",
        "\n",
        "\n",
        " この統計量は$\n",
        "\n",
        "\n",
        " \\chi^2\n",
        "\n",
        "\n",
        " $分布に従います。この事実を利用して、$BP$がある閾値を超えるかどうかによって、ホモスケダスティシティ（等分散性）の仮定が棄却されるかを判断します。\n",
        "\n",
        "\n",
        " 次にPythonでこれを実装するコードを示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb644fc",
      "metadata": {
        "id": "5cb644fc"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします。\n",
        " import numpy as np\n",
        " import statsmodels.api as sm\n",
        " from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "\n",
        " # ダミーデータを生成します。\n",
        " np.random.seed(0) # 再現性のためにシードを設定\n",
        " X = np.random.rand(100, 2) # 100 x 2 のランダムデータを生成\n",
        " beta = np.array([1, 0.5]) # 真のパラメータを設定\n",
        " epsilon = np.random.normal(size=100) # 正規分布に従う誤差項を生成\n",
        " y = X @ beta + epsilon # 線形モデルのデータを生成\n",
        "\n",
        "\n",
        " # 変数の内容を確認します。\n",
        " print(\"X:\", X[:5]) # Xの最初の5行を出力\n",
        " y = sm.add_constant(y) # 切片を追加\n",
        "\n",
        "\n",
        " print(\"被説明変数y:\", y[:5]) # 被説明変数yの最初の5行を出力\n",
        "\n",
        "\n",
        " # モデルをフィットします。\n",
        " model = sm.OLS(y, X) # OLS回帰モデルを定義\n",
        " results = model.fit() # モデルをフィッティング\n",
        "\n",
        "\n",
        " # Breusch-Pagan検定を実行します。\n",
        " # 残差と独立変数で検定を行います。\n",
        " bptest = het_breuschpagan(results.resid, results.model.exog)\n",
        "\n",
        "\n",
        " # 検定統計量やp値を出力します。\n",
        " print('BP統計量:', bptest[0]) # Breusch-Pagan検定統計量\n",
        " print('自由度:', bptest[2]) # 検定の自由度\n",
        " print('p値:', bptest[1]) # 検定のp値\n",
        "\n",
        "\n",
        " # 結果を解釈します。\n",
        " if bptest[1] < 0.05:\n",
        "  print('帰無仮説を棄却します。ホモスケダスティシティがない可能性があります。')\n",
        " else:\n",
        "  print('帰無仮説を棄却できません。ホモスケダスティシティがあります。')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd7192d",
      "metadata": {
        "id": "bcd7192d"
      },
      "source": [
        "<h1 id=\"%E5%87%A6%E7%BD%AE%E7%A2%BA%E7%8E%87\">処置確率</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42362cd",
      "metadata": {
        "id": "e42362cd"
      },
      "source": [
        "### 処置確率（Propensity Score）の解説\n",
        "\n",
        "\n",
        " 処置確率とは、ある個体が特定の処置を受ける確率のことを指します。これは、よく因果推論において用いられるコンセプトで、ランダム化試験を模倣するための手法の一つです。\n",
        "\n",
        "\n",
        " Propensity Scoreは数式で以下のように表現されます:\n",
        "\n",
        "\n",
        " $$ e(x) = P(T = 1 | X = x) $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $e(x)$ は処置確率（Propensity Score）です。\n",
        " - $P(T = 1 | X = x)$ は、特徴量ベクトル $X = x$ において、処置 $T = 1$ となる条件付き確率を表します。\n",
        "\n",
        "\n",
        " このスコアは、推定において補正を行うために使われ、主にマッチングやストラティフィケーション、逆確率重み付け法などに活用されます。処置効果の正確な推定を目的としています。\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』においても、Propensity Scoreは重要な役割を果たし、データにおける偏りを調整するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562230b3",
      "metadata": {
        "id": "562230b3"
      },
      "outputs": [],
      "source": [
        "# Propensity Scoreの計算をPythonで実装\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " import pandas as pd\n",
        " from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        " # ダミーデータの生成\n",
        " np.random.seed(0)\n",
        " X = np.random.normal(size=(100, 5)) # 特徴量データ（100行5列）\n",
        " T = np.random.binomial(1, 0.5, size=100) # 処置のデータ（0または1）\n",
        "\n",
        "\n",
        " # 処置のデータの確認\n",
        " print(\"特徴量データ（X）:\", X)\n",
        " print(\"処置データ（T）:\", T)\n",
        "\n",
        "\n",
        " # ロジスティック回帰モデルの構築\n",
        " model = LogisticRegression()\n",
        "\n",
        "\n",
        " # 処置データを予測するために特徴量データに基づいてモデルを適合させる\n",
        " model.fit(X, T) # 特徴量Xから処置Tを予測するためのモデルを適合させる\n",
        "\n",
        "\n",
        " # 処置確率の予測\n",
        " propensity_scores = model.predict_proba(X)[:, 1] # 処置1の確率を取得\n",
        "\n",
        "\n",
        " # 処置確率の出力\n",
        " print(\"処置確率:\", propensity_scores)\n",
        "\n",
        "\n",
        " # 処置確率は、特徴量が与えられたときに処置が行われる確率を表し、\n",
        " # 因果推論における偏りを調整するために使用される。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99ed688",
      "metadata": {
        "id": "b99ed688"
      },
      "source": [
        "<h1 id=\"%E5%8F%B3%E5%81%B4%E6%A5%B5%E9%99%90\">右側極限</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8947233",
      "metadata": {
        "id": "a8947233"
      },
      "source": [
        "右側極限は、ある点における関数の挙動を、その点の右側（あるいは正の方向）から接近することによって調べる概念です。数学的には、次のように表現されます。\n",
        " $$ \\lim_{x \\to a^+} f(x) $$\n",
        " ここで、$\\lim$は「極限」を示します。$x \\to a^+$は、「$x$が$a$に右側から（または正の方向から）近づく」ということを意味しています。$f(x)$は$x$の関数です。\n",
        "\n",
        "\n",
        " 右側極限は、特に不連続な関数や区分線形関数の挙動を理解する際に重要であり、関数が点$a$で右側からどのように振る舞うかを示します。例えば、Pythonで学ぶ効果検証分野では、挙動の異なるデータポイントの変化（例：介入効果の開始点）を調べるのに役立つことがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eddca57",
      "metadata": {
        "id": "3eddca57"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 関数f(x)を定義します\n",
        " def f(x):\n",
        "  return np.where(x < 1, 2*x, 3*x + 1) # xが1未満のときは2x、それ以外は3x+1を返す関数\n",
        "\n",
        "\n",
        " # xが1に右から近づくときのf(x)の様子を観察します\n",
        " x_values = np.linspace(1, 2, 100, endpoint=False) # 1から2までの範囲で1に近づく100異なるx値を生成\n",
        " print(x_values) # 生成されたx値を出力します\n",
        "\n",
        "\n",
        " # 各xに対するf(x)を計算します\n",
        " y_values = f(x_values)\n",
        " print(y_values) # 計算されたy値を出力します\n",
        "\n",
        "\n",
        " # 極限値を近似します\n",
        " right_limit = y_values[0] # 1に右から最も近い点でのf(x)の値を取る\n",
        " print('右側極限の近似値:', right_limit) # 極限の近似値を出力します"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0410844",
      "metadata": {
        "id": "f0410844"
      },
      "source": [
        "<h1 id=\"%E5%B7%A6%E5%81%B4%E6%A5%B5%E9%99%90\">左側極限</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb3369d",
      "metadata": {
        "id": "9bb3369d"
      },
      "source": [
        "### 左側極限の解説\n",
        "\n",
        "\n",
        " 左側極限とは、ある関数がある点においてその点の直前でどのような挙動を見せるかを測るための概念です。数式で示すと次のようになります：\n",
        "\n",
        "\n",
        " $$ \\lim_{{x \\to a^-}} f(x) $$\n",
        "\n",
        "\n",
        " ここで、$\\lim$は極限を示し、$x \\to a^-$は$x$が$a$に近づくが$a$より小さい値から近づくことを示しています。\n",
        "\n",
        "\n",
        " この理論は多くの応用に用いることができ、特に不連続な点を持つ関数の振る舞いを理解するために重要です。\n",
        " 'Pythonで学ぶ効果検証入門'においては、左側極限を活用することでデータ分析や予測モデルの挙動をより詳しく解析する手段として使うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f225ff58",
      "metadata": {
        "id": "f225ff58"
      },
      "outputs": [],
      "source": [
        "# 左側極限を計算するPythonコード\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " # 定義される関数 f(x)\n",
        " def f(x):\n",
        "  return x ** 2 # 二乗関数として定義\n",
        "\n",
        "\n",
        " # xがaより小さいところから近づくような範囲を設定\n",
        " a = 1.0 # 計算したい点を1に設定\n",
        " x_values = np.linspace(a - 0.1, a, 100, endpoint=False) # aの左側で徐々に接近\n",
        "\n",
        "\n",
        " # 左側極限の近似値を求める\n",
        " left_limit = np.mean([f(x) for x in x_values])\n",
        "\n",
        "\n",
        " # 結果を出力\n",
        " print(\"x_values:\", x_values) # xの値を出力\n",
        " print(\"left_limit:\", left_limit) # 左側極限の近似値を出力\n",
        "\n",
        "\n",
        " # 関数 f(x) は与えられた x に対して x^2 を返す関数\n",
        " # ここでは f(x) を a よりすこし小さい範囲で計算し、左側極限を求めます\n",
        " # x_values は a の左側に詰まった点のリストを生成しています\n",
        " # left_limit はそれらの点に対して f(x) の平均を取ることで左側極限を近似的に求めています"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f61c34",
      "metadata": {
        "id": "94f61c34"
      },
      "source": [
        "<h1 id=\"%E5%B1%80%E6%89%80%E5%A4%9A%E9%A0%85%E5%BC%8F%E5%9B%9E%E5%B8%B0\">局所多項式回帰</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85e7b8d",
      "metadata": {
        "id": "e85e7b8d"
      },
      "source": [
        "### 局所多項式回帰 (Local Polynomial Regression)\n",
        "\n",
        "\n",
        " 局所多項式回帰は、回帰分析の一種で、特に非線形なデータのトレンドを滑らかに近似するために使用されます。この手法は、指定した点付近のデータに重みを与え、その部分だけで多項式回帰を行うことで、全体の関係性を滑らかに捉えます。\n",
        "\n",
        "\n",
        " 局所多項式回帰の数式は次のように表されます。局所多項式回帰では、重み付きの最小二乗法を使って近似関数 $\\hat{f}(x)$ を求めます。\n",
        "\n",
        "\n",
        " $$\n",
        " \\hat{f}(x) = \\sum_{i=1}^{n} W(x, x_i) \\cdot (\\alpha_0 + \\alpha_1 (x_i - x) + \\alpha_2 (x_i - x)^2 + \\ldots )\n",
        " $$\n",
        "\n",
        "\n",
        " ここで、\n",
        " - $n$ はデータポイントの数。\n",
        " - $W(x, x_i)$ は基準点 $x$ の周りでのデータポイント $x_i$ への重み。\n",
        " - $\\alpha_0, \\alpha_1, \\alpha_2, \\ldots$ は回帰係数。\n",
        "\n",
        "\n",
        " ### 記号の解説\n",
        " - $x$: 回帰を行う基準点。\n",
        " - $x_i$: データポイント。\n",
        " - $W(x, x_i)$: 重み関数。基準点 $x$ の周りの $x_i$ に依存した重みを付けます。一般的にはガウスカーネルを用いることが多いです。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        " 局所多項式回帰は、データの非線形性を捉えるのに有効で、特に変化が激しいデータのスムージングに適しています。\n",
        "\n",
        "\n",
        " ### Pythonで学ぶ効果検証入門との関係性\n",
        " この手法は因果推論においても用いられ、例えば政策の効果を滑らかに推定する際に使用されることがあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac45b2",
      "metadata": {
        "id": "1dac45b2"
      },
      "source": [
        "<h1 id=\"%E5%B1%80%E6%89%80%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0\">局所線形回帰</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ea9aa2",
      "metadata": {
        "id": "67ea9aa2"
      },
      "source": [
        "### 局所線形回帰の解説\n",
        "\n",
        "\n",
        " 局所線形回帰（Local Linear Regression）は、データの局所的な領域で線形のモデルを用いて回帰を行う手法です。非線形なデータに対して、データ全体をカバーする1つの線形モデルを用いる代わりに、各データポイントの近傍で線形回帰を行います。\n",
        "\n",
        "\n",
        " #### 局所線形回帰の数式\n",
        "\n",
        "\n",
        " 局所線形回帰では、任意の点 $x_0$ における推定値 $\\hat{y}_0$ は次の数式で表されます。\n",
        "\n",
        "\n",
        " $$\n",
        " \\hat{y}_0 = \\alpha + \\beta(x_0 - x) = \\sum_{i=1}^{n}{w_i(y_i - (\\alpha + \\beta(x_0 - x_i)))}\n",
        " $$\n",
        "\n",
        "\n",
        " - $w_i$: 各データ点 $x_i$ に対する重み。通常はカーネル関数を用いて計算され、近傍の点がより高い重みを持ちます。\n",
        " - $y_i$: データ点 $x_i$ に対する観測値。\n",
        " - $lpha, eta$: 切片と傾きで、推定したいパラメータです。\n",
        "\n",
        "\n",
        " ### 使用用途\n",
        "\n",
        "\n",
        " 局所線形回帰は次のような場合に有用です：\n",
        " - データの局所的な構造を捉える必要があるとき。\n",
        " - データの非線形性が強く、全体を統一した線形モデルが不適切なとき。\n",
        "\n",
        "\n",
        " この手法は、局所的な構造を詳細に捉えることで、より精緻な機械学習モデルを構築するための前処理としても利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec00a711",
      "metadata": {
        "id": "ec00a711"
      },
      "outputs": [],
      "source": [
        "# 局所線形回帰をPythonで実装するには、numpyやscikit-learnなどのライブラリを使用します。\n",
        "\n",
        "\n",
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from sklearn.neighbors import KernelDensity\n",
        "\n",
        "\n",
        " # サンプルデータを生成します。\n",
        " np.random.seed(0)\n",
        " X = np.random.rand(100) * 10 # 0から10までの一様乱数\n",
        " Y = np.sin(X) + np.random.randn(100) * 0.1 # sin関数にノイズを加えたもの\n",
        "\n",
        "\n",
        " # データを確認します。\n",
        " print(\"X:\", X)\n",
        " print(\"Y:\", Y)\n",
        "\n",
        "\n",
        " # カーネル密度推定を用いて、各点での局所的な重みを計算します。\n",
        " kde = KernelDensity(kernel='gaussian', bandwidth=1.0).fit(X[:, np.newaxis]) # Xを2D配列に変換\n",
        "\n",
        "\n",
        " # 評価点を生成します。\n",
        " x_eval = np.linspace(0, 10, 1000)\n",
        "\n",
        "\n",
        " # 各評価点での重みを計算します。\n",
        " log_dens = kde.score_samples(x_eval[:, np.newaxis])\n",
        " weights = np.exp(log_dens)\n",
        "\n",
        "\n",
        " # 重みを確認します。\n",
        " print(\"weights:\", weights)\n",
        "\n",
        "\n",
        " # 各評価点での局所線形回帰を実行します。\n",
        " y_eval = np.zeros_like(x_eval)\n",
        " for i, x0 in enumerate(x_eval):\n",
        "  # 重みを使って線形回帰をフィット\n",
        "  W = np.diag(weights[i] * np.ones_like(X)) # 対角行列にする\n",
        "  X_with_intercept = np.vstack([np.ones_like(X), X]).T # 切片用に1を追加\n",
        "  # 通常の線形回帰の最小二乗法で解を求める\n",
        "  beta = np.linalg.inv(X_with_intercept.T @ W @ X_with_intercept) @ (X_with_intercept.T @ W @ Y)\n",
        "  # x0 における予測値を計算\n",
        "  y_eval[i] = beta[0] + beta[1] * x0\n",
        "\n",
        "\n",
        " # 推定されたy_evalを確認します。\n",
        " print(\"y_eval:\", y_eval)\n",
        "\n",
        "\n",
        " # 結果をプロットします。\n",
        " plt.scatter(X, Y, c='blue', label='Data')\n",
        " plt.plot(x_eval, y_eval, c='red', label='Local Linear Regression')\n",
        " plt.xlabel('X')\n",
        " plt.ylabel('Y')\n",
        " plt.title('局所線形回帰の結果')\n",
        " plt.legend()\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536433fd",
      "metadata": {
        "id": "536433fd"
      },
      "source": [
        "<h1 id=\"%E7%B7%9A%E5%BD%A2%E3%83%A2%E3%83%87%E3%83%AB\">線形モデル</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecce5ac9",
      "metadata": {
        "id": "ecce5ac9"
      },
      "source": [
        "線形モデル（例えば、単回帰分析）は、予測変数（独立変数）と目的変数（従属変数）との間の関係を記述するための統計的手法です。線形モデルは次のように数式で表されます：\n",
        "\n",
        "\n",
        " $$y = eta_0 + eta_1 x + \\epsilon$$\n",
        "\n",
        "\n",
        " ここで：\n",
        " - $y$ は目的変数（従属変数）\n",
        " - $eta_0$ は切片（定数項）\n",
        " - $eta_1$ は傾き（回帰係数）\n",
        " - $x$ は予測変数（独立変数）\n",
        " - $\\epsilon$ は誤差項です。\n",
        "\n",
        "\n",
        " 線形モデルはデータセットが直線的な関係を持つ場合に使用され、一般的には回帰分析の分野で使われます。例えば、Pythonを使った効果検証では、変数間の関係をモデル化し予測するために使われます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9e35e0",
      "metadata": {
        "id": "1b9e35e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        " # ダミーデータの作成（独立変数 x と従属変数 y）\n",
        " x = np.array([1, 2, 3, 4, 5]) # 独立変数\n",
        " print('独立変数 x:', x) # xの中身の確認\n",
        " y = np.array([2, 4, 5, 4, 5]) # 従属変数\n",
        " print('従属変数 y:', y) # yの中身の確認\n",
        "\n",
        "\n",
        " # 2Dの配列に変換（scikit-learnが望む形式）\n",
        " x = x.reshape(-1, 1)\n",
        "\n",
        "\n",
        " # 線形回帰モデルの初期化\n",
        " model = LinearRegression()\n",
        "\n",
        "\n",
        " # モデルの学習\n",
        " model.fit(x, y)\n",
        "\n",
        "\n",
        " # 傾き（回帰係数）と切片を取得\n",
        " beta_1 = model.coef_[0]\n",
        " print('回帰係数 β1:', beta_1) # β1の確認\n",
        " beta_0 = model.intercept_\n",
        " print('切片 β0:', beta_0) # β0の確認\n",
        "\n",
        "\n",
        " # 学習データに対する予測\n",
        " y_pred = model.predict(x)\n",
        " print('予測値 y_pred:', y_pred) # y_predの確認\n",
        "\n",
        "\n",
        " # 散布図と回帰直線の描画\n",
        " plt.scatter(x, y, color='blue', label='散布図')\n",
        " plt.plot(x, y_pred, color='red', label='回帰直線')\n",
        " plt.xlabel('独立変数 x')\n",
        " plt.ylabel('従属変数 y')\n",
        " plt.legend()\n",
        " plt.title('線形回帰モデルによる予測')\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d129a6be",
      "metadata": {
        "id": "d129a6be"
      },
      "source": [
        "<h1 id=\"%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9\">バリアンス</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a82fc14",
      "metadata": {
        "id": "8a82fc14"
      },
      "source": [
        "### バリアンスについての解説\n",
        "\n",
        "\n",
        " バリアンス（分散）は、データのばらつきを表す指標の一つであり、統計や機械学習の分野で頻繁に利用されます。データが平均値からどの程度離れているのかを表す尺度として役立ちます。\n",
        "\n",
        "\n",
        " **バリアンスの数式**\n",
        "\n",
        "\n",
        " バリアンスは以下の数式で計算されます：\n",
        "\n",
        "\n",
        " $$ \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n",
        "\n",
        "\n",
        " この数式の各記号の解説:\n",
        " - $\\sigma^2$: バリアンスを表します。\n",
        " - $N$: データセットの総数を表します。\n",
        " - $x_i$: 個々のデータ点を表します。\n",
        " - $\\mu$: データセットの平均値を表します。\n",
        " - $\\sum$: シグマ記号で和を計算することを示します。\n",
        "\n",
        "\n",
        " **バリアンスの使用用途**\n",
        " - データのばらつきを評価する際に使用されます。\n",
        " - モデルの予測精度の指標としても使われる場合があります。\n",
        " - 機械学習モデルのハイパーパラメータの最適化において、過学習と未学習の評価にも利用されます。\n",
        "\n",
        "\n",
        " **Pythonで学ぶ効果検証入門との関係性**\n",
        "\n",
        "\n",
        " この書籍では、バリアンスを中心とした統計検定や効果量の測定について実践的なコード例を提供し、バリアンスがどのように実際のデータ解析で利用されるかを解説しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f42ffe",
      "metadata": {
        "id": "c0f42ffe"
      },
      "outputs": [],
      "source": [
        "# データセット\n",
        " import numpy as np\n",
        "\n",
        "\n",
        " data = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "\n",
        "\n",
        " # データセットの総数 N を計算\n",
        " N = len(data)\n",
        " print(f'N: {N}') # N: データセットの総数\n",
        "\n",
        "\n",
        " # データセットの平均値 μ を計算\n",
        " mean = np.mean(data)\n",
        " print(f'mean (μ): {mean}') # mean: データセットの平均 μ\n",
        "\n",
        "\n",
        " # バリアンス σ^2 を計算\n",
        " variance = np.sum((np.array(data) - mean) ** 2) / N\n",
        " print(f'variance (σ^2): {variance}') # variance: データセットのバリアンス σ^2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705c4955",
      "metadata": {
        "id": "705c4955"
      },
      "source": [
        "<h1 id=\"%E6%BD%9C%E5%9C%A8%E3%82%B9%E3%82%B3%E3%82%A2\">潜在スコア</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eea8f4a",
      "metadata": {
        "id": "1eea8f4a"
      },
      "source": [
        "潜在スコア（Propensity Score）は、観察研究において処置群と対照群のバランスを取るために使われる統計的手法です。これにより、観察データを用いて因果関係を推定する際のバイアスを減らすことができます。潜在スコアは、処置を受ける確率をモデル化し、そのスコアを基にマッチングや重み付けを行います。\n",
        "\n",
        "\n",
        " 潜在スコアは数式で以下のように表現されます。\n",
        "\n",
        "\n",
        " $$ e(x) = P(T = 1 \\mid X = x) $$\n",
        "\n",
        "\n",
        " ここで、$e(x)$ は潜在スコアを示し、$P(T = 1 \\mid X = x)$ は共変量 $x$ に基づいて処置を受ける確率です。\n",
        " - $T$ : 処置変数（$T=1$ は処置群、$T=0$ は対照群を示します。）\n",
        " - $X$ : 共変量ベクトル。\n",
        "\n",
        "\n",
        " この手法を用いることで、処置の効果をより適切に推定できます。例えば、医療データにおける治療効果の検証や、マーケティングデータにおけるキャンペーン効果の測定などで使われます。\n",
        "\n",
        "\n",
        " 『Pythonで学ぶ効果検証入門』では、このような因果推論の手法が実践的な例とともに紹介されています。潜在スコアの概念は、その中でも重要な要素の一つとして取り扱われています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70b6521",
      "metadata": {
        "id": "a70b6521"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        " import numpy as np\n",
        " from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        " # ダミーデータの作成\n",
        " np.random.seed(0) # 乱数シードの設定\n",
        " n = 100 # データ数\n",
        " X = np.random.normal(size=(n, 2)) # 共変量としての2次元データを生成\n",
        " T = (np.random.rand(n) < 0.5).astype(int) # 0または1の処置を適当に設定\n",
        "\n",
        "\n",
        " # 共変量と処置変数を表示\n",
        " print(\"共変量 X:\\n\", X)\n",
        " print(\"処置変数 T:\\n\", T)\n",
        "\n",
        "\n",
        " # ロジスティック回帰モデルの初期化\n",
        " model = LogisticRegression()\n",
        "\n",
        "\n",
        " # ロジスティック回帰を用いた潜在スコアの推定\n",
        " model.fit(X, T) # モデルの学習\n",
        " propensity_score = model.predict_proba(X)[:, 1] # 潜在スコアの取得（処置群の確率）\n",
        "\n",
        "\n",
        " # 潜在スコアの表示\n",
        " print(\"潜在スコア e(x):\\n\", propensity_score)\n",
        "\n",
        "\n",
        " # このコードでは、2次元の共変量 X を用いて、処置変数 T をロジスティック回帰モデルで推定します。\n",
        " # sklearn の LogisticRegression を用いて、各サンプルが処置を受ける確率（潜在スコア）を計算します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff8051d",
      "metadata": {
        "id": "2ff8051d"
      },
      "source": [
        "<h1 id=\"%E5%85%B1%E5%A4%89%E9%87%8F%E3%81%AE%E3%83%90%E3%83%A9%E3%83%B3%E3%82%B9%E3%83%86%E3%82%B9%E3%83%88\">共変量のバランステスト</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb98376",
      "metadata": {
        "id": "9bb98376"
      },
      "source": [
        "### 共変量のバランステスト\n",
        "\n",
        "\n",
        " 共変量のバランステストは、処理群と対照群の間で共変量（例えば、年齢、性別、収入などの観測可能な特徴量）の分布が類似しているかどうかを統計的に検定する方法です。これを確認することで、処置や介入の効果がその共変量の違いによるものではなく、実際の処置によるものであると判断できます。\n",
        "\n",
        "\n",
        " 数式としては、平均値の差を検定する手段により、例えば次のように表現できます。\n",
        "\n",
        "\n",
        " $$\n",
        " H_0: \\mu_{トリートメント} = \\mu_{コントロール}\n",
        " $$\n",
        "\n",
        "\n",
        " ここで、$H_0$ は帰無仮説を表し、$\\mu_{トリートメント}$ および $\\mu_{コントロール}$ はそれぞれ処理群と対照群の共変量の平均を表します。帰無仮説 $H_0$ の下で、有意な差がないことを示します。\n",
        "\n",
        "\n",
        " このテストの具体的な実施方法にはt検定やカイ二乗検定などがあります。この手法は、処理と結果の関係性が共変量に依存しないことを確認するために使用されます。\n",
        "\n",
        "\n",
        " Pythonで実装する際は、ライブラリ `scipy` や `statsmodels` を使うことが一般的です。また、このようなバランステストは「Pythonで学ぶ効果検証入門」でも取り扱われており、介入の前後でのデータの正当性を担保する際に重要なステップと言えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1957c724",
      "metadata": {
        "id": "1957c724"
      },
      "outputs": [],
      "source": [
        "# Pythonで共変量のバランステストをする際のコード例\n",
        "\n",
        "\n",
        " import numpy as np # 数値計算用ライブラリをインポート\n",
        " from scipy import stats # 統計的な計算のためにscipyからstatsをインポート\n",
        "\n",
        "\n",
        " # サンプルデータの生成：トリートメント群とコントロール群\n",
        " np.random.seed(0) # ランダムシードを設定して結果を再現可能に\n",
        "\n",
        "\n",
        " # トリートメント群の共変量データ（例：年齢）を正規分布に従って生成\n",
        " covariate_treatment = np.random.normal(loc=30, scale=5, size=100)\n",
        "\n",
        "\n",
        " # コントロール群の共変量データを生成\n",
        " covariate_control = np.random.normal(loc=32, scale=5, size=100)\n",
        "\n",
        "\n",
        " # トリートメント群のデータを出力して確認\n",
        " print(\"トリートメント群の共変量データ:\", covariate_treatment)\n",
        "\n",
        "\n",
        " # コントロール群のデータを出力して確認\n",
        " print(\"コントロール群の共変量データ:\", covariate_control)\n",
        "\n",
        "\n",
        " # トリートメント群とコントロール群で平均値に有意な差があるかをt検定で確認\n",
        " result = stats.ttest_ind(covariate_treatment, covariate_control)\n",
        "\n",
        "\n",
        " # t検定の結果を出力して確認（statistic：t値, pvalue：p値）\n",
        " print(\"t検定の結果:\", result)\n",
        "\n",
        "\n",
        " # p値を出力して有意水準（通常0.05）で帰無仮説を棄却するか確認\n",
        " print(\"p値:\", result.pvalue)\n",
        "\n",
        "\n",
        " # もしp値が0.05未満なら有意な差があり、共変量のバランスが取れていないと判断されるます。逆に0.05以上なら\n",
        " # バランスは取れており、共変量による影響は最小限であると判断できます。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}