{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koheikobayashi/machine-learning/blob/main/fixed_generated_notebook_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703b7ddd",
      "metadata": {
        "id": "703b7ddd"
      },
      "source": [
        "### Gini Impurityの解説\n",
        "\n",
        "Gini Impurity（ジニ不純度）は、決定木やランダムフォレストといった機械学習アルゴリズムで使用される不純度指標の一つです。この指標は、あるノードにおけるデータの不純度（分割の質）を測定します。具体的には、データの分散をどれだけ減らすことができたかを評価します。\n",
        "\n",
        "インライン数式としては、$Gini(t) = 1 - \\sum_{i=1}^{n} p_i^2$という形で表現されます。\n",
        "\n",
        "より詳細な数式を用いると以下のようになります：\n",
        "\n",
        "$$\n",
        "Gini(t) = 1 - \\sum_{i=1}^{n} (p_i)^2\n",
        "$$\n",
        "\n",
        "ここで、$p_i$はクラス$i$の確率（そのノードにおけるクラス$i$のサンプルの割合）です。\n",
        "\n",
        "ジニ不純度は0から1の間の値を取り、0は完全に純粋なノード（すべてのサンプルが同じクラスに属すること）を示し、1は最大の不純度を示します。\n",
        "\n",
        "### Gini ImpurityとLightGBM\n",
        "\n",
        "LightGBMは、勾配ブースティングフレームワークであり、高速な実行速度で大規模なデータセットの処理を可能にします。LightGBMは、決定木を用いたブースティングを基にしており、ツリースプリッティングの際にジニ不純度を使用することがあります。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "ジニ不純度は主にデータの分割方法を決定する際に使用されます。決定木アルゴリズムは特定の特徴量でデータを分割する際に、分割後のノードの純度を最大化（不純度を最小化）する方向で進行します。これにより、高精度な分類が行えるようにノードが構成されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56cf1d6",
      "metadata": {
        "id": "d56cf1d6"
      },
      "outputs": [],
      "source": [
        "# Gini不純度を計算するPythonコード\n",
        "\n",
        "def calculate_gini_impurity(labels):\n",
        "    from collections import Counter\n",
        "    # 各クラスのラベルの数をカウント\n",
        "    label_counts = Counter(labels)\n",
        "    total_count = len(labels)\n",
        "    print(f\"Label Counts: {label_counts}\")\n",
        "\n",
        "    # ジニ不純度を計算\n",
        "    impurity = 1.0\n",
        "    for count in label_counts.values():\n",
        "        prob_of_label = count / total_count\n",
        "        impurity -= prob_of_label ** 2\n",
        "        print(f\"Current label count: {count}, probability: {prob_of_label}, impurity: {impurity}\")\n",
        "    return impurity\n",
        "\n",
        "# サンプルラベルデータ\n",
        "labels = [0, 0, 1, 1, 2, 2, 2]\n",
        "\n",
        "# Gini不純度を計算し、結果を出力\n",
        "impurity = calculate_gini_impurity(labels)\n",
        "print(f\"Calculated Gini Impurity: {impurity}\")\n",
        "\n",
        "# このコードは与えられたラベルに基づいてジニ不純度を計算します。\n",
        "# 'labels'はデータポイントが属するクラスを示すリストです。\n",
        "# 結果のジニ不純度は、データセットの純度を評価するために使用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff68794c",
      "metadata": {
        "id": "ff68794c"
      },
      "source": [
        "### エントロピーとは\n",
        "\n",
        "エントロピーは情報理論および統計学で使用される概念で、主に不確実性や情報の量を測定するために用いられます。情報理論では、エントロピーはデータのばらつきの尺度としても理解されています。\n",
        "\n",
        "エントロピーは数学的には次のように定義されます。ある確率分布 $P = \\{p_1, p_2, \\ldots, p_n\\}$ が与えられたとき、エントロピー $H(P)$ は次の式で表されます：\n",
        "\n",
        "- インライン数式: $H(P) = -\\sum_{i=1}^n p_i \\log_2 p_i$\n",
        "\n",
        "- ブロック数式:\n",
        "  $$\n",
        "  H(P) = -\\sum_{i=1}^n p_i \\log_2 p_i\n",
        "  $$\n",
        "\n",
        "ここで、$p_i$ は事象 $i$ の発生確率であり、エントロピー $H(P)$ はその分布が持つ平均の情報量を意味します。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは勾配ブースティングの手法の一つで、エントロピーは情報利得（information gain）の計算に使われます。情報利得とは、ある特徴量によってデータがどれほど分けられるか、またその分けたことでどれほど不確実性が減少するかを定量的に評価する指標です。具体的には、LightGBMや他の決定木アルゴリズムは、エントロピーを用いてどのようにデータを分割するかの基準を設定します。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "エントロピーは以下のような使い方でよく用いられます：\n",
        "- 情報理論におけるデータ圧縮\n",
        "- 機械学習におけるモデル評価\n",
        "- 自然言語処理やクラスタリングにおけるデータの多様性の測定\n",
        "\n",
        "これにより、エントロピーは様々な分野で重要な役割を担っています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875443dd",
      "metadata": {
        "id": "875443dd"
      },
      "outputs": [],
      "source": [
        "# 確率分布を持つデータが与えられているとして、\n",
        "# そのエントロピーの計算をPythonで行います。\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 例: 確率分布\n",
        "probability_distribution = [0.2, 0.5, 0.3]\n",
        "\n",
        "# エントロピーを計算する関数\n",
        "# エントロピーH(P) = -sum(p_i * log2(p_i)) を計算\n",
        "\n",
        "def calculate_entropy(prob_dist):\n",
        "    # エントロピー初期化\n",
        "    entropy = 0\n",
        "\n",
        "    # 各確率について計算\n",
        "    for p in prob_dist:\n",
        "        if p > 0:  # p が0の時のログは未定義のため除外\n",
        "            entropy -= p * np.log2(p)\n",
        "    return entropy\n",
        "\n",
        "# エントロピーの計算\n",
        "entropy_value = calculate_entropy(probability_distribution)\n",
        "\n",
        "# 計算した結果を出力\n",
        "print(\"確率分布:\", probability_distribution)\n",
        "print(\"エントロピー:\", entropy_value)\n",
        "\n",
        "# このコードは確率分布が与えられた際、そのエントロピーを計算します。\n",
        "# 各確率値に対して -p_i * log2(p_i) を計算し、その結果を合計することでエントロピーを求めます。\n",
        "# 確率値が0の場合は計算しません。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22bd3b08",
      "metadata": {
        "id": "22bd3b08"
      },
      "source": [
        "MSE（Mean Squared Error）は、予測モデルの精度を評価するために用いられる指標です。MSEは、予測値と実際の値の差を二乗した後に平均を取ることで算出されます。これにより、モデルの誤差の大きさを平均的に捉えることができます。数式としては以下で表されます。\n",
        "\n",
        "インライン数式: $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
        "\n",
        "ブロック数式:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\n",
        "$$\n",
        "\n",
        "ここで、$y_i$は実際の値、$\\hat{y_i}$は予測値、$n$はデータの数を表します。\n",
        "\n",
        "MSEは特に回帰分析において、モデルの性能を評価する際に頻繁に使用されます。値が小さいほど、モデルが正確であることを意味します。\n",
        "\n",
        "LightGBMにおいても、MSEはロス関数として利用されることがあります。LightGBMは勾配ブースティングを用いるライブラリで、誤差を最小化するようにモデルを訓練します。その際にMSEを誤差の尺度として使用することで、モデルの予測性能を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2714d51",
      "metadata": {
        "id": "f2714d51"
      },
      "outputs": [],
      "source": [
        "# MSEを計算するためのPythonコード\n",
        "import numpy as np\n",
        "\n",
        "# 実際の値と予測値のサンプルデータを用意\n",
        "actual_values = np.array([3, -0.5, 2, 7])\n",
        "predicted_values = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "# MSEを計算\n",
        "def calculate_mse(actual, predicted):\n",
        "    # 予測値と実際の値の差を計算\n",
        "    differences = actual - predicted\n",
        "    print(f'Differences: {differences}')  # 差を表示\n",
        "\n",
        "    # 差の二乗を計算\n",
        "    squared_differences = differences ** 2\n",
        "    print(f'Squared Differences: {squared_differences}')  # 差の二乗を表示\n",
        "\n",
        "    # 二乗誤差の平均を計算\n",
        "    mse = np.mean(squared_differences)\n",
        "    print(f'MSE: {mse}') # MSEを表示\n",
        "    return mse\n",
        "\n",
        "# MSE関数を使用して結果を計算\n",
        "result = calculate_mse(actual_values, predicted_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba5ee92",
      "metadata": {
        "id": "4ba5ee92"
      },
      "source": [
        "情報利得 (Information Gain) は、決定木アルゴリズムの分岐作成において特に重要な概念です。情報利得は、一つの属性でデータを分割した結果、どれだけの不純度が減少したかを測定する尺度です。具体的には、あるノードを分割することによって得られる情報の有用性を評価します。\n",
        "\n",
        "インライン数式で表すと、情報利得 $IG(D, A)$ は次のように計算されます：\n",
        "\n",
        "$$IG(D, A) = H(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} \\cdot H(D_v)$$\n",
        "\n",
        "ここで、$H(D)$ はデータセット $D$ のエントロピーであり、$D_v$ は属性 $A$ に基づく値 $v$ で分割されたデータセットの部分集合を表しています。\n",
        "\n",
        "ブロック数式での表現：\n",
        "\n",
        "$$\n",
        "IG(D, A) = H(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} \\cdot H(D_v)\n",
        "$$\n",
        "\n",
        "LightGBMにおいては、この情報利得が各特徴の分岐点を選択するための主要な基準として使用され、モデル精度向上のために重要です。\n",
        "\n",
        "情報利得の主な使用用途は、デシジョンツリーやツリー型モデル（例えば、LightGBM、ランダムフォレストなど）における分割基準の評価として用いられます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06808c14",
      "metadata": {
        "id": "06808c14"
      },
      "outputs": [],
      "source": [
        "# 情報利得を計算するPythonコード\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# エントロピーを計算する関数\n",
        "def entropy(elements):\n",
        "    # 要素の確率を計算\n",
        "    probabilities = [np.mean(elements == cls) for cls in set(elements)]\n",
        "    # エントロピーを計算\n",
        "    entropy_value = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
        "    return entropy_value\n",
        "\n",
        "# 情報利得を計算する関数\n",
        "def information_gain(D, D_v):\n",
        "    # 全体のエントロピーを計算\n",
        "    H_D = entropy(D)\n",
        "    # 条件付きエントロピーを計算\n",
        "    weighted_entropy = sum((len(subset) / len(D)) * entropy(subset) for subset in D_v)\n",
        "    # 情報利得を計算\n",
        "    IG = H_D - weighted_entropy\n",
        "    return IG\n",
        "\n",
        "# データセットの例\n",
        "data = np.array(['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C', 'C', 'C'])\n",
        "\n",
        "# 特定条件で分割されたサブセットの例\n",
        "subset_1 = np.array(['A', 'A', 'A', 'B'])\n",
        "subset_2 = np.array(['B', 'C', 'C', 'C', 'C', 'C'])\n",
        "subsets = [subset_1, subset_2]\n",
        "\n",
        "# エントロピーを出力して確認\n",
        "entropy_data = entropy(data)\n",
        "print(\"全データセットのエントロピー: \", entropy_data)\n",
        "\n",
        "# 各サブセットのエントロピーを出力して確認\n",
        "entropy_subset1 = entropy(subset_1)\n",
        "entropy_subset2 = entropy(subset_2)\n",
        "print(\"サブセット1のエントロピー: \", entropy_subset1)\n",
        "print(\"サブセット2のエントロピー: \", entropy_subset2)\n",
        "\n",
        "# 情報利得を計算して出力\n",
        "ig_value = information_gain(data, subsets)\n",
        "print(\"情報利得 (Information Gain): \", ig_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10674ca",
      "metadata": {
        "id": "f10674ca"
      },
      "source": [
        "勾配（Gradient）は、関数の変数に対する微分であり、数値的な斜面を示します。特に、機械学習においては、損失関数を最小化するためのパラメータの更新方向を示します。勾配は、関数 $f(x)$ の各変数 $x_i$ についての偏微分を取ったベクトルとして表されます。インライン数式では、勾配は $\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)$ と表されます。\n",
        "\n",
        "ブロック数式では次のように表します:\n",
        "$$\n",
        "\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "\n",
        "LightGBMでの勾配は、ブースティングプロセスを通してモデルを最適化するための中心的な役割を果たします。具体的には、各弱学習器（つまり木構造）が予測誤差の勾配に基づいて修正を行います。\n",
        "\n",
        "使い方としては、すべてのサンプルの損失の勾配を計算し、それに基づいて各木を構築します。そして、この木を用いて勾配を補正し、最終的なアンサンブルモデルを訓練します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c616a1db",
      "metadata": {
        "id": "c616a1db"
      },
      "outputs": [],
      "source": [
        "# 勾配を計算するためのPythonコード\n",
        "import numpy as np\n",
        "\n",
        "# シンプルな二次関数 f(x) = x^2 を考える\n",
        "# これを微分したい\n",
        "\n",
        "def f(x):\n",
        "    return x ** 2\n",
        "\n",
        "# 勾配（微分）を計算する関数\n",
        "# f'(x) = 2x という理論を用います。\n",
        "def gradient(x):\n",
        "    return 2 * x\n",
        "\n",
        "# 例として x = 3 のときの勾配を計算\n",
        "x_value = 3\n",
        "gradient_value = gradient(x_value)\n",
        "\n",
        "# 計算結果を出力\n",
        "print(f'関数 f(x) = x^2 における x = {x_value} での勾配: ', gradient_value)\n",
        "\n",
        "# このコードは LightGBM における基礎勾配計算の理解を目的とします。\n",
        "# 実際の使用では、データセット全体の損失関数に基づいた勾配の計算が行われます。\n",
        "\n",
        "# 変数の中身を表示\n",
        "print(f\"x_value: {x_value}\")\n",
        "print(f\"gradient_value: {gradient_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388e0f4a",
      "metadata": {
        "id": "388e0f4a"
      },
      "source": [
        "ヘッシアン(Hessian)は、ある関数の全ての2次偏導関数を含む行列です。\n",
        "これは、関数の形状や歪みを解析するのに役立ちます。\n",
        "特に、二次形式を用いる際に非常に重要な役割を果たします。\n",
        "インライン数式で書くと、ヘッシアン行列\\(H\\)は次のように定義されます: $H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}$。ブロック数式で表すと、$$H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}$$。\n",
        "\n",
        "LightGBMにおけるヘッシアンは、決定木のブースティングプロセスで使用される重要な概念で、二乗ロスの最小化を通じて勾配に基づく最適化を行います。\n",
        "具体的には、LightGBMは学習時に、各決定木の葉を分割するためにヘッシアンを用いて、より正確な分割基準を決定します。\n",
        "予測精度を向上させるために、勾配だけでなくヘッシアンも考慮することで、より高速かつ効率的な学習が可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c60ffc7",
      "metadata": {
        "id": "0c60ffc7"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        "import numpy as np\n",
        "from sympy import Matrix, symbols, diff\n",
        "\n",
        "# 変数の定義\n",
        "a, b = symbols('a b')\n",
        "\n",
        "# 関数fを定義します\n",
        "def f(a, b):\n",
        "    return a**2 + b**2 + a*b\n",
        "\n",
        "# ヘッシアン行列を計算するための空の行列を作成します\n",
        "H = Matrix([[diff(f(a, b), a, a), diff(f(a, b), a, b)],\n",
        "             [diff(f(a, b), b, a), diff(f(a, b), b, b)]])\n",
        "\n",
        "# ヘッシアン行列を出力します\n",
        "print(\"Hessian Matrix:\")\n",
        "print(H)\n",
        "\n",
        "# 上の結果は関数f(a,b) = a^2 + b^2 + abのヘッシアン行列です\n",
        "# fの2次偏導関数を計算しているため、ヘッシアンは以下のようになります:\n",
        "# [[2, 1],\n",
        "#  [1, 2]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce9f14a",
      "metadata": {
        "id": "0ce9f14a"
      },
      "source": [
        "Leaf-wise Growthは、LightGBMにおける決定木の成長方法の一つで、Gradient Boosting Decision Tree (GBDT)を効率的に構築するために用いられます。通常のLevel-wise Growthでは、ツリーの全てのレベルで一斉にノードを展開しますが、Leaf-wise Growthでは、まず最も情報ゲインが高いリーフを選んで育てることで、ツリーの深さを柔軟に調整します。これは特に深いツリーを好むLightGBMにおいて、より高い予測精度を得るための重要な特徴です。Leaf-wise Growthの基本的な考え方は、情報利得(Information Gain)を最大化することにあります。情報利得は、新しい分割によって得られる情報の増加量を定量化するもので、以下の式で表されます。\n",
        "\n",
        "インライン数式: $Gain =\n",
        "rac{1}{2} \\left(\n",
        "rac{\\sum_{i \\in left} grad_i}{H_{left} + \\lambda} +\n",
        "rac{\\sum_{i \\in right} grad_i}{H_{right} + \\lambda} -\n",
        "rac{\\sum_{i \\in all} grad_i}{H_{all} + \\lambda} \\right)$\n",
        "\n",
        "ブロック数式:\n",
        "$$\n",
        "Gain = \\frac{1}{2} \\left( \\frac{\\sum_{i \\in left} grad_i}{H_{left} + \\lambda} + \\frac{\\sum_{i \\in right} grad_i}{H_{right} + \\lambda} - \\frac{\\sum_{i \\in all} grad_i}{H_{all} + \\lambda} \\right)\n",
        "$$\n",
        "\n",
        "ここで、$grad_i$は各データポイント$i$の勾配であり、$H_{left}$、$H_{right}$、および$H_{all}$はそれぞれ左、右、全体のヒスジアン行列の値です。$\\lambda$は正則化パラメータを指します。この分割基準に基づき、情報利得を最大化する方向にリーフが成長します。Leaf-wise Growthの利点は、より少ない計算量で高い予測精度を達成することができ、特に大規模データセットでその威力を発揮します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5444a964",
      "metadata": {
        "id": "5444a964"
      },
      "outputs": [],
      "source": [
        "# Leaf-wise Growthの基本的な考え方を模したPythonコードの例\n",
        "import numpy as np\n",
        "\n",
        "# グラディエントとヒスジアンのダミーデータ\n",
        "gradient = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n",
        "hessian = np.array([0.9, 0.8, 0.7, 0.6, 0.5])\n",
        "\n",
        "# 正則化パラメータ\n",
        "lambda_ = 1.0\n",
        "\n",
        "# インデックスで分割を考える（例えば、2を境にする）\n",
        "left_indices = np.array([0, 1])\n",
        "right_indices = np.array([2, 3, 4])\n",
        "\n",
        "# 各種計算を行う\n",
        "sum_grad_left = np.sum(gradient[left_indices])\n",
        "sum_grad_right = np.sum(gradient[right_indices])\n",
        "sum_grad_all = np.sum(gradient)\n",
        "\n",
        "sum_hess_left = np.sum(hessian[left_indices])\n",
        "sum_hess_right = np.sum(hessian[right_indices])\n",
        "sum_hess_all = np.sum(hessian)\n",
        "\n",
        "# 情報利得を計算する\n",
        "gain = 0.5 * (\n",
        "    (sum_grad_left ** 2) / (sum_hess_left + lambda_) +\n",
        "    (sum_grad_right ** 2) / (sum_hess_right + lambda_) -\n",
        "    (sum_grad_all ** 2) / (sum_hess_all + lambda_)\n",
        ")\n",
        "\n",
        "# 結果を出力\n",
        "print(\"Sum of gradients (left):\", sum_grad_left)\n",
        "print(\"Sum of gradients (right):\", sum_grad_right)\n",
        "print(\"Sum of gradients (all):\", sum_grad_all)\n",
        "print(\"Sum of hessians (left):\", sum_hess_left)\n",
        "print(\"Sum of hessians (right):\", sum_hess_right)\n",
        "print(\"Sum of hessians (all):\", sum_hess_all)\n",
        "print(\"Calculated Information Gain:\", gain)\n",
        "\n",
        "# このコードは、仮想的な分割に基づいて情報利得を計算することを目的としています。\n",
        "# LightGBMのLeaf-wise Growthにおける分割利得を模倣した基本的な実装の例です。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b770ae",
      "metadata": {
        "id": "71b770ae"
      },
      "source": [
        "### Histogram-based Splitting\n",
        "\n",
        "Histogram-based Splittingは、決定木の構築において特徴量の最適な分割点を効率的に見つけ出すための手法です。通常、決定木は各特徴量について全ての可能な分割点をチェックしますが、これは計算コストが高いため、LightGBMではヒストグラムを使用してこのプロセスを高速化しています。\n",
        "\n",
        "#### 理論と数式\n",
        "データを複数のビンに分割し、それぞれのビンに含まれるデータの情報を集計します。これは次のような数式でヒストグラムを表現できます。\n",
        "\n",
        "- 各ビン \\( i \\) に対して、ビンの境界を \\( b_i \\) として、\n",
        "  $$ H(i) = \\sum_{x_j \\in b_i} g(x_j) $$\n",
        "  ここで、\\( g(x_j) \\) はデータポイント \\( x_j \\) に対応するグラデーションです。\n",
        "\n",
        "- 最適分割は、すべてのビンについて二つの連続するビンの差を計算し、最も大きな差が生じた箇所を探すことで得られます。\n",
        "\n",
        "#### LightGBMとの関係性\n",
        "LightGBMは、このHistogram-based Splittingを利用することで、非常に高速に決定木の分割点を決定します。これにより、大規模なデータセットに対しても迅速に学習を行うことができます。\n",
        "\n",
        "#### 使用用途\n",
        "- 大規模なデータセットを高速に処理したい場合に有効です。\n",
        "- 基本的な特徴として、計算資源が少ない環境でも高精度なモデルを構築することができます。\n",
        "- GBDT（Gradient Boosting Decision Trees）モデルの構築に活用されます。\n",
        "\n",
        "Histogram-based Splittingは、特に大規模なデータセットや高次元のデータセットにおいて、計算リソースを節約しつつモデルの精度を維持するために重要な技術です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a95189",
      "metadata": {
        "id": "27a95189"
      },
      "outputs": [],
      "source": [
        "# PythonでのHistogram-based Splittingの実装例\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 例としてのデータセットを生成する\n",
        "np.random.seed(42)\n",
        "feature = np.random.uniform(1, 100, 100)  # 1から100の範囲の連続値のデータ\n",
        "\n",
        "# ヒストグラムのビンの数を設定する\n",
        "num_bins = 10\n",
        "\n",
        "# ヒストグラムを作成して、各ビンの境界やビンごとのデータ数を取得する\n",
        "hist, bin_edges = np.histogram(feature, bins=num_bins)\n",
        "\n",
        "# ヒストグラム情報を出力\n",
        "print('Histogram:', hist)\n",
        "print('Bin edges:', bin_edges)\n",
        "\n",
        "# ビンごとの誤差の集計、ここではランダムに設定（実際には誤差またはグラデーションを計算）\n",
        "grad = np.random.uniform(1, 10, num_bins)  # 各ビンに対するグラデーションの例\n",
        "\n",
        "# 最適ビンの探索を行う（ここで連続するビン間の差異を利用）\n",
        "best_split = None\n",
        "max_gain = -np.inf\n",
        "\n",
        "# リーフのグラデーションの合計を計算\n",
        "total_grad = np.sum(grad)\n",
        "\n",
        "# ビン間の情報利得を計算\n",
        "for i in range(1, num_bins):\n",
        "    left_grad = np.sum(grad[:i])  # 左のビンの合計グラデーション\n",
        "    right_grad = total_grad - left_grad  # 右のビンの合計グラデーション\n",
        "    gain = left_grad * (total_grad - left_grad)  # シンプルな利得の計算（実際の利得関数はもっと複雑）\n",
        "    if gain > max_gain:\n",
        "        max_gain = gain\n",
        "        best_split = i\n",
        "\n",
        "print('Best split bin index:', best_split)\n",
        "print('Max gain:', max_gain)\n",
        "\n",
        "# ベストな分割ポイントを求める\n",
        "best_split_point = (bin_edges[best_split - 1] + bin_edges[best_split]) / 2\n",
        "print('Best split point:', best_split_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe6e5f4",
      "metadata": {
        "id": "cfe6e5f4"
      },
      "source": [
        "GOSS (Gradient-based One-Side Sampling) は、LightGBM において提案されたサンプリング手法であり、大規模なデータセットでの勾配ブースティング決定木 (GBDT) の学習を効率化するために用いられます。従来の GBDT は、データの全サンプルを使用して木を構築しますが、GOSS は勾配の大きさに基づいてサンプリングを行うことで学習速度を向上させます。具体的には、勾配が大きいサンプルは外れ値を示しやすいため、そのまま抽出し、勾配が小さいサンプルは一定の割合でランダムに抽出します。数式で表すと、あるサンプル $i$ に対する勾配を $g_i$ とすると、大きい勾配に対するサンプルに重みを 1、ランダムに抽出されたサンプルに重みを $$t \\cdot\n",
        "rac{1 - a}{b}$$ としてペナルティを調整します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce10be2",
      "metadata": {
        "id": "bce10be2"
      },
      "outputs": [],
      "source": [
        "# Pythonのコード例\n",
        "import numpy as np\n",
        "\n",
        "# 大きい勾配を持つサンプルを選択する割合\n",
        "large_gradient_fraction = 0.2\n",
        "# 小さい勾配を持つサンプルを抽出する割合\n",
        "small_random_fraction = 0.5\n",
        "\n",
        "# サンプルの総数\n",
        "n_samples = 1000\n",
        "\n",
        "# サンプルに対する人工の勾配データを生成\n",
        "np.random.seed(42)\n",
        "gradients = np.random.rand(n_samples)\n",
        "\n",
        "# 勾配の大きさでサンプルをソート\n",
        "sorted_indices = np.argsort(-gradients)\n",
        "gradients_sorted = gradients[sorted_indices]\n",
        "\n",
        "# 大勾配のサンプル\n",
        "n_large = int(n_samples * large_gradient_fraction)\n",
        "large_gradients = gradients_sorted[:n_large]\n",
        "\n",
        "# 小さい勾配はランダムに選択\n",
        "n_small = int(n_samples * small_random_fraction)\n",
        "random_indices = np.random.choice(range(n_large, n_samples), n_small, replace=False)\n",
        "small_gradients = gradients_sorted[random_indices]\n",
        "\n",
        "# サンプルの重みを定義\n",
        "t = gradients.mean()\n",
        "large_weights = np.ones(n_large)\n",
        "small_weights = t * (1 - large_gradient_fraction) / small_random_fraction\n",
        "\n",
        "print('Large Gradient Samples:', large_gradients)\n",
        "print('Small Gradient Samples:', small_gradients)\n",
        "print('Weight for small gradients:', small_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8610c0d",
      "metadata": {
        "id": "b8610c0d"
      },
      "source": [
        "## Exclusive Feature Bundling (EFB)について\n",
        "\n",
        "Exclusive Feature Bundling (EFB) は、LightGBMにおいて特徴量の数を削減するためのテクニックです。多くの機械学習問題では高次元のデータを扱うことが一般的です。EFBを使用することで、相互排他の特徴量を同じグループ（バンドル）にまとめることができます。\n",
        "\n",
        "### 理論\n",
        "EFBでは、相互排他的な特徴量を一つの特徴量にまとめます。相互排他的とは、同一データ内で同時に1つ以上の特徴がゼロ以外になることがないことを指します。\n",
        "\n",
        "例えば、特徴Aと特徴Bが同時に非ゼロになることがない場合、これら2つの特徴はバンドリングされ、\n",
        "新しい一つの特徴となります。\n",
        "\n",
        "具体的に数式で表すと、特徴量ベクトル $X = [x_1, x_2, ..., x_n]$ の中から相互排他的なサブセットを探し、それらの和（または他の直線変換）として表現できます。\n",
        "\n",
        "$$\n",
        "X_{EFB} = [b_1, b_2, ..., b_m], \\\n",
        "b_i = f(x_{i1}, x_{i2}, ..., x_{ik})\n",
        "$$\n",
        "\n",
        "$X_{EFB}$ は新しい特徴ベクトルであり、$b_i$ はバンドルされた特徴です。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "LightGBMは、高速で高性能な決定木ベースの学習アルゴリズムを提供します。その一環として、EFBを利用することで、効率的な学習とメモリ使用量の削減を実現します。\n",
        "\n",
        "### 使用用途\n",
        "EFBは、特にディスクリートかつまばらなデータセットにおいて有効です。これにより、以下のような効果があります。\n",
        "- 特徴量数の減少\n",
        "- メモリ消費の削減\n",
        "- 計算速度の向上"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a27f56",
      "metadata": {
        "id": "17a27f56"
      },
      "outputs": [],
      "source": [
        "# Exclusive Feature Bundling (EFB) の理論を理解するためのPythonコード例です。\n",
        "# このコードは、EFBの基本的なアイデアをシミュレートしたものであり、実行中のLightGBM内での実装ではありません。\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# サンプルデータとして相互排他的な特徴量を作成\n",
        "# 二つの特徴量ベクトル（AとB）があるとします。\n",
        "features_A = np.array([0, 1, 0, 0, 0])\n",
        "features_B = np.array([0, 0, 1, 0, 0])\n",
        "\n",
        "# EFBを用いて、AとBを一つのベクトルにバンドルします。\n",
        "# 条件として、AとBは相互に排他的です（同時に非ゼロにならない）。\n",
        "bundled_features = features_A + features_B\n",
        "\n",
        "# 結果を表示\n",
        "print('Original Features A:', features_A)\n",
        "print('Original Features B:', features_B)\n",
        "print('Bundled Features:', bundled_features)\n",
        "\n",
        "# この例では、bundled_featuresはfeatures_Aとfeatures_Bの情報を持っています。\n",
        "# しかし、それは単一の特徴となって、サイズが削減されたことを示します。\n",
        "\n",
        "# Output\n",
        "# Original Features A: [0 1 0 0 0]\n",
        "# Original Features B: [0 0 1 0 0]\n",
        "# Bundled Features:    [0 1 1 0 0]\n",
        "\n",
        "# EFBはこのような単純な足し算だけでなく、必要に応じてもっと複雑な操作をすることもできます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06305d4",
      "metadata": {
        "id": "d06305d4"
      },
      "source": [
        "Early Stopping は機械学習アルゴリズムの一種で、主にモデルの過学習を防ぐために使用されます。過学習とは、モデルが訓練データに対してあまりにもよく適応しすぎてしまうことであり、これにより他の新しいデータに対しての性能が低下する恐れがあります。        \n",
        "\n",
        "Early Stoppingは一定の基準（通常は検証データにおける誤差や精度の変動）を満たすことができなくなった場合に訓練を中止します。例えば、一定エポック数にわたってベストな検証誤差が更新されない場合に学習を停止することができます。        \n",
        "\n",
        "LightGBMにおいては、多くの場合、このパラメータは `early_stopping_rounds` という名称で設定されます。たとえば、検証データに対して誤差が一定のラウンド数更新されなければ学習を停止します。        \n",
        "\n",
        "理論的には、早期停止のアルゴリズムは次のように概念化することができます：        \n",
        "\n",
        "- 現在の検証誤差 $E_{val}$ が最良の誤差 $E_{best}$ より小さい場合、$E_{best}$ を更新する。        \n",
        "\n",
        "- エポック $i$ に対して誤差が最良の状態からの許容幅 $\\Delta$ 以上に改善されない場合停止する。        \n",
        "\n",
        "ところで、モデルパラメータが学習するごとに更新されるとして、誤差は次のように定義されると考えられます：         $$ E(\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(x_i, \theta)) $$         ここで、$L$ は損失関数、$y_i$ は正解ラベル、$f(x_i, \theta)$ は予測関数です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e37b33d",
      "metadata": {
        "id": "8e37b33d"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# データをロードして訓練とテストセットに分割します\n",
        "boston = load_boston()\n",
        "X_train, X_val, y_train, y_val = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM用のデータセットを作成します\n",
        "d_train = lgb.Dataset(X_train, label=y_train)\n",
        "d_val = lgb.Dataset(X_val, label=y_val, reference=d_train)\n",
        "\n",
        "# パラメータを設定します\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "# モデルの訓練を行い、早期停止を使用します\n",
        "print('Train LightGBM model with early stopping')\n",
        "evals_result = {}\n",
        "model = lgb.train(params,\n",
        "                  d_train,\n",
        "                  valid_sets=[d_train, d_val],\n",
        "                  valid_names=['train', 'valid'],\n",
        "                  evals_result=evals_result,\n",
        "                  num_boost_round=100,\n",
        "                  early_stopping_rounds=10)\n",
        "\n",
        "# 最良のラウンド数を出力します\n",
        "print('Best iteration:', model.best_iteration)\n",
        "\n",
        "# 検証データに対する予測を行います\n",
        "y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "\n",
        "# 予測結果と実際のラベルを比較して誤差を出力します\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print('Validation MSE:', mse)\n",
        "\n",
        "# 各ラウンドでの評価結果を出力します\n",
        "print('Evaluation results:', evals_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42def6a",
      "metadata": {
        "id": "a42def6a"
      },
      "source": [
        "### Learning Rate Decay\n",
        "\n",
        "Learning Rate Decay（学習率減衰）は、機械学習特に深層学習において、学習の進行に従って学習率を徐々に減少させる手法です。学習率は、モデルのパラメータを更新する際の一歩の大きさを決定する重要なハイパーパラメータです。初期の学習では高い学習率を設定して高速に学習を進め、後半では学習率を減少させて微調整するというアプローチが効果的であることが知られています。\n",
        "\n",
        "LightGBMにおいても、Learning Rate Decayを用いることで、学習の安定性を向上させ、最適なモデルを得ることが可能です。モデルの学習が進むにつれて、高い学習率では損失が安定しにくくなるため、Learning Rate Decayを適用してパラメータの最適化を図ります。\n",
        "\n",
        "#### 数式\n",
        "Learning Rate Decayの典型的な形は次のように定義されます：\n",
        "\n",
        "1. **指数的減衰:**\n",
        "   学習率 \\( \\eta_t \\) はエポック数 \\( t \\) に応じて\n",
        "   $\\( \\eta_t = \\eta_0 \\cdot \\exp(-kt) \\)$\n",
        "   $$\\eta_t = \\eta_0 \\cdot \\exp(-kt)$$\n",
        "   ここで、\\( \\eta_0 \\) は初期学習率で、\\( k \\) は減衰率を調整するハイパーパラメータです。\n",
        "\n",
        "2. **段階的減衰:**\n",
        "   $\\( \\eta_t = \\eta_0 \\cdot \\text{decay\\_rate}^{\\lfloor t / \\text{decay\\_step} \\rfloor} \\)$\n",
        "   $$\\eta_t = \\eta_0 \\cdot \\text{decay\\_rate}^{\\lfloor t / \\text{decay\\_step} \\rfloor}$$\n",
        "   ここで、\\( \text{decay\\_rate} \\) は何度も学習率を減少させる割合、\\( \text{decay\\_step} \\) は何エポックごとに減少させるかを決定します。\n",
        "\n",
        "#### 使用用途\n",
        "Learning Rate Decayは、特に深層学習のトレーニングにおいて頻用されます。これは、初期段階で迅速に最適化をすすめ、大域的最適値へ近づく目的と、後半での微調整によって局所的な最適化を達成するためです。LightGBMでは、こうした手法により勾配ブースティングの性能を高めることを狙います。この機能は、LGBMの`学习率`パラメータにおけるdecayオプションとして実装されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead597ea",
      "metadata": {
        "id": "ead597ea"
      },
      "outputs": [],
      "source": [
        "# Learning Rate DecayをPythonでシミュレーション\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 初期学習率\n",
        "eta_0 = 0.1\n",
        "\n",
        "# エポック数（100とする）\n",
        "epochs = np.arange(0, 100)\n",
        "\n",
        "# 減衰率（指数的減衰をシミュレーション）\n",
        "k = 0.1\n",
        "\n",
        "# Exponential Decay (指数的減衰)\n",
        "learning_rate_exponential = eta_0 * np.exp(-k * epochs)\n",
        "\n",
        "# Step Decay (段階的減衰) - decay_rate = 0.5, decay_step = 10\n",
        "decay_rate = 0.5\n",
        "decay_step = 10\n",
        "learning_rate_step = eta_0 * (decay_rate ** (epochs // decay_step))\n",
        "\n",
        "# 学習率の減衰をプロットする\n",
        "def plot_learning_rate_decay(epochs, learning_rate_exponential, learning_rate_step):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, learning_rate_exponential, label='Exponential Decay')\n",
        "    plt.plot(epochs, learning_rate_step, label='Step Decay', linestyle='--')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Decay')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 学習率の変化を確認\n",
        "print('初期学習率:', eta_0)\n",
        "print('指数的減衰の最終学習率:', learning_rate_exponential[-1])\n",
        "print('段階的減衰の最終学習率:', learning_rate_step[-1])\n",
        "\n",
        "# プロット\n",
        "plot_learning_rate_decay(epochs, learning_rate_exponential, learning_rate_step)\n",
        "\n",
        "# 出力は各エポックにおける学習率の変化を示すものであり、\n",
        "# 学習率が徐々に低下していく様子を視覚的に確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85fd3e62",
      "metadata": {
        "id": "85fd3e62"
      },
      "source": [
        "### L1正則化とL2正則化の理論\n",
        "\n",
        "正則化は、モデルの過学習を防ぐためによく使用されるテクニックです。L1正則化（Lasso）は重みの絶対値の合計にペナルティを課す手法で、以下のように表されます。\n",
        "\n",
        "インライン数式: $J(w) = \text{Loss} + \\lambda \\sum |w_i|$\n",
        "\n",
        "ブロック数式:\n",
        "$$J(w) = \text{Loss} + \\lambda \\sum |w_i|$$\n",
        "\n",
        "L2正則化（Ridge）は重みの二乗の合計にペナルティを課し、以下のように表されます。\n",
        "\n",
        "インライン数式: $J(w) = \text{Loss} + \\frac{\\lambda}{2} \\sum w_i^2$\n",
        "\n",
        "ブロック数式:\n",
        "$$J(w) = \text{Loss} + \\frac{\\lambda}{2} \\sum w_i^2$$\n",
        "\n",
        "ここで、$\\text{Loss}$は損失関数、$w$はモデルの重み、$\\lambda$は正則化の強度を決定するハイパーパラメータです。\n",
        "\n",
        "### L1/L2正則化とLightGBMの関係性\n",
        "\n",
        "LightGBMは勾配ブースティング機械学習フレームワークで、過学習を防ぐためにL1正則化(L1)およびL2正則化(L2)のパラメータがサポートされています。これらは目的関数に追加され、モデルの複雑さを制御します。通常、L2正則化（`lambda_l2`）はデフォルト設定で使用されており、L1正則化 (`lambda_l1`) も可変パラメータとして設定できます。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "L1正則化は主に特徴選択に有用であり、重要でない特徴の重みをゼロにする傾向があります。L2正則化は重みの大きさを抑制し、過学習を防ぐためにモデルの複雑さにペナルティを与えるために使われます。L1/L2正則化の適切な組み合わせを選択することにより、モデルの汎化性能を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc589f2e",
      "metadata": {
        "id": "fc589f2e"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "import numpy as np\n",
        "\n",
        "# ダミーデータの生成\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
        "\n",
        "# L1正則化（Lasso回帰）の適用\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Lassoの係数を出力\n",
        "print(\"Lasso coefficients:\", lasso.coef_)\n",
        "\n",
        "# L2正則化（Ridge回帰）の適用\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "# Ridgeの係数を出力\n",
        "print(\"Ridge coefficients:\", ridge.coef_)\n",
        "\n",
        "# コメント：\n",
        "# ここでは、LassoとRidgeのそれぞれの回帰係数を比較することができます。\n",
        "# Lassoは多くの場合、いくつかの係数をゼロに設定し特徴選択を行うのに対し、\n",
        "# Ridgeは全ての特徴を利用し、重みの絶対値を抑制します。\n",
        "\n",
        "# alphaは正則化の強さを表します。\n",
        "# alphaの値が大きいほど、ペナルティが大きくなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeeabc16",
      "metadata": {
        "id": "aeeabc16"
      },
      "source": [
        "### SHAP値について\n",
        "\n",
        "SHAP（SHapley Additive exPlanations）値は、機械学習モデルの各予測に対する特徴量の寄与を定量化するための手法です。ゲーム理論に由来し、特に協力ゲームにおける報酬分配問題から派生しました。Shapley値は、プレイヤー（ここでは特徴量）が協力して得られる総報酬をどのように分配するかを決定するもので、以下の特性を満たします：\n",
        "- **効率性**：すべての特徴量の寄与の合計が、実際の予測出力に等しい。\n",
        "- **対称性**：もし二つの特徴量が同じ寄与を持つ場合、彼らのShapley値も等しい。\n",
        "- **無関係性**：寄与がない特徴量のShapley値はゼロ。\n",
        "- **加法性**：複数の予測モデルのShapley値は合計できます。\n",
        "\n",
        "SHAP値の計算式は以下のようになっています。特徴量の集合を\\( N \\)、特定の特徴量を\\( i \\)とし、\\( S \\)を\\( i \\)を含まない特徴量の部分集合とした場合、Shapley値\\( \\phi \\)は次のように定義されます。\n",
        "\n",
        "$$\n",
        "\\phi_i(x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)]\n",
        "$$\n",
        "\n",
        "ここで、\\( v(S) \\)は特徴量の集合\\( S \\)が得る報酬、もしくはモデルの予測値です。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは勾配ブースティングを利用した決定木ベースの学習アルゴリズムです。特徴量重要度の評価において、SHAP値は個々の特徴量の影響度を明示的に提供するため、LightGBMモデルの解釈性を向上させます。これにより、各特徴が予測にどれだけ寄与しているかを可視化できます。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "SHAP値は、以下のような使用用途があります：\n",
        "- **モデルの解釈**：予測モデルにおける各特徴量の影響を理解する。\n",
        "- **特徴量選択**：特に重要な特徴量を特定し、モデルの性能を改善させる。\n",
        "- **異常検知**：予期しないSHAP値を持つデータポイントを異常として検出する。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66db0d5c",
      "metadata": {
        "id": "66db0d5c"
      },
      "outputs": [],
      "source": [
        "# SHAP値を計算し、LightGBMモデルの解釈を行うPythonコード\n",
        "\n",
        "# 必要なライブラリをインポート\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# データセットをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 訓練用とテスト用にデータを分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMモデルを訓練\n",
        "model = lgb.LGBMClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# SHAP値の計算\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# SHAP値を可視化\n",
        "shap.summary_plot(shap_values, X_test, feature_names=iris.feature_names)\n",
        "\n",
        "# 各特徴量のSHAP値を出力\n",
        "print(\"SHAP値：\")\n",
        "for i, feature_name in enumerate(iris.feature_names):\n",
        "    print(f\"{feature_name}: {shap_values[:, i].values}\")\n",
        "\n",
        "# 'explainer'オブジェクトの内容の確認\n",
        "print(\"explainerの詳細:\", explainer)\n",
        "\n",
        "# 訓練データの形状を確認\n",
        "print(\"訓練データの形状:\", X_train.shape)\n",
        "\n",
        "# LightGBMモデルのパラメータを確認\n",
        "print(\"モデルのパラメータ:\", model.get_params())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7564b6",
      "metadata": {
        "id": "4c7564b6"
      },
      "source": [
        "## Permutation Importanceの解説\n",
        "\n",
        "Permutation Importance（順列重要度）は、特徴量の重要度を評価するための手法です。この手法は、モデルの予測性能に対する各特徴量の寄与を、他の特徴量とは独立に測定します。\n",
        "\n",
        "Permutation Importanceの計算は以下のステップで行われます：\n",
        "1. モデルの基礎予測精度を計算する。\n",
        "2. 各特徴量ごとに、その特徴量の値をランダムにシャッフルし、予測がどれだけ悪化するかを確認する。\n",
        "3. シャッフル後の予測精度を基に、元の予測精度に対する予測性能の減少を計算する。\n",
        "\n",
        "これにより、ある特徴量が予測にどれだけ重要かを示します。予測精度の低下が大きければ大きいほど、その特徴量は予測において重要であると判断されます。\n",
        "\n",
        "数学的には、Permuation Importanceは以下のように記述されます。\n",
        "\n",
        "ブロック数式:\n",
        "$$\text{PI}(j) =\n",
        "rac{1}{R} \\sum_{r=1}^R \big( \text{Loss}(f(X), y) - \text{Loss}(f(X_{\text{perm}, j}^{(r)}), y) \big)$$\n",
        "\n",
        "インライン数式:  \n",
        "$\\text{PI}(j) = \\frac{1}{R} \\sum_{r=1}^R \\big( \\text{Loss}(f(X), y) - \\text{Loss}(f(X_{\\text{perm}, j}^{(r)}), y) \\big)$\n",
        "\n",
        "ここで、$f(X)$は元のデータセット$X$でのモデルの予測、$\text{Loss}$は予測誤差、$X_{\\text{perm}, j}^{(r)}$は$j$番目の特徴量がランダムにシャッフルされたデータセット、$y$は実際のラベルを示します。\n",
        "\n",
        "LightGBMを初めとするツリーベースのモデルにおいて、Permutation Importanceは特徴量の重要度を確認するための一般的な手法です。また、モデルがどのように予測を行っているのかを解釈する際に役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58fb9f21",
      "metadata": {
        "id": "58fb9f21"
      },
      "outputs": [],
      "source": [
        "# Permutation Importanceを計算するためのPythonコード\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Irisデータセットをロード\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# データセットをトレーニングとテストに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルを作成し、トレーニング\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# モデルの予測性能をテストデータで評価\n",
        "baseline_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print('Baseline Accuracy:', baseline_accuracy)\n",
        "\n",
        "# permutation_importanceを使用して特徴量の重要度を計算\n",
        "perm_importance = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)\n",
        "\n",
        "# 結果を表示\n",
        "for i in range(len(iris.feature_names)):\n",
        "    print(f'Feature: {iris.feature_names[i]}')\n",
        "    print(f'Importance: {perm_importance.importances_mean[i]}')\n",
        "\n",
        "# 各ステップを確認するために主要な変数を出力\n",
        "print('Importances (Each Feature):', perm_importance.importances_mean)\n",
        "print('Iris Feature Names:', iris.feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a099e61",
      "metadata": {
        "id": "9a099e61"
      },
      "source": [
        "### 決定木の構築\n",
        "決定木は、データを使って分類や回帰を行うアルゴリズムの一種です。決定木の構築において、データは条件に基づいて再帰的に分割されます。この分割方法はデータの情報利得（information gain）やジニ不純度（Gini impurity）、エントロピー（entropy）などの基準を使って決定されます。一般に、以下のような手順で決定木が構築されます。\n",
        "\n",
        "1. **初期状態**として全データを含むノードを持ちます。\n",
        "2. **各ノード**で、条件を使ってデータを二つ以上のグループに分割します。\n",
        "3. **再帰的**に同様の操作を各グループに適用し、サブノードを構築します。\n",
        "4. **停止条件**に達したら（例: ノードが十分小さい、情報利得が小さいなど）枝分かれを止めます。\n",
        "\n",
        "LightGBM（Light Gradient Boosting Machine）は、決定木を構築するための手法の一つで、効率的かつ高精度なモデルを得ることを目的とします。LightGBMの特徴として、以下が挙げられます。\n",
        "\n",
        "- **Leaf-wise growth（葉ごとの成長）**: 通常のレベル別の成長法（level-wise growth）と異なり、LightGBMは葉ごとに成長します。この方法はより深い木になりやすく、より精細な特徴を捉えます。\n",
        "- **効率性**: ランダム加重サンプリング（Random Feature Weighted Sampling）やエセルティルサンプリング（Exclusive Feature Bundling）を利用して計算効率を向上させています。\n",
        "\n",
        "#### 決定木の分岐基準\n",
        "- 情報利得（Information Gain）は、エントロピーを基に分割前後の不確実性がどれだけ減少したかを測ります。エントロピー $H$ は以下で表されます。\n",
        "  $$ H(S) = -\\sum{p_i \\log_2{p_i}} $$\n",
        "  ここで、$p_i$ はクラス $i$ の確率です。\n",
        "\n",
        "- ジニ不純度（Gini Impurity）は以下のように定義されます。\n",
        "  $$ Gini(S) = 1 - \\sum{p_i^2} $$\n",
        "  ここで、$p_i$ はクラス $i$ の確率です。\n",
        "\n",
        "### 使用用途\n",
        "決定木は以下のような場面で使用されます。\n",
        "- **分類問題**: 例) スパムメール分類、病気診断\n",
        "- **回帰問題**: 例) 不動産価格予測\n",
        "- **特徴選択と解釈**: 決定木の深さや分岐条件から、データの特徴の重要性を理解できます。\n",
        "\n",
        "このように、決定木はわかりやすくて解釈しやすいため、機械学習モデルとして広く利用されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d924d4cb",
      "metadata": {
        "id": "d924d4cb"
      },
      "outputs": [],
      "source": [
        "# Pythonでの決定木の基本構築\n",
        "# `DecisionTreeClassifier` を使って簡単な決定木を作成します。\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Iris データセットをロード\n",
        "iris = load_iris()\n",
        "X = iris.data  # 特徴量\n",
        "y = iris.target  # ラベル\n",
        "\n",
        "# データをトレーニングセットとテストセットに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 決定木分類器を初期化\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
        "\n",
        "# モデルをトレーニングデータで学習\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 訓練されたモデルを使ってテストデータを予測\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 予測精度を表示\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# 決定木の深さを表示\n",
        "print(f\"Tree Depth: {clf.get_depth()}\")\n",
        "\n",
        "# 決定木の構造をエクスポート (テキスト形式)\n",
        "from sklearn.tree import export_text\n",
        "r = export_text(clf, feature_names=iris['feature_names'])\n",
        "print(r)\n",
        "\n",
        "# 決定木の特徴量の重要度を出力\n",
        "eprint(f\"Feature Importances: {clf.feature_importances_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765419ab",
      "metadata": {
        "id": "765419ab"
      },
      "source": [
        "### ブートストラップサンプリングの解説\n",
        "\n",
        "ブートストラップサンプリングは、統計学および機械学習において広く使用される手法で、大元のデータセットから一部を復元抽出する方法です。この手法は、推定量のバイアスの評価や信頼区間の構築に使用されます。復元抽出とは、選ばれたデータポイントを再度抽出の候補に戻すことで、同じデータポイントが複数回抽出される可能性があるということです。\n",
        "\n",
        "数式的には、データセット $D = \\{x_1, x_2, ..., x_n\\}$ から $m$ 個のブートストラップサンプルを作成する場合、それぞれのサンプル $D_i$ は以下のように得られます。\n",
        "$$ D_i = \\{x_{i_1}, x_{i_2}, ..., x_{i_n}\\} $$\n",
        "\n",
        "ここで、各 $x_{i_j}$ は $D$ からの任意のデータポイントであり、復元抽出を行うため $x_{i_j} = x_{k}$ （$j \\neq k$）である可能性があります。\n",
        "\n",
        "LightGBMにおけるブートストラップサンプリングは、ブースティングアルゴリズムの一部として使用される場合があります。具体的には、ブートストラップサンプリングを用いて複数の部分データにモデルをフィッティングし、その複数のモデルの結果を組み合わせることで、汎化性能を向上させます。\n",
        "\n",
        "使用用途としては、以下のようなものがあります。\n",
        "- **信頼区間の推定**: ブートストラップは、パラメータ推定の信頼区間を計算するために使用されます。\n",
        "- **多様性の確保**: モデルのアンサンブル学習でブートストラップを使用することで、モデルの多様性を確保し、汎化能力を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e848fb18",
      "metadata": {
        "id": "e848fb18"
      },
      "outputs": [],
      "source": [
        "# Pythonでブートストラップサンプリングを実施するコード例\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 元データセット\n",
        "original_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# ブートストラップサンプリングで生成するサンプル数\n",
        "data_size = len(original_data)\n",
        "\n",
        "# ブートストラップサンプルの数\n",
        "n_samples = 5\n",
        "\n",
        "# ブートストラップサンプルを保存するリスト\n",
        "bootstrap_samples = []\n",
        "\n",
        "# ブートストラップサンプリングの実行\n",
        "for _ in range(n_samples):\n",
        "    # 復元抽出によりサンプルを生成\n",
        "    sample = random.choices(original_data, k=data_size)\n",
        "    bootstrap_samples.append(sample)\n",
        "    print(f\"ブートストラップサンプル: {sample}\")\n",
        "\n",
        "# 結果を出力\n",
        "print(\"\\n生成されたブートストラップサンプル:\")\n",
        "for i, sample in enumerate(bootstrap_samples, 1):\n",
        "    print(f\"サンプル {i}: {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5936e13",
      "metadata": {
        "id": "f5936e13"
      },
      "source": [
        "### ランダムフォレストの理論と数式\n",
        "ランダムフォレストは、複数の決定木を使ったアンサンブル学習手法です。各木はランダムに選ばれた特徴量のサブセット上で学習され、それらの予測結果を集約することで正確な予測を行います。ランダムフォレストの主な利点は、過学習を防ぎつつ、予測精度を高められることです。\n",
        "\n",
        "#### ランダムフォレストの数式\n",
        "1. 各決定木に対して、ブートストラップサンプルを用いたデータセットを生成します。\n",
        "2. 決定木の各ノードで、全特徴量の部分集合から分割を選択します。これがランダム性を導入する要因です。\n",
        "3. 決定木を完全に成長させます（剪定はしません）。\n",
        "4. 新しいデータが来たとき、各木で予測を行い、多数決または平均で投票します。\n",
        "\n",
        "例えば、分類問題の場合、予測は以下のように決定されます：\n",
        "\n",
        "\\[\n",
        "\\text{最終予測} = \\text{mode}(h_1(x), h_2(x), \\ldots, h_n(x))\n",
        "\\]\n",
        "\n",
        "ここで、\\( h_i(x) \\) は \\( i \\) 番目の決定木による予測を表します。\n",
        "\n",
        "#### LightGBMとの関係性\n",
        "LightGBMは、ランダムフォレストの一種であるGBDT（Gradient Boosting Decision Tree）の実装です。LightGBMは勾配ブースティングを使用して連続的にモデルを強化する一方で、ランダムフォレストは独立した木の集合作成します。このため、LightGBMは特に大規模なデータに対して、速さと効率の面で優れています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf70a97a",
      "metadata": {
        "id": "cf70a97a"
      },
      "outputs": [],
      "source": [
        "# サンプルデータを生成してランダムフォレストを訓練\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ランダムなデータを生成\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=0)\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルの訓練\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 訓練したモデルを使って予測\n",
        "train_predictions = clf.predict(X_train)\n",
        "test_predictions = clf.predict(X_test)\n",
        "\n",
        "# 結果を出力\n",
        "print('Train Predictions:', train_predictions)\n",
        "print('Test Predictions:', test_predictions)\n",
        "\n",
        "# モデルの特長重要度の出力\n",
        "feature_importances = clf.feature_importances_\n",
        "print('Feature importances:', feature_importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d0a39ab",
      "metadata": {
        "id": "0d0a39ab"
      },
      "source": [
        "アンサンブル学習は、複数の機械学習モデルを組み合わせてより良い予測パフォーマンスを実現する手法です。基本的なアイデアは、個々のモデルの長所を最大限に活かし、モデル間の誤差をキャンセルし合うことです。代表的な手法には、バギング（例: ランダムフォレスト）やブースティング（例: AdaBoost, LightGBM）が含まれます。LightGBMはこのブースティングの概念を利用して多くの弱学習器を組み合わせ、精度を向上させています。アンサンブル学習の一般的な数式として、個々のモデル$f_i(x)$があり、重み$w_i$を適用して予測値を算出する場合、アンサンブルの予測$F(x)$は以下のように表されます: $$ F(x) = \\sum_{i=1}^{N} w_i f_i(x) $$ 使用用途としては、分類や回帰問題に広く使われており、特に、モデルの安定性や精度を向上させるのに役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a316700a",
      "metadata": {
        "id": "a316700a"
      },
      "outputs": [],
      "source": [
        "# アンサンブル学習のデモンストレーションとしてランダムフォレストとLightGBMを使ったPythonコード\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# データをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# データをトレーニングセットとテストセットに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルをトレーニング\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# LightGBMモデルをトレーニング\n",
        "d_train = lgb.Dataset(X_train, label=y_train)\n",
        "params = {'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss'}\n",
        "lgb_model = lgb.train(params, d_train, num_boost_round=100)\n",
        "\n",
        "# モデルの予測\n",
        "rf_pred = rf.predict(X_test)\n",
        "lgb_pred = np.argmax(lgb_model.predict(X_test), axis=1)\n",
        "\n",
        "# 結果の出力\n",
        "print('Random Forest Predictions:', rf_pred)\n",
        "print('LightGBM Predictions:', lgb_pred)\n",
        "\n",
        "# ここでアンサンブルの予測を考えてみます。\n",
        "# 効果的なアンサンブルのためにはモデルの予測を組み合わせることになります。\n",
        "ensemble_pred = (rf_pred + lgb_pred) // 2\n",
        "\n",
        "print('Ensemble Predictions:', ensemble_pred)  # アンサンブルの予測を出力\n",
        "\n",
        "# このコードは、2つの異なるモデルの予測を平均化することで少し単純化されたアンサンブル手法を示しています。\n",
        "# より複雑なアンサンブル手法では、異なる重み付けやより多くのモデルを含めることがあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c7bb34",
      "metadata": {
        "id": "f3c7bb34"
      },
      "source": [
        "交差検証 (クロスバリデーション、Cross Validation) は、機械学習モデルの汎化性能を評価するための手法です。汎化性能とは、モデルが新しいデータに対してどの程度正確に予測できるかの指標です。交差検証を用いることで、過学習や過適合を避け、モデルの性能をより正確に評価できます。特に、データが限られている場合に有効な手法です。\n",
        "\n",
        "主な種類として、K-Fold Cross Validationがあります。これはデータセットを K 個の部分集合に分割し、うち1つをテスト用、残りを訓練用としてモデルを訓練します。これを K 回繰り返し、各試行で得られた性能を平均化することによってモデルの評価を行います。\n",
        "\n",
        "**数式**\n",
        "\n",
        "K-Fold Cross Validation の評価指標は通常次のように計算されます：\n",
        "\n",
        "\\[\n",
        "\\text{CV}_{(K)} = \\frac{1}{K} \\sum_{i=1}^{K} M_{i}\n",
        "\\]\n",
        "\n",
        "ここで、\\( M_{i} \\) は各 fold に対する評価指標、例えば、精度や誤差率などです。\n",
        "\n",
        "**LightGBMと交差検証**\n",
        "\n",
        "LightGBM は決定木に基づく勾配ブースティングフレームワークで、K-Fold Cross Validation を用いたハイパーパラメータチューニングに広く活用されています。LightGBM のインターフェースは sklearn に対応しているため、sklearn の cross_val_score や KFold をそのまま使用できます。\n",
        "\n",
        "**使用用途**\n",
        "\n",
        "- モデルの選択: いくつかのモデルを比較して、どのモデルが最も良い性能を示すか決定する。\n",
        "- ハイパーパラメータのチューニング: 交差検証を用いて最適なパラメータを探索する。\n",
        "- データが少ない場合の信頼できる性能評価。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11437fe",
      "metadata": {
        "id": "c11437fe"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Irisデータセットをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# LGBMClassifierのインスタンスを作成\n",
        "model = LGBMClassifier()\n",
        "\n",
        "# KFold Cross Validationインスタンスを作成 (K=5)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# cross_val_scoreを用いて交差検証\n",
        "scores = cross_val_score(model, X, y, cv=kf)\n",
        "\n",
        "# 各foldのスコアを出力\n",
        "print(\"Cross-validation scores for each fold:\", scores)\n",
        "\n",
        "# クロスバリデーションの平均スコアを出力\n",
        "average_score = scores.mean()\n",
        "print(\"Average cross-validation score:\", average_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7623542b",
      "metadata": {
        "id": "7623542b"
      },
      "source": [
        "### ハイパーパラメータチューニングとGridSearch\n",
        "\n",
        "ハイパーパラメータチューニングとは、機械学習モデルの性能を向上させるために、最適なハイパーパラメータを見つけるプロセスです。ハイパーパラメータは、モデルが学習する時の設定値のことを指し、例えば、学習率や決定木の深さなどがあります。\n",
        "\n",
        "GridSearchは、指定されたハイパーパラメータの範囲内で全ての組み合わせを試して、最適なハイパーパラメータを見つける手法です。具体的には、各パラメータの全ての組み合わせに対して交差検証を行い、評価指標に基づいてベストな組み合わせを選定します。これにより、ハイパーパラメータの空間を体系的に探索することができます。\n",
        "\n",
        "$$\n",
        "\\text{optimal parameters} = \\underset{\\theta ε \\Theta}{\\text{argmin }} \\frac{1}{K} \\sum_{k=1}^{K} L(f_{\\theta}(X_{train}^{(k)}), y_{train}^{(k)})\n",
        "$$\n",
        "\n",
        "ここで、$\\theta$はハイパーパラメータの組み合わせ、$\\Theta$は全てのハイパーパラメータの組み合わせの集合、$K$は交差検証の分割数、$L$は損失関数を表します。\n",
        "\n",
        "GridSearchは、LightGBMのような勾配ブースティングフレームワークでも広く使用されており、モデルの精度を最大限に引き出すために重要です。LightGBMにおいては、例えば最大深さ、学習率、葉の数などをチューニングするパラメータとして挙げられます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318c1e5d",
      "metadata": {
        "id": "318c1e5d"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Irisデータセットのロード\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# データセットの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMモデルの初期化\n",
        "model = LGBMClassifier()\n",
        "\n",
        "# ハイパーパラメータの範囲を定義\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 63],\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "    'n_estimators': [20, 40]\n",
        "}\n",
        "\n",
        "# GridSearchCVによるチューニングの設定\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# グリッドサーチの実行\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 最良のパラメータとスコアの出力\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n",
        "\n",
        "# チューニングされたモデルの性能をテストセットで評価\n",
        "best_model = grid_search.best_estimator_\n",
        "accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65fc841",
      "metadata": {
        "id": "f65fc841"
      },
      "source": [
        "### バギング (Bagging)\n",
        "\n",
        "バギング（Bootstrap Aggregating）は、統計学と機械学習の手法で、アンサンブル学習の一種です。個々のモデルのバリエーションを増やし、全体のパフォーマンスを向上させるために使われます。特に、過学習を防ぎ、モデルの汎化性能を向上させる効果があります。バギングは以下の手順で行われます：\n",
        "\n",
        "1. 元のデータセットからリプレイスを許した状態（復元抽出）で複数のサブセットを作成します。これをブートストラップサンプルと呼びます。\n",
        "2. 各サブセットに対して同じアルゴリズムを用いてモデルを複数作成します。\n",
        "3. これらのモデルを統合し、最終的な予測を行います。分類問題では多数決、回帰問題では平均を取るのが一般的です。\n",
        "\n",
        "バギングはLightGBMと親和性が高いです。LightGBMもアンサンブル学習の技術を利用していますが、ツリーモデルを利用したブースティングに特化しています。ただし、バギングを組み合わせることで、更なる精度向上が見込めます。\n",
        "\n",
        "数式で表すと、各モデルの予測を $h_i(x)$ とした場合、バギングによる最終予測 $\\hat{f}(x)$ は次のように表されます：\n",
        "\n",
        "- 分類問題の場合の最終予測:\n",
        "  $$ \\hat{f}(x) = \\text{mode}\\{h_1(x), h_2(x), ..., h_m(x)\\} $$\n",
        "  \n",
        "- 回帰問題の場合の最終予測:\n",
        "  $$ \\hat{f}(x) = \\frac{1}{m} \\sum_{i=1}^{m} h_i(x) $$\n",
        "\n",
        "ここで、$m$は作成したモデルの総数です。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "- 過学習の抑制\n",
        "- モデルの精度向上\n",
        "- ノイズの軽減\n",
        "\n",
        "バギングはリスクを分散するための手法とも言え、モデルの頑強性を向上させるためによく使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a9db6a",
      "metadata": {
        "id": "06a9db6a"
      },
      "outputs": [],
      "source": [
        "# Pythonでバギングをシミュレートする\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データセットをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# データセットをトレーニングとテストに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ベースラインモデルとしての決定木を構築\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "base_model.fit(X_train, y_train)\n",
        "y_pred_base = base_model.predict(X_test)\n",
        "\n",
        "# ベースラインの精度を計算して出力\n",
        "accuracy_base = accuracy_score(y_test, y_pred_base)\n",
        "print(f\"Baseline Accuracy: {accuracy_base:.2f}\")\n",
        "\n",
        "# バギングクラスターを用いてモデルの作成\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# バギングモデルのトレーニング\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで予測\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# バギングモデルの精度を計算して出力\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Accuracy: {accuracy_bagging:.2f}\")\n",
        "\n",
        "# バギングで精度が向上しているか確認\n",
        "print(\"バギングによる精度の向上は: \", accuracy_bagging > accuracy_base)\n",
        "\n",
        "# 各種変数を出力\n",
        "print(\"テスト予測(ベースライン):\", y_pred_base)\n",
        "print(\"テスト予測(バギング):\", y_pred_bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8092282",
      "metadata": {
        "id": "d8092282"
      },
      "source": [
        "### ブースティング (Boosting) の解説\n",
        "\n",
        "ブースティングとは、既存の学習モデルの精度を向上させるための手法の1つで、特に機械学習における回帰や分類問題で用いられます。ブースティングは、複数の弱い学習器（通常は決定木）を組み合わせて、1つの強力な学習器を作成することを目的としています。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "ブースティングでは、各ステップでエラーを最小化するように次の弱い学習器がトレーニングされ、これによりモデルの予測精度が向上します。典型的なブースティングアルゴリズムとしてAdaBoostやGradient Boostingがあります。\n",
        "\n",
        "例えば、Gradient Boostingの理論的背景は、以下のような回帰問題における目的関数 $L(y, F(x))$ の最小化として表現されます。\n",
        "\n",
        "- インライン数式: $ J = \\sum_{i=1}^{n} L(y_i, F(x_i)) $\n",
        "\n",
        "- ブロック数式:\n",
        "$$\n",
        "J = \\sum_{i=1}^{n} L(y_i, F(x_i))\n",
        "$$\n",
        "\n",
        "#### LightGBMとの関係性\n",
        "\n",
        "LightGBMは、Gradient Boostingの一種である高速で高性能なグラディエントブースティングフレームワークです。CART（Classification and Regression Trees、分類回帰木）を学習器として利用し、ヒストグラムベースの技法を使用することで学習速度と精度を向上させています。\n",
        "\n",
        "#### 使用用途\n",
        "- クラス分類\n",
        "- 回帰問題\n",
        "- ランキング\n",
        "\n",
        "など、さまざまなタイプのデータに対して適用可能です。特に大規模なデータセットに対するパフォーマンスに優れています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdde6b8",
      "metadata": {
        "id": "9cdde6b8"
      },
      "outputs": [],
      "source": [
        "# Pythonコードによるブースティングの基本イメージ\n",
        "\n",
        "from sklearn.datasets import load_boston  # データセットをロード\n",
        "from sklearn.model_selection import train_test_split  # データを分割\n",
        "from sklearn.ensemble import GradientBoostingRegressor  # 回帰のためのGradient Boosting\n",
        "\n",
        "# ボストンデータセットのロード\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# データをトレーニングセットとテストセットに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Gradient Boosting Regressorのインスタンスを作成\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# モデルを学習\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# トレーニングデータを使ってR^2スコアを計算\n",
        "train_score = gbr.score(X_train, y_train)\n",
        "print('Training R^2 score:', train_score)\n",
        "\n",
        "# テストデータを使ってR^2スコアを計算\n",
        "test_score = gbr.score(X_test, y_test)\n",
        "print('Test R^2 score:', test_score)\n",
        "\n",
        "# 変数 y_train と y_test の中身を出力\n",
        "y_train_sample = y_train[:5]  # サンプルとして最初の5つを表示\n",
        "print('y_train sample:', y_train_sample)\n",
        "\n",
        "y_test_sample = y_test[:5]  # サンプルとして最初の5つを表示\n",
        "print('y_test sample:', y_test_sample)\n",
        "\n",
        "# モデルの予測例を出力\n",
        "predictions = gbr.predict(X_test[:5])\n",
        "print('Predictions:', predictions)\n",
        "\n",
        "# これにより、トレーニングデータとテストデータのスコア、\n",
        "# およびいくつかの予測値が出力され、Gradient Boostingがどのように機能するのかを理解するのに役立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6816052e",
      "metadata": {
        "id": "6816052e"
      },
      "source": [
        "損失関数とは、モデルの予測と実際の値とのずれ（誤差）を定量的に測定するための関数です。この関数を最小化することで、モデルの性能を最適化します。Log Loss（対数損失）とも呼ばれる損失関数は、特に分類問題において評価指標として使用されます。Log Lossは次の数式で表されます：\n",
        "\n",
        "$$\n",
        "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n",
        "$$\n",
        "\n",
        "ここで$N$はサンプル数、$y_i$は実際のラベル（0または1）、$p_i$は予測確率です。ロジスティック回帰のように確率出力を提供するモデルの評価に便利です。\n",
        "\n",
        "LightGBM（Light Gradient Boosting Machine）は、勾配ブースティングフレームワークの一つで、高速かつ効率的に大規模データから学習できます。LightGBMはLog Lossを使用して、バイナリ分類タスクの評価を行います。\n",
        "\n",
        "使用用途としては、分類タスクのモデル選択やハイパーパラメータ調整の際に、モデルの性能を確認するための指標として用いられます。特に確率出力が必要なケースで有効です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d8329b",
      "metadata": {
        "id": "65d8329b"
      },
      "outputs": [],
      "source": [
        "# Log Lossを計算するためのPythonコード\n",
        "import numpy as np\n",
        "\n",
        "# 実際のラベルと予測確率を定義\n",
        "# y_trueが実際のラベル、y_predがモデルの予測確率\n",
        "# 0: ネガティブクラス, 1: ポジティブクラス\n",
        "\n",
        "y_true = np.array([0, 1, 1, 0])\n",
        "y_pred = np.array([0.1, 0.9, 0.8, 0.4])\n",
        "\n",
        "# Log Lossを計算する関数を定義します。\n",
        "def log_loss(y_true, y_pred):\n",
        "    # 安全な計算のために予測確率をクリップします。\n",
        "    # 非常に小さいまたは大きい確率を防ぐため\n",
        "    eps = 1e-15\n",
        "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "\n",
        "    # Log Lossの計算式を実装します。\n",
        "    logloss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return logloss\n",
        "\n",
        "# 関数を用いてLog Lossを計算し、出力します。\n",
        "computed_log_loss = log_loss(y_true, y_pred)\n",
        "print(\"Computed Log Loss:\", computed_log_loss)\n",
        "\n",
        "# 変数の中身を確認\n",
        "y_true, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa60a8f8",
      "metadata": {
        "id": "aa60a8f8"
      },
      "source": [
        "データの重み付け（データウェイティング）とは、各データポイントに異なる重要度を与えるプロセスです。たとえば、特定のデータポイントが他のデータポイントよりも結果に与えたい影響が大きい場合、そのデータポイントにより高い重みを設定することができます。LightGBMにおいては、重み付けを通じてモデルの学習において特定のデータの影響度を調整することができます。理論的には、重み付けを行うことでモデルが誤差をより重要なデータに集中させることができ、結果的により精度の高いモデルを得ることが可能です。具体的には、損失関数においてデータポイントの誤差に重みをかける形で重み付けを考慮します。\\(L(w, x, y) = w \\cdot \text{Loss}(x, y)\\) ここで、\\(w\\)はデータポイントの重み、\\(x\\)は特徴量、\\(y\\)はターゲット値、\\(\text{Loss}(x, y)\\)は損失関数です。LightGBMでは、この重みを例として `instance_weight` というパラメータで設定することができます。使用用途としては、クラス分布の不均衡の是正、異なる信頼度を持つデータセットの併用などがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e27e1eb",
      "metadata": {
        "id": "3e27e1eb"
      },
      "outputs": [],
      "source": [
        "# データの重み付けを考慮した損失計算の例\n",
        "\n",
        "# サンプルデータ（特徴量、ターゲット、重み）\n",
        "data = [\n",
        "    {'features': [0.1, 0.2], 'target': 1, 'weight': 0.5},\n",
        "    {'features': [0.2, 0.1], 'target': 0, 'weight': 1.5},\n",
        "    {'features': [0.3, 0.4], 'target': 1, 'weight': 0.8}\n",
        "]\n",
        "\n",
        "# 仮の予測結果（本来はモデルの予測が入る）\n",
        "predictions = [0.6, 0.3, 0.8]\n",
        "\n",
        "# 損失関数の計算（ここでは二乗誤差を使用）\n",
        "def weighted_loss(data, predictions):\n",
        "    total_loss = 0.0\n",
        "    for i, entry in enumerate(data):\n",
        "        prediction = predictions[i]\n",
        "        target = entry['target']\n",
        "        weight = entry['weight']\n",
        "        # 重み付き損失の計算\n",
        "        loss = weight * (prediction - target) ** 2\n",
        "        total_loss += loss\n",
        "        # 各項目の損失を表示\n",
        "        print(f\"データ {i+1}：予測: {prediction}, ターゲット: {target}, 重み: {weight}, 損失: {loss}\")\n",
        "    return total_loss\n",
        "\n",
        "# 重み付き損失の計算および出力\n",
        "total_weighted_loss = weighted_loss(data, predictions)\n",
        "print(f\"総重み付き損失: {total_weighted_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42352075",
      "metadata": {
        "id": "42352075"
      },
      "source": [
        "ランダムサンプリングは、データセットからランダムにサンプルを選び出す手法です。この方法は、データを統計的に扱うための基礎となります。ランダムにサンプリングすることで、データのバイアスを排除し、多様なサンプルを取得することができます。LightGBMは、データの分布に依存せずにランダムなサブセットを選んでモデルをトレーニングするため、計算コストを削減しつつ精度を高めることを可能にします。\n",
        "\n",
        "インライン数式としてランダムサンプリングは、確率 $P$ を利用します。各データポイントが選ばれる確率 $P = \\frac{1}{N}$ です。\n",
        "\n",
        "ブロック数式としてのランダムサンプリングは以下の通りです：\n",
        "$$ P(x_i) = \\frac{1}{N} $$\n",
        "ここで $x_i$ はデータポイントを表し、$N$ はデータセット内の総データ数です。\n",
        "\n",
        "使用用途としては、以下が挙げられます：\n",
        "- データセット全体を使用するのが計算的に非現実的な場合に、トレーニングやテストに利用。\n",
        "- 各クラス間の不均衡を解消するための、データセットのバランス調整。\n",
        "- アンサンブル学習における異なるモデル間の多様性を確保するため。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e9ecdc",
      "metadata": {
        "id": "06e9ecdc"
      },
      "outputs": [],
      "source": [
        "# ランダムサンプリングの実装例\n",
        "import numpy as np\n",
        "\n",
        "# 全データセットを100から500までの整数に制限\n",
        "full_dataset = np.arange(100, 500)\n",
        "\n",
        "# ランダムサンプリングするサンプルサイズを指定\n",
        "sample_size = 50\n",
        "\n",
        "# numpyのrandom.choiceを使ってランダムにサンプリング\n",
        "random_sample = np.random.choice(full_dataset, size=sample_size, replace=False)\n",
        "\n",
        "# サンプリングされたデータを表示\n",
        "print(\"Random Sample:\", random_sample)\n",
        "\n",
        "# 変数中身の確認\n",
        "print(\"Full Dataset Length:\", len(full_dataset))  # 元のデータセットのサイズ\n",
        "print(\"Sample Size:\", sample_size)  # 選んだサンプルサイズ\n",
        "print(\"Sample Length:\", len(random_sample))  # 実際に得られたサンプルのサイズ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56f7149",
      "metadata": {
        "id": "f56f7149"
      },
      "source": [
        "### SHAP値について\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) 値は、各特徴量が予測に与える影響を測定するために使用される評価指標です。ゲーム理論に基づいており、Shapley値として知られています。モデルの出力を特徴量に応じて分解することで、各特徴がどの程度予測に寄与しているかを理解できます。\n",
        "\n",
        "理論的には、SHAP値は、あるプレイヤー（特徴量）がゲーム（モデルの予測）にどれだけ貢献したかを示します。Shapley値は次のように定義されます：\n",
        "\n",
        "- インライン数式: $\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} (v(S \\cup \\{i\\}) - v(S))$\n",
        "\n",
        "ここで、\\( \\phi_i \\) は特徴量 \\( i \\) のShapley値、\\( S \\) は特徴量の部分集合（\\( i \\) を含まない）、\\( N \\) は全ての特徴量の集合、\\( v(S) \\) は \\( S \\) に属する特徴量のみを使用したときの予測値です。\n",
        "\n",
        "- ブロック数式:\n",
        "\n",
        "$$\n",
        "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} (v(S \\cup \\{i\\}) - v(S))\n",
        "$$\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "SHAP値はモデル解釈のための強力なツールです。具体的な使用用途には次のようなものがあります。\n",
        "\n",
        "1. **特徴量の重要性評価**: モデルの予測に対する各特徴量の貢献度を視覚的に分析できます。\n",
        "2. **モデルの解釈性改善**: 複雑なモデルでも、人間が理解しやすい形でモデルの挙動を説明できます。\n",
        "3. **異常値の検出**: 特定の入力がどのようにして異常な出力を生成するかを調べることができます。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは高速かつ高性能な勾配ブースティングアルゴリズムです。SHAP値はLightGBMを含む様々なモデルに応用可能で、特に木構造を用いたモデルでは、SHAP値の計算が効率的に行えます。これにより、LightGBMで構築したモデルの解釈性を大幅に向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dd3416",
      "metadata": {
        "id": "75dd3416"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# データの用意（例として、ランダムなデータを使用します）\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.random.binomial(1, 0.5, 100)\n",
        "\n",
        "# LightGBMのデータセットに変換\n",
        "data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# モデルの訓練\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss'\n",
        "}\n",
        "\n",
        "# LightGBMモデルの訓練\n",
        "clf = lgb.train(params, data, num_boost_round=20)\n",
        "\n",
        "# SHAP値の計算\n",
        "explainer = shap.TreeExplainer(clf)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# SHAP値を利用した可視化\n",
        "# summary_plotは全体の特徴量重要度の可視化を行います。\n",
        "shap.summary_plot(shap_values, X)\n",
        "\n",
        "# SHAP値の確認\n",
        "print(\"SHAP値:\", shap_values)\n",
        "\n",
        "# 各データポイントに対するSHAP値のインパクトの可視化\n",
        "# summary_plotを再度利用して確認\n",
        "def print_summary_plot():\n",
        "    shap.summary_plot(shap_values, X)\n",
        "\n",
        "# SHAPのforce_plotを使って、個別のデータポイントがどのように予測値に影響するかを可視化\n",
        "individual_index = 0  # 特定のデータポイントのインデックス\n",
        "shap.initjs()  # JavaScriptの初期化\n",
        "force_plot_ = shap.force_plot(explainer.expected_value, shap_values[individual_index, :], X[individual_index, :])\n",
        "\n",
        "print(\"個別データポイントのSHAP force plot:\", force_plot_)\n",
        "\n",
        "# 変数の中身を確認\n",
        "def print_variables_check():\n",
        "    print(\"データセットXの最初のレコード:\", X[0])\n",
        "    print(\"予測ラベルの最初の要素:\", y[0])\n",
        "    print(\"訓練モデルのパラメータ:\", params)\n",
        "\n",
        "# 実行して確認\n",
        "def execute_all():\n",
        "    print_variables_check()\n",
        "    print_summary_plot()\n",
        "\n",
        "execute_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da1a693",
      "metadata": {
        "id": "9da1a693"
      },
      "source": [
        "ハイパーパラメータの最適化は、モデルのパフォーマンスを最大化するために設定すべきモデルのハイパーパラメータを調整するプロセスです。ハイパーパラメータは学習率や決定木の深さなど、モデルの構造や学習アルゴリズムの動作を調整するためのパラメータです。LightGBMは勾配ブースティングのためのフレームワークで、効果的なハイパーパラメータの設定が性能向上に直結します。理論的には、ハイパーパラメータの最適化は次のように定式化できます：\n",
        "\n",
        "$\\theta^* = \\arg\\max_{\\theta} \\mathbb{E}_{(X, y) \\sim D}[L(X, y; \\theta)]$\n",
        "\n",
        "ここで、$\\theta$はハイパーパラメータのセット、$D$はデータセット、$L$はロス関数です。\n",
        "\n",
        "$$\\theta^* = \\arg\\max_{\\theta} \\mathbb{E}_{(X, y) \\sim D}[L(X, y; \\theta)]$$\n",
        "\n",
        "多くのアルゴリズムがこの最適化問題を解決するために使用されます。代表的な手法にはグリッドサーチ、ランダムサーチ、ベイズ最適化などがあります。LightGBMでは、これらの手法を使ってハイパーパラメータを最適化し、モデルの精度や計算時間を改善します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5246ad5",
      "metadata": {
        "id": "c5246ad5"
      },
      "outputs": [],
      "source": [
        "# ハイパーパラメータの最適化をPythonで実装する例として、Scikit-learnのGridSearchCVを使用した例\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# データのロード\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# LightGBM分類器を作成\n",
        "model = LGBMClassifier()\n",
        "\n",
        "# ハイパーパラメータの候補を定義\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 50],\n",
        "    'max_depth': [-1, 10, 20],\n",
        "    'learning_rate': [0.1, 0.05, 0.01]\n",
        "}\n",
        "\n",
        "# グリッドサーチを設定\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# グリッドサーチの実行\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# ベストハイパーパラメータの表示\n",
        "grid_best_params = grid_search.best_params_\n",
        "print('Best Hyperparameters:', grid_best_params)\n",
        "\n",
        "# ベストスコアの表示\n",
        "grid_best_score = grid_search.best_score_\n",
        "print('Best Cross-Validation Accuracy:', grid_best_score)\n",
        "\n",
        "# コメント:\n",
        "# - データセットとしてIrisを使用し、LightGBMの分類器でモデルを作成\n",
        "# - ハイパーパラメータの候補は辞書形式で定義し、GridSearchCVで最適化を実行\n",
        "# - ベストなハイパーパラメータセットとそれに対するクロスバリデーション精度を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c877c5f7",
      "metadata": {
        "id": "c877c5f7"
      },
      "source": [
        "予測確率のキャリブレーションとは、機械学習モデルが出力する確率を実際の確率に一致させるための手法です。たとえば、あるクラスの発生確率を80%と予測した場合、そのクラスが実際に80%の確度で発生するように調整（校正）を行います。予測確率キャリブレーションの主な理論は、予測される確率と実際の分布を対応付けることです。この調整は、モデルの性能を客観的に評価するために重要です。\n",
        "\n",
        "具体例として、バイナリ分類モデルの出力をキャリブレーションする方法として`Platt Scaling`や`Isotonic Regression`があります。これらは、モデルの出力スコアを実際の確率に変換するための手法です。\n",
        "\n",
        "数式的には、予測確率 \\( \\hat{p} \\) と実際の確率 \\( p \\) の差異を小さくすることを目指します。主な方法として二乗誤差を用いることがあります：\n",
        "\n",
        "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (p_i - \\hat{p}_i)^2 \\]\n",
        "\n",
        "キャリブレーションは、たとえば信用リスク評価、医療の診断支援、天気予報など、確率的な意思決定が関係する分野で広く用いられています。LightGBMモデルでも、予測確率をキャリブレーションすることでモデルの信頼性を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5138853a",
      "metadata": {
        "id": "5138853a"
      },
      "outputs": [],
      "source": [
        "# Pythonで予測確率のキャリブレーションを実施する例\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# データセットの準備\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ロジスティック回帰モデルのトレーニング（生の確率予測）\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# キャリブレーションなしの校正曲線\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, probs, n_bins=10)\n",
        "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"未校正モデル\")\n",
        "\n",
        "# モデルのキャリブレーション\n",
        "calibrated_model = CalibratedClassifierCV(base_estimator=LogisticRegression(), cv=\"prefit\")\n",
        "calibrated_model.fit(X_test, y_test)\n",
        "calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# キャリブレーション後の校正曲線\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, calibrated_probs, n_bins=10)\n",
        "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"キャリブレーション後\")\n",
        "\n",
        "# 完全なキャリブレーションの場合の線\n",
        "plt.plot([0, 1], [0, 1], \"--\", label=\"完全なキャリブレーション\")\n",
        "\n",
        "plt.xlabel(\"予測値\")\n",
        "plt.ylabel(\"実際の確率\")\n",
        "plt.title(\"予測確率のキャリブレーション\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "# 生の確率予測を出力\n",
        "print(\"生の確率予測:\", probs[:5])\n",
        "# キャリブレーション後の確率予測を出力\n",
        "print(\"キャリブレーション後の確率予測:\", calibrated_probs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da85f1db",
      "metadata": {
        "id": "da85f1db"
      },
      "source": [
        "### 分布適応型のバギング (Balanced Bagging)の解説\n",
        "\n",
        "分布適応型のバギングは、特に不均衡データセットにおけるバギング性能を改善するための手法です。通常のバギングは、ランダムにデータをサンプリングしてそれぞれのモデルを学習しますが、不均衡データにおいては少数クラスのデータが十分にサンプリングされないことがあります。分布適応型のバギングでは、少数クラスのデータを優先的にサンプリングすることで、この問題に対処します。\n",
        "\n",
        "理論的には、バギングの各ステップで異なるサンプリング分布を用いることで、モデルのバランスを取ることが狙いです。\n",
        "\n",
        "**数式**\n",
        "\n",
        "インライン数式で表現すると、サンプリングの確率を $p_{i} = \\frac{1}{n_{i}}$ とします。ただし、$n_{i}$ はクラス $i$ のサンプル数を表します。\n",
        "\n",
        "ブロック数式で示すと、\n",
        "\n",
        "$$\n",
        "P(Sample = x) = \\begin{cases}\n",
        "\\frac{1}{n_+} & \\text{if } x \\text{ is a positive sample} \\\\\n",
        "\\frac{1}{n_-} & \\text{if } x \\text{ is a negative sample}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "ここで、$n_+$ と $n_-$ はそれぞれ少数クラスと多数クラスのサンプル数です。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは、効率よく大規模データを扱うための勾配ブースティングフレームワークですが、Balanced Baggingの概念を取り入れることで、特に不均衡なデータセットにおいて、より良いパフォーマンスを発揮します。LightGBMにおいても、クラスごとのサンプリング比率を設定することで、このコンセプトを適用することができます。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "分布適応型のバギングは、不均衡なクラス分布を持つ分類問題に対して有効です。この手法を用いることで、特に少数クラスの検出精度を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d261e82",
      "metadata": {
        "id": "2d261e82"
      },
      "outputs": [],
      "source": [
        "# Balanced Bagging の基本的な概念を Python でシミュレーションするコード例\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# クラスラベルとサンプル数を設定\n",
        "class_labels = np.array([0] * 90 + [1] * 10)  # 0: negative, 1: positive\n",
        "np.random.shuffle(class_labels)  # ランダムな順序に並べ替え\n",
        "\n",
        "# サンプリングの確率を設定 (逆数使用)\n",
        "class_sample_counts = np.array([np.sum(class_labels == 0), np.sum(class_labels == 1)])\n",
        "print('Class Sample Counts:', class_sample_counts)\n",
        "\n",
        "sampling_probabilities = 1.0 / class_sample_counts\n",
        "sampling_probabilities /= sampling_probabilities.sum()\n",
        "print('Sampling Probabilities:', sampling_probabilities)\n",
        "\n",
        "# サンプリング実施\n",
        "n_samples = 20  # サンプリング数を決定\n",
        "sample_indices = np.random.choice(\n",
        "    len(class_labels), size=n_samples, replace=True, p=sampling_probabilities\n",
        ")\n",
        "\n",
        "# サンプルの抽出\n",
        "sampled_labels = class_labels[sample_indices]\n",
        "print('Sampled Labels:', sampled_labels)\n",
        "\n",
        "# 各クラスの抽出サンプル数を出力\n",
        "sampled_class_counts = [np.sum(sampled_labels == 0), np.sum(sampled_labels == 1)]\n",
        "print('Sampled Class Counts:', sampled_class_counts)\n",
        "\n",
        "# このようにして、少数クラスから多くのサンプルを得ることができる"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a0c544",
      "metadata": {
        "id": "f9a0c544"
      },
      "source": [
        "### スパースデータの処理\n",
        "\n",
        "スパースデータとは、データセット内に多くのゼロを含むデータのことを指します。例えば、テキストデータのBag of Wordsやユーザアイテム行列などが該当します。このようなデータは、メモリ効率や計算効率の観点から、特別な処理が必要です。\n",
        "\n",
        "LightGBMはスパースデータを処理するための最適化が施されています。具体的には以下の方法が用いられています：\n",
        "\n",
        "1. **スパース特徴のサポート**：LightGBMはスパースな特徴を直接扱うことができます。これにより、スパースなデータ構造のメモリ効率が向上します。\n",
        "\n",
        "2. **ゼロの排除**：スパース性を利用して、ゼロの情報を効率的に排除し、計算資源を節約します。\n",
        "\n",
        "3. **直交配列とHistogram計算の効率化**：LightGBMは、計算を高速化するためにHistogramベースの決定木構築を行います。これは、スパースデータにおいても有効です。\n",
        "\n",
        "数式として、スパースベクトル\\( x \\)は次のように表されます：\n",
        "\n",
        "- フルベクトル: \\( x = [x_1, x_2, ..., x_n] \\)\n",
        "- スパースベクトル: \\( x = (i_k, v_k) \\) where \\( v_k \\neq 0 \\), \\( i_k \\)はインデックス\n",
        "\n",
        "LightGBMではこのようなスパースデータを効率的に扱うために特化したアルゴリズムを用いており、学習効率および予測精度の向上が期待できます。\n",
        "\n",
        "#### 使用用途\n",
        "スパースデータは多くの分野で活用されます。特に以下のような用途で有効です：\n",
        "- テキスト分類\n",
        "- レコメンデーションシステム\n",
        "- 高次元特徴を持つデータの解析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77003da",
      "metadata": {
        "id": "c77003da"
      },
      "outputs": [],
      "source": [
        "# スパースデータの例として、SciPyのcsr_matrixを使用します\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# スパースデータの作成\n",
        "# 9つの要素のうち、実際に使うのは3つ\n",
        "row = np.array([0, 1, 2])\n",
        "col = np.array([0, 2, 1])\n",
        "data = np.array([1, 2, 3])\n",
        "\n",
        "# CSR (Compressed Sparse Row) matrixの作成\n",
        "sparse_matrix = csr_matrix((data, (row, col)), shape=(3, 3))\n",
        "\n",
        "# スパースマトリクスの内容を出力\n",
        "print(\"Sparse Matrix:\\n\", sparse_matrix)\n",
        "\n",
        "# 密な表現に変換（理解のため）\n",
        "dense_matrix = sparse_matrix.toarray()\n",
        "print(\"\\nDense Matrix:\\n\", dense_matrix)\n",
        "\n",
        "# LightGBMでスパースデータを扱う例\n",
        "# LightGBMはスパース行列をそのまま受け取ります\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ダミーのデータとラベル\n",
        "X = sparse_matrix\n",
        "y = np.array([0, 1, 0])\n",
        "\n",
        "# データセットの作成\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# 訓練用のパラメータ\n",
        "params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"verbose\": -1  # ログを減らす\n",
        "}\n",
        "\n",
        "# モデルの訓練\n",
        "bst = lgb.train(params, train_data, num_boost_round=10)\n",
        "\n",
        "# ここではモデルを予測に使用することを示します\n",
        "print(\"Model prediction for sparse input:\", bst.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370b21a7",
      "metadata": {
        "id": "370b21a7"
      },
      "source": [
        "### 学習曲線のプロット (Learning Curve)\n",
        "\n",
        "学習曲線 (Learning Curve) は、モデルの性能とトレーニングセットのサイズまたはトレーニングの時間の関係を示すプロットです。このプロットは、モデルがデータを学習する過程での誤差の変化を視覚化するのに役立ちます。\n",
        "\n",
        "#### 理論:\n",
        "学習曲線は通常、横軸にトレーニングセットのサイズ、縦軸にモデルの誤差（例えば、損失や精度）を描画します。トレーニングデータが増えるに連れて、モデルのパフォーマンスがどのように変化するかを示します。\n",
        "\n",
        "- **過学習 (Overfitting):** トレーニングセットのパフォーマンスは良好ですが、テストまたは検証セットではパフォーマンスが低い。\n",
        "\n",
        "- **未学習 (Underfitting):** トレーニングセットでもテストセットでもパフォーマンスが悪い。\n",
        "\n",
        "数式で表すと、\\( E_{\text{train}} \\) はトレーニングセットの誤差、\\( E_{\text{test}} \\) はテストセットの誤差です。\n",
        "$$ E_{\text{train}}(m) = f(m) $$\n",
        "$$ E_{\text{test}}(m) = g(m) $$\n",
        "ここで、\\( m \\) はトレーニングデータのサイズを示しています。\n",
        "\n",
        "#### LightGBMとの関係性:\n",
        "LightGBMは高効率な勾配ブースティング決定木アルゴリズムであり、学習曲線をプロットすることで、LightGBM モデルがデータに対してどのように学習を進めているかを理解するのに役立ちます。\n",
        "\n",
        "#### 使用用途:\n",
        "1. モデル選択とハイパーパラメータ調整の際の助け。\n",
        "2. データセットサイズがどの段階でパフォーマンスに大きく影響するのかを理解する。\n",
        "3. 学習の進行状況をチェックし、過学習または未学習を特定する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f05f18",
      "metadata": {
        "id": "a5f05f18"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# データの読み込み\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# モデルの初期化\n",
        "model = LGBMClassifier()\n",
        "\n",
        "# 学習曲線を取得する関数\n",
        "def plot_learning_curve(estimator, X, y):\n",
        "    # 学習曲線を計算\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "\n",
        "    # 平均と標準偏差を計算\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # 学習曲線のプロット\n",
        "    plt.figure()\n",
        "    plt.title('Learning Curves')\n",
        "    plt.xlabel('Training examples')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    # トレーニングスコアのプロット\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color='r')\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',\n",
        "             label='Training score')\n",
        "\n",
        "    # テストスコアのプロット\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color='g',\n",
        "             label='Cross-validation score')\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "# モデルの学習曲線をプロット\n",
        "plot_learning_curve(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e771f1b5",
      "metadata": {
        "id": "e771f1b5"
      },
      "source": [
        "適応的学習率調整（Adaptive Learning Rate）は、機械学習アルゴリズムの訓練過程において使用される手法で、各重みパラメータの更新に用いる学習率を適応的に調整します。一般的な固定学習率のアルゴリズムに比べ、適応的に学習率を調整することにより、より効率的かつ効果的なモデルの訓練を可能にします。代表的な手法には、AdaGrad、RMSprop、Adamなどがあります。\n",
        "\n",
        "LightGBMは勾配ブースティングアルゴリズムを用いたライブラリで、特に大量のデータや高速な訓練が必要な場合に向いています。LightGBM自体は適応的学習率を直接実装しているわけではありませんが、学習率をschedulerや外部の調整アルゴリズムと組み合わせて使用することができます。\n",
        "\n",
        "**数式**:\n",
        "\n",
        "インライン数式の例: $\\theta_t = \\theta_{t-1} - \\eta_t \\cdot \\nabla L(\\theta_{t-1})$\n",
        "\n",
        "ブロック数式の例:\n",
        "\n",
        "$$\n",
        "\\theta_t = \\theta_{t-1} - \\eta_t \\cdot \\nabla L(\\theta_{t-1})\n",
        "$$\n",
        "\n",
        "ここで、$\\theta_t$は時刻$t$におけるモデルのパラメータ、$\\eta_t$は時刻$t$の学習率、$\\nabla L(\\theta_{t-1})$は損失関数の勾配です。\n",
        "\n",
        "適応的学習率は、特定の状況において学習率を調整し、誤った方向に進むのを防ぎ、学習の収束を促進します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05127ed",
      "metadata": {
        "id": "d05127ed"
      },
      "outputs": [],
      "source": [
        "# 適応的学習率調整の例を示すため、Adamオプティマイザーを使用した簡単なニューラルネットワークの例を示します。\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# シンプルなニューラルネットワークモデルの定義\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = SimpleModel().to(device)\n",
        "\n",
        "# 損失関数と最適化方法（Adam）\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adamは適応的学習率を使用する\n",
        "\n",
        "# ダミーデータの作成\n",
        "inputs = torch.randn(10, 10).to(device)  # 入力: 10サンプル、各サンプル10次元\n",
        "outputs = torch.randn(10, 1).to(device)  # 出力: 各サンプル1次元\n",
        "\n",
        "# 学習率の調整の様子を観察するためのディクショナリ\n",
        "learning_rates = []\n",
        "\n",
        "# トレーニングループ\n",
        "for epoch in range(5):  # 簡単な例なので5エポックだけ繰り返す\n",
        "    optimizer.zero_grad()  # 勾配の初期化\n",
        "    predictions = model(inputs)  # モデルの予測\n",
        "    loss = criterion(predictions, outputs)  # 損失の計算\n",
        "    loss.backward()  # 勾配の計算\n",
        "    optimizer.step()  # パラメータの更新\n",
        "\n",
        "    # 現在の学習率を取得して保存\n",
        "    for param_group in optimizer.param_groups:\n",
        "        current_lr = param_group['lr']\n",
        "        learning_rates.append(current_lr)\n",
        "        print(f'Epoch {epoch + 1}, Current Learning Rate: {current_lr}')  # 各epochの学習率を出力\n",
        "\n",
        "# 学習率がどのように変化しているかを表示\n",
        "print(\"Learning Rates over Epochs:\", learning_rates)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55733b3",
      "metadata": {
        "id": "c55733b3"
      },
      "source": [
        "### 対数尤度の計算について\n",
        "\n",
        "対数尤度（log-likelihood）は、統計モデルが観測データにどれだけ適合するかを表す指標です。尤度とは、与えられた観測データがモデルによって生成される確率を指し、対数尤度はその対数です。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "尤度の最大化は、パラメータ推定に用いられる手法の一つで、これを最大尤度推定（MLE）と呼びます。尤度関数$L(\\theta)$は観測データ$X$とパラメータ$\\theta$に依存し、多くの場合対数尤度$\\log L(\\theta)$を最大化することでパラメータを推定します。以下のように表されます。\n",
        "\n",
        "- インライン数式: $\\log L(\\theta) = \\sum_{i=1}^n \\log f(x_i | \\theta)$\n",
        "- ブロック数式:\n",
        "  $$\n",
        "  \\log L(\\theta) = \\sum_{i=1}^n \\log f(x_i | \\theta)\n",
        "  $$\n",
        "\n",
        "ここで、$f(x_i | \\theta)$は観測データ$x_i$がパラメータ$\\theta$の下で発生する確率です。\n",
        "\n",
        "#### LightGBMとの関係性\n",
        "\n",
        "LightGBMは勾配ブースティング決定木（GBDT）を元にした高性能なアルゴリズムであり、二項分類問題や多項分類問題などで対数尤度が使用されます。特に分類問題では、ロジスティック回帰における対数尤度が交差エントロピーとして用いられ、コスト関数を最適化するために重要な役割を果たします。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "対数尤度は以下のような用途があります。\n",
        "\n",
        "- 統計モデルのパラメータ推定\n",
        "- モデル選択（AICやBICの計算）\n",
        "- 機械学習におけるコスト関数の最適化\n",
        "\n",
        "これらは、モデルの複雑さやデータのフィットをバランス良く考慮するために利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af431be",
      "metadata": {
        "id": "9af431be"
      },
      "outputs": [],
      "source": [
        "# 対数尤度の計算をPythonで実装する例\n",
        "import numpy as np\n",
        "\n",
        "# 観測データ\n",
        "x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "print('観測データ:', x)\n",
        "\n",
        "# モデルの平均 (mu) と標準偏差 (sigma)\n",
        "mu = 3.0\n",
        "sigma = 1.0\n",
        "print('平均 (mu):', mu)\n",
        "print('標準偏差 (sigma):', sigma)\n",
        "\n",
        "# 正規分布における対数尤度関数を定義\n",
        "# f(x_i | theta) は正規分布 N(mu, sigma^2) の確率密度関数\n",
        "\n",
        "log_likelihood = -np.sum((x - mu)**2 / (2 * sigma**2) + np.log(sigma * np.sqrt(2 * np.pi)))\n",
        "print('対数尤度:', log_likelihood)\n",
        "\n",
        "# 対数尤度は与えられたデータがモデルにどれくらい適合するかを示す。\n",
        "# この数値が大きいほどモデルはデータに適合していると考えられる。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148ceb1e",
      "metadata": {
        "id": "148ceb1e"
      },
      "source": [
        "### 勾配ブースティング決定木 (GBDT) の基礎理論\n",
        "\n",
        "勾配ブースティング決定木 (Gradient Boosting Decision Tree, GBDT) は、複数の決定木を組み合わせて高精度な予測を行うためのアンサンブル学習手法です。アンサンブル学習とは、複数のモデルを組み合わせて予測パフォーマンスを向上させる方法です。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "GBDT の主要なアイデアは、基本の決定木を順次追加し、それぞれが前のモデルの誤差を修正するというものです。各ステップでの目的は、損失関数の負の勾配（最も急降下する方向）に従う方向で、予測誤差を減少させることです。\n",
        "\n",
        "GBDT における損失関数 $L(y, F(x))$ は、$y$ が真の値、$F(x)$ が予測される関数（モデルの予測値）です。新しいモデル $h(x)$ を追加することで、この損失を最小化しようとします。数学的には以下のような更新を行います：\n",
        "\n",
        "\\[\n",
        "F_{m+1}(x) = F_m(x) +\n",
        "u h_m(x)\n",
        "\\]\n",
        "\n",
        "ここで、$\\nu$ は学習率です。学習率は小さくすることで、モデルの柔軟性を高め、過学習を防ぐのに役立ちます。\n",
        "\n",
        "#### LightGBM との関係性\n",
        "\n",
        "LightGBM は、GBDT を効率的に実装するためのツールです。LightGBM はデータの量が多い場合やカテゴリカルデータが多い場合に特に有効です。いくつかの工夫（例：リーフワイズツリー成長戦略、ヒストグラムベースのアルゴリズム）により、高速な学習と小さいメモリ消費を実現しています。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "GBDT は分類問題や回帰問題に広く利用されています。特に、構造化データを扱う際や、非線形な関係を捉える必要があるタスクにおいて高いパフォーマンスを示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1132893",
      "metadata": {
        "id": "a1132893"
      },
      "outputs": [],
      "source": [
        "# GBDTの手作り実装 (簡易版)\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# 仮のデータセットを準備\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([5, 7, 9, 11, 13])  # 線形関係を持つデータ\n",
        "\n",
        "# モデルのパラメータ設定\n",
        "n_estimators = 3  # 木の数\n",
        "learning_rate = 0.1  # 学習率\n",
        "\n",
        "# 初期モデル（平均値）を考える\n",
        "F0 = np.mean(y)\n",
        "F_m = np.full(y.shape, F0)  # 初期予測\n",
        "print(\"Initial model predictions:\", F_m)\n",
        "\n",
        "# 決定木のリストを用意\n",
        "trees = []\n",
        "\n",
        "# ブースティングを開始\n",
        "for m in range(n_estimators):\n",
        "    # 現在の負の勾配（= 残差）を計算\n",
        "    residuals = y - F_m\n",
        "    print(f\"Residuals after iteration {m}:\", residuals)\n",
        "\n",
        "    # 決定木をトレイン\n",
        "    tree = DecisionTreeRegressor(max_depth=2)\n",
        "    tree.fit(X, residuals)\n",
        "    trees.append(tree)\n",
        "\n",
        "    # 新しい予測を作成\n",
        "    F_m = F_m + learning_rate * tree.predict(X)\n",
        "    print(f\"Model predictions after iteration {m}:\", F_m)\n",
        "\n",
        "print(\"Final model predictions:\", F_m)\n",
        "\n",
        "# 各ステップにおける決定木と残差を基に予測が更新される。\n",
        "# 実際のLightGBMではこれがかなり最適化され、効率的に計算される。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fee4ff5",
      "metadata": {
        "id": "0fee4ff5"
      },
      "source": [
        "推論速度の最適化には、モデルサイズの縮小、特徴量の選択、木構造の調整などが含まれます。LightGBMは勾配ブースティングの一種で、木構造による予測を高速化するための様々な最適化技術を導入しています。例えば、リーフワイズのツリーニューグローストラテジーを使用することで、計算コストを削減し推論を高速化します。\n",
        "\n",
        "数式上、一般的な木構造モデルの推論速度$S$は次のように表されます：\n",
        "\n",
        "$S = O(\text{depth} \times \\log(\text{leaves}))$\n",
        "\n",
        "ここで、$depth$は木の深さ、$leaves$はリーフの数です。LightGBMではデータの特徴に応じた最適なツリー構造を選択するため、推論速度が最適化されます。\n",
        "\n",
        "さらに、高速化には特徴量の数を減らす手法も含まれます。特徴量を削減することで、推論に必要な計算量が減少し、速度が向上します。LightGBMでは負の勾配方向を利用して効率的に特徴量を選択することで、推論速度の最適化を図っています。\n",
        "\n",
        "推論速度の最適化は、大規模なデータセットをリアルタイムで処理する必要があるアプリケーションで特に重要であり、エッジデバイスやモバイル環境でもスムーズな動作を実現します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1730fab",
      "metadata": {
        "id": "b1730fab"
      },
      "outputs": [],
      "source": [
        "# LightGBMでモデルの推論速度を最適化する例\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データセットを生成\n",
        "X, y = make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "\n",
        "# データセットを訓練用とテスト用に分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMのデータセットに変換\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# ハイパーパラメータ\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',  # GBDTによるブースティング\n",
        "    'num_leaves': 31,         # 高速化のためのリーフ数\n",
        "    'learning_rate': 0.05,    # 学習率\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# モデルを訓練\n",
        "model = lgb.train(params,\n",
        "                  train_data,\n",
        "                  valid_sets=[train_data, test_data],\n",
        "                  num_boost_round=100,\n",
        "                  early_stopping_rounds=10)\n",
        "\n",
        "# モデルによる予測を実行\n",
        "predictions = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "\n",
        "# 予測を二値化\n",
        "binary_predictions = np.rint(predictions)\n",
        "\n",
        "# 精度を評価\n",
        "accuracy = accuracy_score(y_test, binary_predictions)\n",
        "print(f'Accuracy: {accuracy}')  # 精度の出力\n",
        "\n",
        "# モデルの予測値を出力して確認\n",
        "print('Predictions:', predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661747db",
      "metadata": {
        "id": "661747db"
      },
      "source": [
        "LightGBMは勾配ブースティングに基づく決定木アルゴリズムで、特に高速な学習と高精度を両立することができる点が特徴です。一方で、クラスタリング手法はデータを非教師ありで分類する方法で、各データポイントを類似性に基づいてグループ分けします。クラスタリングとLightGBMを組み合わせることで、クラスタリングによりデータの特徴を捉えた後、その情報をLightGBMの学習に活用し、より効率的かつ精度の高い予測を行うことが可能になります。クラスタリングの結果を特徴量として追加することで、LightGBMの学習過程に新たな視点を与え、モデルの表現力を高めることができます。\n",
        "\n",
        "例えば、まずクラスタリングを用いてデータをいくつかのクラスタに分割し、そのクラスタ情報を特徴量としてLightGBMモデルに組み込むことが考えられます。この方法により、データセットの内部構造をモデルが意識しながら学習することができます。\n",
        "\n",
        "理論的には、クラスタリング結果をどのように特徴量として用いるかという点に工夫が必要です。例えば、クラスタ番号をそのまま特徴量にしたり、各クラスタ内の統計量（平均や分散など）を特徴量にすることが考えられます。\n",
        "\n",
        "具体的にはk-meansクラスタリングを用いる場合、各データポイントがどのクラスタに所属するかという情報を追加の特徴量としてLightGBMに渡すことができます。このとき、データセット$ X $、ターゲット$ y $であるとすると、まず$ X $に対してクラスタリングを行い、結果のクラスタ番号を$ X_{cluster} $としてLightGBMに渡します。数式で表すと次のようになります:\n",
        "\n",
        "$$ C = \text{kmeans}(X) $$\n",
        "\n",
        "ここで$ C $はクラスタ番号のベクトルです。そしてLightGBMに渡すデータセットは次のようになります:\n",
        "\n",
        "$$ X' = [X, C] $$\n",
        "\n",
        "この手法により、データの潜在パターンを捉えた特徴を利用することができ、モデルの性能向上につながります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26db2ed0",
      "metadata": {
        "id": "26db2ed0"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.cluster import KMeans\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# データセットの生成: ランダムな分類問題を生成\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# KMeansクラスタリングの適用\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "X_cluster = kmeans.fit_predict(X)\n",
        "\n",
        "# クラスタリングの結果を確認\n",
        "print(\"Cluster labels:\", X_cluster)\n",
        "\n",
        "# 元の特徴量にクラスタラベルを追加\n",
        "X_with_cluster = np.hstack((X, X_cluster.reshape(-1, 1)))\n",
        "\n",
        "# 追加された特徴量を確認\n",
        "print(\"X with cluster labels (first 5 samples):\\n\", X_with_cluster[:5])\n",
        "\n",
        "# LightGBM用のデータセットを作成\n",
        "lgb_train = lgb.Dataset(X_with_cluster, label=y)\n",
        "\n",
        "# LightGBMのハイパーパラメータを設定\n",
        "params = {\n",
        "    'objective': 'binary',  # 2クラス分類問題\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# モデルの学習\n",
        "gbm = lgb.train(params, lgb_train, num_boost_round=100)\n",
        "\n",
        "# モデルの学習完了\n",
        "print(\"Model training completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}