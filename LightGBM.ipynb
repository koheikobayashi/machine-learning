{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koheikobayashi/machine-learning/blob/main/LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68363a16",
      "metadata": {
        "id": "68363a16"
      },
      "source": [
        "### Gini Impurityの解説\n",
        "Gini Impurity（ジニ不純度）は、主に決定木アルゴリズムで使われる不純度指標の1つです。分類問題におけるデータの純度を測定するために利用されます。Gini Impurityの値は0から1の範囲を取り、値が小さいほどデータが純粋であることを示します。\n",
        "\n",
        "**数式**: 不純度は次のように形式化されます。\n",
        "\n",
        "\\[\n",
        "Gini(p) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "\\]\n",
        "\n",
        "ここで、\\(p_i\\)はクラス\\(i\\)のサンプルの割合、\\(C\\)はクラスの総数です。\n",
        "\n",
        "**使用用途**: 決定木において、ノードの分割点を選択する際に使われます。Gini Impurityが最小になるように分割を行います。この指標を使用することで、ノードの純度を高める（つまり、ノード内のデータがなるべく同じクラスに属するような分割を行う）ことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fef03f",
      "metadata": {
        "id": "f3fef03f"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "def gini_impurity(labels):\n",
        "    total_count = len(labels)\n",
        "    if total_count == 0:\n",
        "        return 0\n",
        "    label_counts = Counter(labels)\n",
        "    impurity = 1\n",
        "    for label in label_counts:\n",
        "        prob_of_label = label_counts[label] / total_count\n",
        "        impurity -= prob_of_label ** 2\n",
        "    return impurity\n",
        "\n",
        "# 使用例:\n",
        "labels = [\"cat\", \"dog\", \"dog\", \"cat\", \"dog\", \"cat\", \"cat\"]\n",
        "gini = gini_impurity(labels)\n",
        "print(\"Gini Impurity:\", gini)\n",
        "```\n",
        "このコードは、与えられたラベルのリストに基づいてGini Impurityを計算します。Counterを使って各ラベルの出現数をカウントし、その出現確率を元にジニ不純度を求めます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a33306d",
      "metadata": {
        "id": "2a33306d"
      },
      "source": [
        "エントロピーは、情報理論や熱力学で重要な概念であり、一般にシステムの不確実性や乱雑さの度合いを定量化するために使用されます。情報理論においては、シャノンエントロピーが最もよく知られており、確率分布に基づいて情報の不確実性を測定します。シャノンエントロピー $H(X)$ は、離散的な確率変数 $X$ に対して以下のように定義されます。\n",
        "\n",
        "\\[\n",
        "H(X) = - \\sum_{i=1}^n P(x_i) \\log_b P(x_i)\n",
        "\\]\n",
        "\n",
        "ここで、$P(x_i)$ は $X$ が値 $x_i$ を取る確率であり、$b$ は対数の底を示します。一般的には、自然対数（$b=e$）または二進対数（$b=2$）が使われます。エントロピーはデータ圧縮や通信の理論的限界を理解するためにも利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1237bd",
      "metadata": {
        "id": "1a1237bd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_entropy(probabilities, base=2):\n",
        "    \"\"\"\n",
        "    Calculate the Shannon entropy of a probability distribution.\n",
        "\n",
        "    Parameters:\n",
        "    probabilities (list): A list of probabilities for each event.\n",
        "    base (int): The logarithmic base to use, default is 2.\n",
        "\n",
        "    Returns:\n",
        "    float: The Shannon entropy.\n",
        "    \"\"\"\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities) / np.log(base))\n",
        "    return entropy\n",
        "\n",
        "# 使用例\n",
        "probabilities = [0.25, 0.25, 0.25, 0.25]  # 四つの等しい確率\n",
        "entropy = calculate_entropy(probabilities)\n",
        "print(f'エントロピー: {entropy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be606912",
      "metadata": {
        "id": "be606912"
      },
      "source": [
        "MSE（Mean Squared Error、平均二乗誤差）は、回帰分析やモデル性能評価において最も一般的な損失関数の一つです。これは、ある予測モデルが実際のデータとどれだけずれているかを定量的に評価するための指標として使われます。MSEは予測値と実際の値の差の二乗を平均した値として計算されます。\n",
        "\n",
        "理論的には、MSEは次のように表されます：\n",
        "\n",
        "\\[\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "\n",
        "ここで、 \\(y_i\\) は実際の値、\\(\\hat{y}_i\\) は予測値、\\(n\\) はデータポイントの総数です。\n",
        "\n",
        "MSEの特徴として、二乗を取ることで予測誤差が大きな値に敏感になる点が挙げられます。これにより、大きな誤差に対するペナルティが強化されます。また、MSEは尺度の次元（例：平方メートルなど）に依存するため、異なるデータセット間での比較には不向きです。\n",
        "\n",
        "MSEは多くの最適化アルゴリズムで最小化される目的関数としても使用され、回帰タスクではモデルのパフォーマンスを改善するために重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e4d769",
      "metadata": {
        "id": "81e4d769"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 実際の値と予測値の例\n",
        "actual_values = np.array([3, -0.5, 2, 7])\n",
        "predicted_values = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "# MSEの計算\n",
        "mse = np.mean((actual_values - predicted_values) ** 2)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfc756e",
      "metadata": {
        "id": "7bfc756e"
      },
      "source": [
        "情報利得 (Information Gain) は、特に決定木の構築において重要な概念で、データセットにおけるランダム性の減少を示します。これは特定の属性でデータを分割することにより得られる不確実性の減少を定量化するものです。情報利得はエントロピーの概念に基づいています。\n",
        "\n",
        "エントロピー \\( H(D) \\) は、データセット \\( D \\) の不確実性の尺度であり、以下のように定義されます。\n",
        "\n",
        "\\[\n",
        "H(D) = - \\sum_{i=1}^{n} p(c_i) \\log_2 p(c_i)\n",
        "\\]\n",
        "\n",
        "ここで \\( p(c_i) \\) はクラス \\( i \\) の発生確率です。\n",
        "\n",
        "情報利得 \\( IG(D, A) \\) は、属性 \\( A \\) を用いてデータセット \\( D \\) を分割したときのエントロピーの減少として定義され、次のように表されます。\n",
        "\n",
        "\\[\n",
        "IG(D, A) = H(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} H(D_v)\n",
        "\\]\n",
        "\n",
        "ここで、\\( D_v \\) は \\( A \\) の値 \\( v \\) を持つサブセットです。\n",
        "\n",
        "情報利得は、決定木アルゴリズム（例えばID3やC4.5）の中で、データを分割するのに最適な属性を選択するために使用されます。この属性選択の過程を通じて、モデルは効率的に学習し、予測の精度を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc690d9",
      "metadata": {
        "id": "1cc690d9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# エントロピーを計算する関数\n",
        "def entropy(labels):\n",
        "    # クラスのラベルごとの頻度を計算\n",
        "    label_counts = np.bincount(labels)\n",
        "    probabilities = label_counts / len(labels)\n",
        "    # np.log2(0) は NaN を返すため、ゼロではない確率のみを考慮\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "# 情報利得を計算する関数\n",
        "def information_gain(data, labels, attribute_index):\n",
        "    # 親エントロピーを計算\n",
        "    original_entropy = entropy(labels)\n",
        "    # 属性を使ったデータの分割\n",
        "    attribute_values = data[:, attribute_index]\n",
        "    unique_values = np.unique(attribute_values)\n",
        "\n",
        "    # 重み付きエントロピーの計算\n",
        "    weighted_entropy = 0\n",
        "    for value in unique_values:\n",
        "        subset_indices = np.where(attribute_values == value)\n",
        "        subset_labels = labels[subset_indices]\n",
        "        weighted_entropy += len(subset_labels) / len(labels) * entropy(subset_labels)\n",
        "\n",
        "    # 情報利得を計算\n",
        "    return original_entropy - weighted_entropy\n",
        "\n",
        "# 例: データセットとラベル\n",
        "data = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n",
        "labels = np.array([1, 0, 1, 0])\n",
        "\n",
        "# 最初の属性の情報利得を計算\n",
        "info_gain = information_gain(data, labels, 0)\n",
        "print(f'情報利得: {info_gain}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77824957",
      "metadata": {
        "id": "77824957"
      },
      "source": [
        "勾配（Gradient）は、ベクトル場での最急降下の方向を示すベクトルのことです。ベクトル関数における各変数についての偏微分を集約したもので、多変数関数が最も速く増加する方向とその変化率を示します。\n",
        "\n",
        "数式としては、スカラー関数 \\( f(x, y, z) \\) の勾配は次のように表されます：\n",
        "\n",
        "\\[ \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right) \\]\n",
        "\n",
        "勾配は主に最適化問題の解法、特に勾配降下法（Gradient Descent）で活用されます。勾配降下法は、関数の最小値を見つけるために勾配の反対方向で少しずつ移動するという手法です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d2414f",
      "metadata": {
        "id": "d5d2414f"
      },
      "outputs": [],
      "source": [
        "# 勾配を計算するためのPythonのコード\n",
        "import numpy as np\n",
        "\n",
        "def function(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "# 偏微分を計算する関数\n",
        "def gradient(f, x, y, h=1e-5):\n",
        "    df_dx = (f(x + h, y) - f(x - h, y)) / (2 * h)  # xについての偏微分\n",
        "    df_dy = (f(x, y + h) - f(x, y - h)) / (2 * h)  # yについての偏微分\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# 例として x=3, y=4 での勾配を計算する\n",
        "x = 3\n",
        "y = 4\n",
        "grad = gradient(function, x, y)\n",
        "print(f\"Gradient at (x={x}, y={y}): {grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "822a0e9b",
      "metadata": {
        "id": "822a0e9b"
      },
      "source": [
        "### ヘッシアン (Hessian) について\n",
        "\n",
        "ヘッシアン行列は、多変数関数の2階偏導数をまとめた行列です。具体的には、関数 \\( f(x_1, x_2, \\ldots, x_n) \\) のヘッシアン行列 \\( H \\) は、以下のように定義されます。\n",
        "\n",
        "\\[\n",
        "H(f) = \\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "この行列はもともと最適化問題や微分幾何学において重要です。例えば、ヘッシアン行列の正定性を調べることで、ある点が極小点、極大点、または鞍点であるかどうかを判定することができます。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "- **最適化問題**: ニュートン法などの最適化アルゴリズムでは、ヘッシアン行列を用いることで収束の速度が向上します。\n",
        "- **機械学習**: 二次導数に基づくハイパーパラメータの調整や、モデルのフィッティング評価などで利用されます。\n",
        "- **物理学、工学**: ポテンシャルエネルギー面の形状解析や、安定性の評価に使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1089949",
      "metadata": {
        "id": "c1089949"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sympy import symbols, hessian, Function\n",
        "\n",
        "# 変数を定義\n",
        "x, y = symbols('x y')\n",
        "\n",
        "# 関数を定義\n",
        "f = Function('f')(x, y)\n",
        "\n",
        "# f を例えば f(x, y) = x**2 + y**2 として定義する\n",
        "f_expr = x**2 + y**2\n",
        "\n",
        "# ヘッシアン行列を計算\n",
        "H = hessian(f_expr, (x, y))\n",
        "\n",
        "# ヘッシアン行列を表示\n",
        "H_numeric = np.array(H).astype(np.float64)\n",
        "H_numeric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771196c3",
      "metadata": {
        "id": "771196c3"
      },
      "source": [
        "### Leaf-wise Growth\n",
        "\n",
        "Leaf-wise Growth は勾配ブースティング決定木 (GBDT) の木構造を成長させる際のアプローチの一つで、XGBoost をはじめとするブースティングアルゴリズムで用いられます。鳥瞰的に見ると、各イテレーションでの木の構築の際にノードのうち、最も損失関数の改善が大きいノードを選んで分割していく手法です。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "- **費用関数**\n",
        "  損失関数 \\(L\\) の改善が最大になるように\n",
        "  \n",
        "  \\(\n",
        "  \text{Gain} =\n",
        "rac{1}{2} \\left(\n",
        "rac{G_L^2}{H_L + \\lambda} +\n",
        "rac{G_R^2}{H_R + \\lambda} -\n",
        "rac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
        "ight) - \\gamma\n",
        "  \\)\n",
        "  \n",
        "  ここで、\n",
        "  - \\( G_i \\) は Node \\( i \\) の全データに対する勾配の合計。\n",
        "  - \\( H_i \\) は Node \\( i \\) の全データに対するヘッセ行列の合計、もしくは勾配の二次微分の和。\n",
        "  - \\( \\lambda \\) は L2 正則化項。\n",
        "  - \\( \\gamma \\) はリーフノードを作る際のペナルティ。\n",
        "\n",
        "その際、**Leaf-wise** では最もゲインが大きいノードを優先的に選んで分割していきます。\n",
        "\n",
        "分割が終わると、ツリーの深さがランダムになり、時には深すぎるツリーが構築されることもあるため、正則化や過学習の抑制が重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb13e273",
      "metadata": {
        "id": "eb13e273"
      },
      "outputs": [],
      "source": [
        "# Leaf-wise Growth をシミュレーションする Python コード\n",
        "import numpy as np\n",
        "\n",
        "class LeafWiseTree:\n",
        "    def __init__(self, lambda_l2=1.0, gamma=0.1):\n",
        "        self.lambda_l2 = lambda_l2\n",
        "        self.gamma = gamma\n",
        "        self.tree_structure = []\n",
        "\n",
        "    def calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        return 0.5 * ((G_L**2 / (H_L + self.lambda_l2)) + (G_R**2 / (H_R + self.lambda_l2)) - ((G_L + G_R)**2 / (H_L + H_R + self.lambda_l2))) - self.gamma\n",
        "\n",
        "    def fit(self, gradients, hessians):\n",
        "        \"\"\"\n",
        "        Simulate a leaf-wise growth step given gradients and hessians.\n",
        "        \"\"\"\n",
        "        best_gain = -np.inf\n",
        "        best_node = None\n",
        "\n",
        "        # Example simulation loop through hypothetical nodes\n",
        "        for node in hypothetical_nodes:\n",
        "            G_L, H_L, G_R, H_R = self.get_gradients_and_hessians(node, gradients, hessians)\n",
        "            gain = self.calculate_gain(G_L, H_L, G_R, H_R)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_node = node\n",
        "\n",
        "        if best_node:\n",
        "            self.split_node(best_node)\n",
        "            self.tree_structure.append(best_node)\n",
        "\n",
        "    def get_gradients_and_hessians(self, node, gradients, hessians):\n",
        "        # Dummy function to extract gradients and hessians; replace with actual logic\n",
        "        G_L, H_L = np.sum(gradients[node['left']]), np.sum(hessians[node['left']])\n",
        "        G_R, H_R = np.sum(gradients[node['right']]), np.sum(hessians[node['right']])\n",
        "        return G_L, H_L, G_R, H_R\n",
        "\n",
        "    def split_node(self, node):\n",
        "        # Dummy function to split a node; replace with actual logic\n",
        "        pass\n",
        "\n",
        "# Example usage\n",
        "hypothetical_nodes = [{'left': [0, 1], 'right': [2, 3]}, {'left': [1, 2], 'right': [0, 3]}]\n",
        "gradients = np.random.randn(4)\n",
        "hessians = np.abs(np.random.randn(4))\n",
        "\n",
        "tree = LeafWiseTree()\n",
        "tree.fit(gradients, hessians)\n",
        "print(tree.tree_structure)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f935f45c",
      "metadata": {
        "id": "f935f45c"
      },
      "source": [
        "### Histogram-based Splitting\n",
        "\n",
        "Histogram-based Splittingは、決定木や勾配ブースティングアルゴリズムなどで使用される分割方法のひとつです。この手法の背景にある考え方は、データセット全体をヒストグラムに分け、そこで得られる情報を基に最適な分割点を探索することです。この方法は、大規模なデータセットに対するスケーラビリティを改善し、メモリ使用量を低減することができます。\n",
        "\n",
        "#### 理論\n",
        "データをある特徴のbin（バケット）に分割し、そのbin内でのデータの分布に基づいて情報利得（information gain）を計算します。計算された利得が最大となるポイントを探索し、そのポイントを分割点として採用します。ヒストグラムベースの手法では、連続的な値を持つ変数を離散化することにより、計算量を抑えつつ効率的な分割を行います。\n",
        "\n",
        "#### 数式\n",
        "分割基準を選ぶ際の情報利得は次の式で表されます。\n",
        "\n",
        "\\[\n",
        "IG(T, X) = H(T) - \\sum_{v \\in Values(X)} \\frac{|T_v|}{|T|} H(T_v)\n",
        "\\]\n",
        "\n",
        "ここで、\n",
        "- \\( IG(T, X) \\) は特徴Xを使用した場合の情報利得。\n",
        "- \\( H(T) \\) は現在の不純度（エントロピーやジニ不純度など）。\n",
        "- \\( T_v \\) はXの値がvとなるサブセット。\n",
        "- \\( |T| \\) は全体のサンプル数。\n",
        "\n",
        "#### 使用用途\n",
        "- **決定木学習**: 特にカート（CART）アルゴリズムにおいて、ノードの分割を最適化する手法として用いられます。\n",
        "- **勾配ブースティングフレームワーク（例: XGBoost, LightGBM）**: 大規模データに対する高速な学習を実現するために、この技術が使用されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e93745",
      "metadata": {
        "id": "21e93745"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットを作成\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, random_state=42)\n",
        "\n",
        "# ヒストグラムを使用した分割\n",
        "# 決定木でヒストグラムベースの分割をシミュレート\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# データを可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=10)\n",
        "\n",
        "# 分割を可視化\n",
        "xlim = plt.gca().get_xlim()\n",
        "ylim = plt.gca().get_ylim()\n",
        "x = np.linspace(xlim[0], xlim[1], 100)\n",
        "y = np.linspace(ylim[0], ylim[1], 100)\n",
        "X1, X2 = np.meshgrid(x, y)\n",
        "Z = clf.predict(np.c_[X1.ravel(), X2.ravel()])\n",
        "Z = Z.reshape(X1.shape)\n",
        "\n",
        "plt.contourf(X1, X2, Z, alpha=0.3, cmap='viridis')\n",
        "plt.title('Histogram-based Splitting using Decision Tree')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad0e603",
      "metadata": {
        "id": "4ad0e603"
      },
      "source": [
        "### GOSS (Gradient-based One-Side Sampling)  \n",
        "GOSSは、Gradient Boosting Decision Tree (GBDT)の効率を改善するためのテクニックです。GBDTは決定木モデルのアンサンブルであり、小さなデータセットには効果的ですが、非常に多くのデータがある場合、そのトレーニングには時間がかかります。この問題を解決するために、GOSSは適切なサンプルを選択することでトレーニングデータ全体ではなく、部分的なデータセットでモデルを学習させるというアプローチを取ります。  \n",
        "\n",
        "GOSSは主に次の2つのステップを含みます：  \n",
        "1. 高い勾配と低い勾配を持つサンプルの中からデータを選択します。ただし、高い勾配を持つサンプルはそのまま選び、低い勾配を持つサンプルはランダムに選びます。  \n",
        "2. 勾配に基づいて選ばれたサンプルに再スケーリングを施し、元のデータセットの勾配分布を保持します。  \n",
        "\n",
        "この技術により、GOSSは計算資源を節約しながらもモデルの性能を維持しやすくなっています。特に大規模なデータセットでのGBDTの効率を高めるために使用されます。  \n",
        "\n",
        "#### 数式  \n",
        "GOSSの要点は勾配\n",
        "g\\_iを使い、特定の条件を満たすサンプルを選びます。全てのサンプルi = 1, ..., n として勾配を計算します。高い勾配を持つ上位a\\%のサンプルを全て取り、その後低い勾配を持つサンプルから、b\\%のランダムサンプルを選びます。これにより、サンプル数は元の(a + b)\\%になります。その次に、低勾配のサンプルに(1-a)\\%/\\(b\\%\\)を掛けて再スケーリングを行います：  \n",
        "\n",
        "\\[ g\\_i' =\n",
        "\\begin{cases}\n",
        "g\\_i, & \\text{if } i \\in \\text{high gradient subset}\\\\\n",
        "\\frac{1-a}{b}g\\_i, & \\text{if } i \\in \\text{low gradient subset}\n",
        "\\end{cases}\n",
        "\\]  \n",
        "\n",
        "このサンプル再スケーリングは勾配の合計をできるだけ全データに近い状態で保つためです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99c1f3b",
      "metadata": {
        "id": "d99c1f3b"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# サンプルデータを生成する\n",
        "np.random.seed(42)\n",
        "grads = np.random.rand(1000)  # サンプル数1000のランダム勾配データ\n",
        "\n",
        "# パラメータを設定する\n",
        "a_percent = 0.2  # 高勾配データの上位20%\n",
        "b_percent = 0.1  # 低勾配データからランダムに選ぶ10%\n",
        "a = int(a_percent * len(grads))\n",
        "b = int(b_percent * len(grads))\n",
        "\n",
        "# 高勾配と低勾配のインデックスを取得する\n",
        "sorted_indices = np.argsort(grads)[::-1]  # 勾配の降順でインデックスを並べる\n",
        "high_grad_indices = sorted_indices[:a]\n",
        "low_grad_indices = sorted_indices[a:]\n",
        "random_low_grad_indices = np.random.choice(low_grad_indices, b, replace=False)\n",
        "\n",
        "# 新サンプルセットを作成する\n",
        "g_selected = grads[high_grad_indices]\n",
        "g_selected = np.concatenate((g_selected, grads[random_low_grad_indices]))\n",
        "\n",
        "# 勾配のリスケーリング\n",
        "def rescale_gradients(g_indices, original_grads, a, b):\n",
        "    rescaled_grads = original_grads.copy()\n",
        "    rescaled_grads[g_indices] *= (1.0 - a_percent) / b_percent\n",
        "    return rescaled_grads\n",
        "\n",
        "rescaled_grads = rescale_gradients(random_low_grad_indices, grads, a, b)\n",
        "\n",
        "print(\"Original gradients: \", grads[:10])  # 元の勾配\n",
        "print(\"Selected gradients: \", g_selected[:10])  # 選択された勾配\n",
        "print(\"Rescaled gradients: \", rescaled_grads[:10])  # リスケーリングされた勾配\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18172d4",
      "metadata": {
        "id": "b18172d4"
      },
      "source": [
        "Exclusive Feature Bundling (EFB)は、特に大規模なデータセットで多くの特徴が存在する場合に、特徴量の冗長性を削減するための技術です。EFBは、互いに相互排他的である特徴（同時に非ゼロになることがない特徴）を1つの特徴に束ねることにより、メモリ消費を削減し、モデルの訓練速度を向上させます。これにより、高次元のデータセットに対しても効率的に学習を行うことが可能となります。EFBの背後にある基本的なアイデアは、特徴の相関関係を利用して、一度に1つのカテゴリーのみが有効となるカテゴリカル特徴を効率的に統合することです。このプロセスは通常、特徴選択や次元削減の一環として行われます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a0cb54",
      "metadata": {
        "id": "55a0cb54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# サンプルデータの作成\n",
        "X = np.array([\n",
        "    ['A', '0', '0'],\n",
        "    ['0', 'B', '0'],\n",
        "    ['0', '0', 'C'],\n",
        "    ['A', '0', '0'],\n",
        "    ['0', 'B', '0'],\n",
        "])\n",
        "\n",
        "# OneHotEncoderを用いてカテゴリカルなデータをバイナリ特徴に変換\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Exclusive Feature Bundlingの処理\n",
        "# ここでは簡単な方法として、手動で排他的な特徴をまとめます\n",
        "X_efb = np.array([X_encoded[:, 0] + X_encoded[:, 1] + X_encoded[:, 2],\n",
        "                  X_encoded[:, 3] + X_encoded[:, 4] + X_encoded[:, 5],\n",
        "                  X_encoded[:, 6] + X_encoded[:, 7] + X_encoded[:, 8]]).T\n",
        "\n",
        "# データフレームにして表示\n",
        "columns = ['FeatureBundle_1', 'FeatureBundle_2', 'FeatureBundle_3']\n",
        "df_efb = pd.DataFrame(X_efb, columns=columns)\n",
        "print(df_efb)\n",
        "\n",
        "# 出力結果が元の相互排他的な特徴量の束ねられたバージョンであることを確認\n",
        "\"\"\"\n",
        "## コードの説明:\n",
        "- カテゴリカルデータをOne-Hotエンコーディングで数値的なバイナリに変換します。\n",
        "- この例では、3つの相互排他的な特徴があり、各サンプルで1つのカテゴリーのみが有効になります。\n",
        "- EFBを適用するため、同時に1つしかアクティブにならない特徴を手動で束ねます。\n",
        "- 各新しい特徴量は、元の相互排他的なカテゴリーを束ねたものになります。\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f768fa6",
      "metadata": {
        "id": "9f768fa6"
      },
      "source": [
        "### Early Stopping\n",
        "\n",
        "Early Stoppingは、機械学習モデルをトレーニングしている間に過学習を防ぐための技術です。トレーニング中、モデルの性能はトレーニングデータに対して改善され続けることが多いですが、テストデータ（または未使用のバリデーションデータセット）に対する性能はある地点で低下し始める可能性があります。過学習が発生すると、モデルはトレーニングデータに過剰に適合してしまい、新しいデータに対しての一般化性能が低下します。\n",
        "\n",
        "Early Stoppingは、トレーニング中にバリデーションデータの性能を監視し、一定のエポック数（またはステップ数）連続して性能が改善されない場合にトレーニングを終了する方法です。こうすることで過学習を防ぎ、モデルの一般化性能を向上させることができます。\n",
        "\n",
        "### 数式\n",
        "\n",
        "Early Stoppingの基礎は以下のように形式化できます。\n",
        "\n",
        "1. バリデーション損失 $L_{val}$ を追跡します。\n",
        "2. $L_{val}$ が最小値に達したときを記録します。\n",
        "3. その後、$n$エポックの間に$L_{val}$が改善しない場合、トレーニングを停止します。\n",
        "\n",
        "このプロセスにより、最も良いバリデーション性能をもたらした重みを持つモデルが得られるようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1280a3",
      "metadata": {
        "id": "db1280a3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# データの生成\n",
        "X = np.random.rand(1000, 10)  # 特徴\n",
        "y = np.random.rand(1000, 1)  # ターゲット\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# モデルの構築\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=10, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# コンパイル\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early Stopping コールバックの設定\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2c9ec4",
      "metadata": {
        "id": "4b2c9ec4"
      },
      "source": [
        "### 学習率減衰 (Learning Rate Decay)\n",
        "\n",
        "学習率減衰は、ニューラルネットワークのトレーニングにおいて、時間の経過とともに学習率（Learning Rate）を段階的に減少させる手法です。初期には高い学習率を設定して高速なコンバージェンスを図り、その後徐々に学習率を低下させることで最適解に滑らかに収束させます。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "学習率減衰を用いることで、初期段階の高速な探索能力と、最終段階の安定した収束性を両立することが可能です。一般的な減衰方法は以下のようなものがあります:\n",
        "\n",
        "- **ステップ減衰 (Step Decay):** 特定のエポック数が経過するごとに学習率を一定比率で減少させます。\n",
        "  $$ \\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor \\frac{t}{T} \\rfloor} $$\n",
        "  ここで、$\\eta_t$は時間$t$における学習率、$\\eta_0$は初期学習率、$\\gamma$は減衰率、$T$はステップ数です。\n",
        "- **指数減衰 (Exponential Decay):** 指数関数的に学習率を減少させます。\n",
        "  $$ \\eta_t = \\eta_0 \\cdot e^{-kt} $$\n",
        "  ここで、$k$は減衰定数です。\n",
        "- **時間ベース減衰 (Time-based Decay):** 時間とともに徐々に学習率を減少させます。\n",
        "  $$ \\eta_t = \\frac{\\eta_0}{1 + kt} $$\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "学習率減衰は、特にディープラーニングにおいてトレーニングプロセスを最適化するためによく使われます。初期フェーズでの高速な収束を促しつつ、後半で精度を安定化させたい場合に有効です。これにより、オーバーフィッティングを防ぎ、モデルの汎化能力を高めることができるとされています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9a4a92",
      "metadata": {
        "id": "ff9a4a92"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習率減衰のタイプ\n",
        "class LearningRateDecay:\n",
        "    def __init__(self, initial_lr, decay_type='step', gamma=0.1, decay_step=10, k=0.1):\n",
        "        self.initial_lr = initial_lr\n",
        "        self.decay_type = decay_type\n",
        "        self.gamma = gamma\n",
        "        self.decay_step = decay_step\n",
        "        self.k = k\n",
        "\n",
        "    def get_lr(self, epoch):\n",
        "        if self.decay_type == 'step':\n",
        "            return self.initial_lr * np.power(self.gamma, np.floor(epoch / self.decay_step))\n",
        "        elif self.decay_type == 'exponential':\n",
        "            return self.initial_lr * np.exp(-self.k * epoch)\n",
        "        elif self.decay_type == 'time-based':\n",
        "            return self.initial_lr / (1 + self.k * epoch)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported decay type: {self.decay_type}\")\n",
        "\n",
        "# 学習率のプロット\n",
        "initial_lr = 0.1\n",
        "epochs = 100\n",
        "\n",
        "for decay_type in ['step', 'exponential', 'time-based']:\n",
        "    lr_decay = LearningRateDecay(initial_lr, decay_type)\n",
        "    lrs = [lr_decay.get_lr(epoch) for epoch in range(epochs)]\n",
        "    plt.plot(lrs, label=f'{decay_type} decay')\n",
        "\n",
        "plt.title('Learning Rate Decay')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2050e8fa",
      "metadata": {
        "id": "2050e8fa"
      },
      "source": [
        "### L1/L2 正則化の解説\n",
        "\n",
        "**正則化**は、過学習を防ぐためにモデルの複雑さを制御するテクニックです。L1とL2正則化はその中でも特によく使われる手法です。\n",
        "\n",
        "#### L1 正則化\n",
        "- **概念**: L1正則化は損失関数に係数の絶対値の合計をペナルティとして追加します。\n",
        "- **数式**: $\\text{L1正則化} = \\lambda \\sum_{i=1}^{n} |w_i|$\n",
        "- **使用用途**: 特徴選択に効果があり、非ゼロの特徴を重要視する。そのため、スパース性があるデータセットに向いています。\n",
        "\n",
        "#### L2 正則化\n",
        "- **概念**: L2正則化は損失関数に係数の二乗の合計をペナルティとして追加します。\n",
        "- **数式**: $\\text{L2正則化} = \\lambda \\sum_{i=1}^{n} w_i^2$\n",
        "- **使用用途**: 小さなモデルパラメータの変更でも損失が変化しにくくなるため、スムーズで安定した学習に向いています。過学習を防ぐのに有効です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980c742f",
      "metadata": {
        "id": "980c742f"
      },
      "outputs": [],
      "source": [
        "# L1/L2 正則化の例\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# データセットの生成\n",
        "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# L1正則化 (Lasso) の適用\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"Lasso 回帰係数:\", lasso.coef_)\n",
        "print(\"Lasso テストセットスコア:\", lasso.score(X_test, y_test))\n",
        "\n",
        "# L2正則化 (Ridge) の適用\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"Ridge 回帰係数:\", ridge.coef_)\n",
        "print(\"Ridge テストセットスコア:\", ridge.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf2bdd4",
      "metadata": {
        "id": "cdf2bdd4"
      },
      "source": [
        "### SHAP値 (SHapley Additive exPlanations)の解説\n",
        "SHAP値（SHapley Additive exPlanations）は、機械学習モデルの結果を解釈可能にするための手法の一つです。SHAPは各特徴量の寄与度をフェアに測定することを目指しています。これは、ゲーム理論におけるShapley値の概念に基づいており、各特徴量が予測結果にどの程度貢献したかを定量的に評価します。\n",
        "\n",
        "SHAP値は、特徴量の組み合わせのすべてのパターンを考慮し、特徴量が追加されたときにどれだけモデルの予測が変化するかを計算します。これにより、特徴量の寄与を公平に評価することが可能になります。\n",
        "\n",
        "#### SHAPの理論と数式\n",
        "Shapley値の計算は次の数式で表されます：\n",
        "\n",
        "$$\n",
        "\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} (v(S \\cup \\{i\\}) - v(S))\n",
        "$$\n",
        "\n",
        "ここで、\n",
        "- $N$ は特徴量の集合。\n",
        "- $S$ は特徴量の部分集合。\n",
        "- $v(S)$ はモデルの予測関数であり、特徴量の部分集合 $S$ に基づく予測値を返します。\n",
        "- $\\phi_i$ は特徴量iに対するSHAP値を表しています。\n",
        "\n",
        "#### 使用用途\n",
        "主に以下のような場合に用いられます：\n",
        "- モデル予測の説明性向上\n",
        "- 特徴量の重要性の評価\n",
        "- モデルのバイアス検出\n",
        "\n",
        "SHAP値は、勾配ブースティングモデル（例：XGBoost, LightGBM）やニューラルネットワークといった様々なモデルに対して適用可能で、特にブラックボックスモデルの解釈に有用です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f4606f",
      "metadata": {
        "id": "83f4606f"
      },
      "outputs": [],
      "source": [
        "# PythonでSHAP値を計算し、視覚的に解釈するためのコード\n",
        "```\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# データのロードと訓練/テストへの分割\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoostモデルの訓練\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# SHAP値の計算\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# 単一の予測結果に対するSHAP値のプロット\n",
        "shap.initjs()\n",
        "shap.plots.waterfall(shap_values[0])\n",
        "\n",
        "# 全体の特徴量の寄与を示すSummary Plot\n",
        "shap.plots.beeswarm(shap_values)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5548b5c1",
      "metadata": {
        "id": "5548b5c1"
      },
      "source": [
        "Permutation Importance（順列重要度）とは、特定の特徴量がモデルの予測にどの程度寄与しているかを測るための手法の一つです。この手法は、各特徴量の値をランダムにシャッフルし、モデルの予測性能がどれだけ低下するかを観察することで、その特徴量の重要性を評価します。重要な特徴量であれば、その値をランダムにすることでモデルの予測性能が大きく低下します。具体的な手順は以下の通りです：  \n",
        "1. 初期の性能メトリクス（例：精度やR²スコア）を計算する。  \n",
        "2. 各特徴量に対して、その特徴量の値をランダムにシャッフル。  \n",
        "3. シャッフル後のデータで再度モデルの性能を評価。  \n",
        "4. 初期の性能との差を特徴量の重要度とする。この手法の利点は、モデルがどのアルゴリズムであっても適用可能であり、直接的に予測精度への影響を見ることができる点です。ただし、計算コストが高くなる可能性があることや、多重共線性のある特徴量間で結果が不安定になることもあります。数式として表すと、特徴量 \\( j \\) についてのPermutation Importanceは以下のように表されます：  \n",
        "\\[\n",
        "PI(j) = E(L(f(X), Y)) - E(L(f(X_{\\setminus j}^{\\text{perm}}), Y))\n",
        "\\]  \n",
        "ここで、\\( PI(j) \\) は特徴量 \\( j \\) の重要度、\\( L \\) は損失関数、\\( f \\) は学習済みモデル、\\( X \\) は入力データ、\\( Y \\) はターゲット、\\( X_{\\setminus j}^{\\text{perm}} \\) は特徴量 \\( j \\) がシャッフルされたデータです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c61e1d2",
      "metadata": {
        "id": "0c61e1d2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "import random\n",
        "\n",
        "# データのロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# モデルの訓練\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 元のデータでの精度を計算\n",
        "baseline_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "# Permutation Importanceを計算\n",
        "def permutation_importance(model, X_test, y_test):\n",
        "    baseline_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "    importances = {}\n",
        "    for col in range(X_test.shape[1]):\n",
        "        X_permuted = X_test.copy()\n",
        "        random.shuffle(X_permuted[:, col])  # 特徴量をシャッフル\n",
        "        shuffled_accuracy = accuracy_score(y_test, model.predict(X_permuted))\n",
        "        importance = baseline_accuracy - shuffled_accuracy\n",
        "        importances[col] = importance\n",
        "    return importances\n",
        "\n",
        "importances = permutation_importance(model, X_test, y_test)\n",
        "\n",
        "# 結果を表示\n",
        "for feature_idx, importance in importances.items():\n",
        "    print(f\"Feature {feature_idx}: {importance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7304a4",
      "metadata": {
        "id": "5f7304a4"
      },
      "source": [
        "### 決定木の構築について\n",
        "\n",
        "決定木は、データを用いて物事を決定するためのツールであり、木構造に基づいて意思決定を行うアルゴリズムです。決定木は、分類問題や回帰問題に利用されます。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "決定木はデータを特徴量に基づいて分割する方法です。それは、根ノードから始まり、各特徴量に基づいてノードが分割され、各ノードでの判断が行われます。これらのノードには最後に、すべての分割が終了した際の出力（クラスまたは値）が含まれており、これを葉ノードと呼びます。\n",
        "\n",
        "決定木の構築は以下のステップを含みます。\n",
        "\n",
        "1. **特徴選択**: 分割の基準として最適な特徴を選択します。一般的な基準にはジニ不純度、エントロピーを用いた情報利得などがあります。\n",
        "\n",
        "2. **分割の実行**: 最適な基準に基づいてデータを分割します。\n",
        "\n",
        "3. **停止条件の確認**: 分割を継続するか、停止して葉ノードとするかを決定します。条件には、データが十分に純粋である、指定した最大深さに達したなどがあります。\n",
        "\n",
        "4. **終端処理**: 新しいデータポイントを分類するために、完成した決定木を使用します。\n",
        "\n",
        "#### 数式\n",
        "\n",
        "1. **ジニ不純度** $G_i = 1 - \\sum_{k=1}^K p_{i,k}^2$\n",
        "2. **情報利得** $IG(T, X) = H(T) - \\sum_{i=1}^n \\frac{|T_i|}{|T|} H(T_i)$\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "- **分類**: スパムメールの判定、がんの診断\n",
        "- **回帰**: 家の価格の予測、売上の予測\n",
        "- **特徴選択**や**ルール生成**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc925c5d",
      "metadata": {
        "id": "fc925c5d"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットのロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# データをトレーニングとテストに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 決定木のインスタンス化\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "\n",
        "# モデルのトレーニング\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# モデルの可視化\n",
        "plt.figure(figsize=(12,8))\n",
        "tree.plot_tree(clf, filled=True)\n",
        "plt.show()\n",
        "\n",
        "# テストデータに対する予測\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# モデルの精度\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675b926f",
      "metadata": {
        "id": "675b926f"
      },
      "source": [
        "### ブートストラップサンプリングの解説\n",
        "\n",
        "ブートストラップサンプリングは、与えられたデータセットから再サンプリングを行う手法で、統計推定の精度を評価するために広く使用されます。再サンプリングは置換抽出により行われ、元のデータセットと同じサイズの新しいデータセットを生成します。これにより、仮想的に独立したサンプルを多数作成し、統計量の分布や信頼区間を推定することができます。\n",
        "\n",
        "#### 理論\n",
        "ブートストラップ法では、以下の手順を実行します：\n",
        "1. 元のデータセット $X = \\{x_1, x_2, ..., x_n\\}$ から置換ありでサンプルを取り、ブートストラップサンプル $X^*_b$ を生成します（ここで $b = 1, 2, ..., B$）。\n",
        "2. 各ブートストラップサンプル $X^*_b$ に対して、関心のある統計量（例えば平均、分散、回帰係数など）を計算します。\n",
        "3. ブートストラップサンプルで得られた統計量の分布を利用して、元の統計量の分布を推定します。\n",
        "\n",
        "#### 使用用途\n",
        "- **信頼区間の推定**: 統計量の真の値を囲む確率的な範囲を計算するのに使用されます。\n",
        "- **統計的仮説検定**: データの特徴に基づく仮説の検証に効果的です。\n",
        "- **モデルの性能評価**: 機械学習モデルの汎化性能を評価する際に用いられます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1784e9",
      "metadata": {
        "id": "8c1784e9"
      },
      "outputs": [],
      "source": [
        "# Pythonによるブートストラップサンプリングの実装例\n",
        "import numpy as np\n",
        "\n",
        "# 元のデータセット\n",
        "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# ブートストラップサンプリングの設定\n",
        "n_iterations = 1000  # ブートストラップサンプルの数\n",
        "n_size = len(original_data)  # ブートストラップサンプルのサイズ\n",
        "\n",
        "# 統計量を保存するリスト\n",
        "means = []\n",
        "\n",
        "# ブートストラップサンプリングの実行\n",
        "for _ in range(n_iterations):\n",
        "    # ランダムにサンプリング\n",
        "    sample = np.random.choice(original_data, size=n_size, replace=True)\n",
        "    # サンプルの平均を計算して保存\n",
        "    means.append(np.mean(sample))\n",
        "\n",
        "# ブートストラップサンプルから得た平均の標準偏差を計算\n",
        "std_error = np.std(means)\n",
        "print(f\"推定平均の標準誤差: {std_error}\")\n",
        "\n",
        "# 結果を使って95%信頼区間を計算\n",
        "confidence_interval = np.percentile(means, [2.5, 97.5])\n",
        "print(f\"95%信頼区間: {confidence_interval}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05ce417d",
      "metadata": {
        "id": "05ce417d"
      },
      "source": [
        "### ランダムフォレストの解説\n",
        "\n",
        "**ランダムフォレスト**は、主に分類や回帰の問題を解決するために使用されるアンサンブル学習アルゴリズムの一つです。以下にその特徴や理論を説明します。\n",
        "\n",
        "#### 理論\n",
        "- **決定木の集まり**: ランダムフォレストは大量の決定木を使って予測を行います。これにより、一つの決定木では得られない精度や汎化性能を高めることができます。\n",
        "- **バギング手法**: ランダムフォレストはバギング (Bootstrap Aggregating) と呼ばれる手法を使用しています。これは、データの中からランダムにサンプルを取得して複数のモデルを学習させ、その結果を平均化することで全体の予測精度を向上させる方法です。\n",
        "- **特徴のランダム性**: 各決定木を作る際に、無作為に特徴量を選択して学習します。これによりモデルの多様性が増し、過学習を防ぐ効果が得られます。\n",
        "\n",
        "#### 数式\n",
        "ランダムフォレストは、個々の決定木 \\(h(x, \\Theta_k)\\) の出力を集め、それらの平均を取ることで最終的な出力を決定します。ここで、\\(x\\) は入力データ、\\(\\Theta_k\\) はランダムに生成されたパラメータです。\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{K} \\sum_{k=1}^{K} h(x, \\Theta_k)\n",
        "\\]\n",
        "\n",
        "#### 使用用途\n",
        "- **分類問題**: スパムメールのフィルタリングや画像認識など、カテゴリ分けする問題に応用。\n",
        "- **回帰問題**: 家の価格予測や在庫予測など、連続値の予測問題に応用。\n",
        "- **特徴選択**: 特徴の重要度を評価する機能を持ち、多くの特徴量の中から重要な特徴を選択する際に利用可能。\n",
        "\n",
        "ランダムフォレストは簡単に使えることに加え、様々な問題で強力な性能を示すため、機械学習の初心者から上級者まで広く利用されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f32bf1b",
      "metadata": {
        "id": "5f32bf1b"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# ランダムフォレストの実装例\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データのロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルの作成\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# モデルの訓練\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 予測\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 精度の評価\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```\n",
        "\n",
        "このコードは、scikit-learnライブラリを用いてランダムフォレストを実装する簡単な例です。`load_iris`関数でデータセットを取得し、データを訓練用とテスト用に分割します。`RandomForestClassifier`を使用してモデルを訓練し、テストデータで予測を行った後、精度を出力します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e439ee21",
      "metadata": {
        "id": "e439ee21"
      },
      "source": [
        "### アンサンブル学習について\n",
        "\n",
        "アンサンブル学習とは、複数のモデルを組み合わせて予測性能を向上させる手法です。個別のモデルが持つ弱点を補い、より強力な予測力を持つモデルを生成することが目的です。アンサンブル学習は、モデルの多様性を利用することで、過学習を抑制し、汎化性能を向上させることが期待されます。\n",
        "\n",
        "#### 主な手法\n",
        "1. **バギング (Bagging)**: ブートストラップサンプリングによって複数のデータセットを生成し、それぞれにモデルを学習させ、最終的に予測する際にモデルの平均をとる手法です。ランダムフォレストが代表的な例です。\n",
        "\n",
        "2. **ブースティング (Boosting)**: 弱学習器を順番に学習させ、誤分類されたデータにより重みを置くことで、次の学習器を訓練します。最終的な結果は重み付き多数決や加重平均によって出されます。AdaBoostやGradient Boostingが有名です。\n",
        "\n",
        "3. **スタッキング (Stacking)**: 複数の異なるモデルの出力をメタ学習器に入力し、最終予測を生成する手法です。メタ学習器はこれらの出力から最適な予測を導き出します。\n",
        "\n",
        "#### 使用用途\n",
        "アンサンブル学習は広範な分野で使用され、多くの機械学習コンペティションでもその有効性が示されています。特に大量のデータがある場合や、単純なモデルを超える予測性能が求められる場合に効果的です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b4a2758",
      "metadata": {
        "id": "7b4a2758"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# アンサンブル学習の例: ランダムフォレストによる分類\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データセットをロードし、トレーニングセットとテストセットに分割\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルのインスタンスを作成\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# モデルをトレーニング\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# テストセットで予測\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 予測精度を表示\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b61ea33",
      "metadata": {
        "id": "7b61ea33"
      },
      "source": [
        "## 交差検証 (Cross Validation)\n",
        "\n",
        "交差検証は、モデルの性能を評価するためのテクニックです。データをいくつかの分割に分け、その分割を使ってモデルを訓練・テストします。最も一般的な方法は K-分割交差検証 (K-Fold Cross Validation) です。\n",
        "\n",
        "### K-分割交差検証のプロセス\n",
        "\n",
        "1. データセットを K 個の等しい部分に分割します。\n",
        "2. そのうち K-1 個を訓練に使用し、残りの 1 個をテストに使います。\n",
        "3. このプロセスを K 回繰り返し、すべての部分がテストデータとして一度使われるようにします。\n",
        "4. 各反復で得られた評価指標（例：精度）を平均して、モデルの性能の推定値を得ます。\n",
        "\n",
        "K-分割交差検証の利点は、データのバリエーションにかかわらず、より安定したモデル評価を提供することです。複数の分割を用いるため、特定の分割に依存する過学習を避けることができます。\n",
        "\n",
        "### 数式\n",
        "\n",
        "各分割における評価指標 (例えば、Accuracy) を $A_k$ とすると、最終的な評価指標は以下のように計算されます：\n",
        "\n",
        "$$\\text{Accuracy}_{CV} = \\frac{1}{K} \\sum_{k=1}^{K} A_k$$\n",
        "\n",
        "ここで、$K$ は分割の数を示します。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "- **モデル選択**: モデルやハイパーパラメータの選択を行う際に利用されます。\n",
        "- **パフォーマンス評価**: データの分割に依存しない性能評価を行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5a601d",
      "metadata": {
        "id": "4b5a601d"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# データセットのロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# モデルの定義\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# K-分割交差検証 (K=5) を実行\n",
        "scores = cross_val_score(clf, X, y, cv=5)\n",
        "\n",
        "# 各分割のスコアと平均スコアを表示\n",
        "print('Cross-validation scores: {}'.format(scores))\n",
        "print('Mean accuracy: {:.2f}'.format(scores.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ec09f9",
      "metadata": {
        "id": "c8ec09f9"
      },
      "source": [
        "### ハイパーパラメータチューニング (GridSearch)\n",
        "\n",
        "ハイパーパラメータチューニングは、モデルの性能を最適化するために使用する一連のプロセスです。ハイパーパラメータは、モデルのトレーニングプロセスにおいて事前に指定され、それによってモデルの挙動を制御する値です。これに対して、モデルによって学習されるパラメータはデータに基づいて決まるものです。\n",
        "\n",
        "**GridSearch**とは、あらかじめ定義されたハイパーパラメータの組み合わせを網羅的に探索して、モデル性能を最大化するためのプロセスです。GridSearchでは、複数のハイパーパラメータに対して考えられるすべての組み合わせを試し、それぞれに対してモデルを訓練し、最適な組み合わせを見つけ出します。\n",
        "\n",
        "#### 数式\n",
        "ハイパーパラメータ空間 $\\mathcal{H}$の探索において、GridSearchは次のように定義されます：\n",
        "\n",
        "$$\\mathcal{H} = \\{ (h_1^{(i)}, h_2^{(j)}, \\ldots, h_d^{(k)}) \\ |\\  i=1,\\ldots,I; j=1,\\ldots,J; \\ldots; k=1,\\ldots,K \\}$$\n",
        "\n",
        "ここで、$h_1^{(i)}, h_2^{(j)}, \\ldots, h_d^{(k)}$はそれぞれのハイパーパラメータの可能な値を示しています。\n",
        "\n",
        "#### 使用用途\n",
        "GridSearchは、機械学習モデルのハイパーパラメータチューニングに用いられ、特に学術研究や実務で精度を改善するために広く利用されている手法です。計算コストが高い場合もありますが、全ての組み合わせを試すため、適切なハイパーパラメータを見つける可能性を十分に確保することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42075660",
      "metadata": {
        "id": "42075660"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# データセットの読み込み\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# モデルとパラメータの定義\n",
        "model = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "# GridSearchの実行\n",
        "grid = GridSearchCV(model, param_grid, refit=True, verbose=2, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "# 最適なパラメータの表示\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# 最適なモデルのスコアの表示\n",
        "print(\"Best Cross-validation Score:\", grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3c8837",
      "metadata": {
        "id": "8e3c8837"
      },
      "source": [
        "### バギング (Bagging)についての解説\n",
        "\n",
        "**バギング (Bootstrap Aggregating)** は、機械学習において慎重に扱われるアンサンブル学習の一手法です。バギングは、元のデータセットから重複を許して複数のサブセットをリサンプルし、それぞれにモデルを訓練します。最終的な予測は、これらのモデルからの出力を集約することで行います。\n",
        "\n",
        "#### 理論:\n",
        "- **ブートストラップサンプリング**: 元のデータセットからサンプリングし、サイズの等しい新しいデータセットを作成します。サンプルは重複を許します。\n",
        "- **平均化/多数決**: 回帰問題であれば予測値の平均を取り、分類問題であればモード（最頻値）を取ることで予測します。\n",
        "\n",
        "バギングの主な利点は、モデルの分散を減少させることにあります。個々のモデルの誤差が互いに打ち消し合うことによって、全体としての予測精度が向上します。これは特に、決定木のように高い分散を持つモデルにおいて効果的です。\n",
        "\n",
        "数式での表現は以下のようになります：\n",
        "\n",
        "1. データセット $D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$ からブートストラップサンプル $D^{(b)}$ を $B$ 個作成。\n",
        "2. 各ブートストラップサンプルごとにモデル $f^{(b)}(x)$ を学習。\n",
        "3. 予測はアンサンブル平均で計算されます。\n",
        "\n",
        "\\[\n",
        "\\hat{f}(x) = \\frac{1}{B} \\sum_{b=1}^{B} f^{(b)}(x)\n",
        "\\]\n",
        "\n",
        "ここで、$\\hat{f}(x)$ はアンサンブルモデルの出力です。\n",
        "\n",
        "#### 使用用途:\n",
        "- 決定木に基づくランダムフォレストは、バギングの代表例であり、競争力のある精度を特徴とします。\n",
        "- ノイズに敏感なモデルにおける、精度の向上が期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e064ab",
      "metadata": {
        "id": "40e064ab"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データのロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 決定木ベースのバギング分類器を作成\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=10, random_state=42)\n",
        "\n",
        "# モデルの訓練\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# 予測\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# 精度の評価\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c74d665",
      "metadata": {
        "id": "8c74d665"
      },
      "source": [
        "### ブースティング (Boosting) についての解説\n",
        "\n",
        "ブースティング（Boosting）は、機械学習のアンサンブル学習手法の一つで、複数の弱学習器（通常、決定木）が連携して性能の高い予測を行う方法です。ブースティングは、モデルの過学習を防ぎつつ、予測精度を向上させることができます。\n",
        "\n",
        "ブースティングの基本的なアイデアは、順次的な学習プロセスを用いて、前のモデルで間違えたデータポイントにより重点を置くことです。これは通常、アルゴリズムが満たされるたびに、新しいモデルが作られて誤分類されたデータに焦点を当てるような方法で行われます。最も一般的なブースティング手法としてAdaBoostやGradient Boosting、XGBoostなどがあります。\n",
        "\n",
        "#### 数式\n",
        "ブースティングでは、基本的に以下のような形式の予測モデルを構築します：\n",
        "\n",
        "\\[ F(x) = \\sum_{m=1}^{M} w_m h_m(x) \\]\n",
        "\n",
        "- \\( F(x) \\) は最終的な予測結果。\n",
        "- \\( h_m(x) \\) はそれぞれの弱学習器。\n",
        "- \\( w_m \\) は各弱学習器の重み。\n",
        "\n",
        "### 使用用途\n",
        "- **分類問題**：画像認識、スパムフィルター、顧客離反予測\n",
        "- **回帰問題**：家賃価格予測、売上予測\n",
        "\n",
        "ブースティングが適しているのは、精度が非常に重要な場合や、データセットが複雑で異なる特徴を持っている場合です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2aa642",
      "metadata": {
        "id": "ee2aa642"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# 必要なライブラリのインポート\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# データセットの作成\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
        "\n",
        "# データセットの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# モデルの初期化\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 予測\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 評価\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "```\n",
        "このコードでは、`sklearn`を用いて著名なブースティング技法であるGradient Boostingを実装しています。まず、`make_classification`でダミーデータセットを生成し、それをトレーニングデータとテストデータに分割します。その後、Gradient Boostingのモデルを訓練し、テストデータに対しての予測結果を評価しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c242c75d",
      "metadata": {
        "id": "c242c75d"
      },
      "source": [
        "### Log Loss (対数損失)\n",
        "Log Lossは分類問題においてモデルの性能を評価するための損失関数です。特に二項分類問題によく使われます。\n",
        "\n",
        "#### 理論\n",
        "Log Lossは、確率を用いて予測の不確実性を測定します。具体的には、あるクラスに属する確率が高いほどLog Lossが低くなり、逆に確率が低い場合はLog Lossが高くなります。\n",
        "\n",
        "数式で表すと、Log Lossは以下のようになります：\n",
        "\n",
        "- \\( y_i \\) は実際のラベル (1 もしくは 0)、\n",
        "- \\( \\hat{y_i} \\) はモデルが予測したラベルが1になる確率。\n",
        "\n",
        "\\[ \text{Log Loss} = -\n",
        "rac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})\n",
        "ight] \\]\n",
        "\n",
        "ここで、\\( N \\) はサンプルの数です。この式は、予測が正しいほど損失が小さく、予測が間違っているほど損失が大きくなるように設計されています。\n",
        "\n",
        "#### 使用用途\n",
        "- **評価指標**として：モデルの良し悪しを定量的に評価。\n",
        "- **モデル最適化**：モデルのパラメータ調整時に、Log Lossを最小化することで、モデル精度を向上。\n",
        "\n",
        "Log Lossは、特にクラス不均衡があるデータセットで役立つ指標です。なぜなら、正しいクラスの確率を考慮するため、単に正解率を考えるよりも、モデリングの精度をより詳細に評価できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8434e98d",
      "metadata": {
        "id": "8434e98d"
      },
      "outputs": [],
      "source": [
        "# Python code to calculate Log Loss\n",
        "import numpy as np\n",
        "\n",
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the log loss for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (numpy array): True binary labels (0 or 1).\n",
        "    y_pred (numpy array): Predicted probabilities of the positive class.\n",
        "\n",
        "    Returns:\n",
        "    float: Log loss value.\n",
        "    \"\"\"\n",
        "    # Add epsilon to prevent log(0)\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Calculate log loss\n",
        "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# Example usage:\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.6])\n",
        "print(\"Log Loss:\", log_loss(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7517489a",
      "metadata": {
        "id": "7517489a"
      },
      "source": [
        "### データの重み付けの理論\n",
        "\n",
        "データの重み付けとは、データの各サンプルや特徴に重要度を示すための値（重み）を割り当てる手法です。この手法は、より重要なデータに対して優先順位を与え、分析結果の精度を高める目的で使用されます。重み付けは、統計学、機械学習、データ分析などで広く利用されます。\n",
        "\n",
        "### 数式\n",
        "\n",
        "一般的な重み付けの数式は次のようになります:\n",
        "\n",
        "\\[\n",
        "Y = \\sum_{i=1}^{n} w_i \\cdot x_i\n",
        "\\]\n",
        "\n",
        "ここで、\n",
        "- \\(Y\\) は重みを考慮した出力。\n",
        "- \\(w_i\\) はサンプルや特徴に対する重み。\n",
        "- \\(x_i\\) はデータポイント。\n",
        "- \\(n\\) はデータポイントの数。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "1. **機械学習モデル**: 重み付けは、損失関数内で誤差の種類に応じて異なる重みを割り当てることで、モデルの精度向上に貢献します。\n",
        "2. **アンケート調査**: 無作為抽出ではないデータセットで、参加者の重要度に応じて結果を調整するために使用されます。\n",
        "3. **ポートフォリオ管理**: 個々の資産に対し異なる投資比率を割り当て、リスクを管理します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deff7427",
      "metadata": {
        "id": "deff7427"
      },
      "outputs": [],
      "source": [
        "# データの重み付けを示すシンプルなPythonコード\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def weighted_sum(weights, data):\n",
        "    return np.dot(weights, data)\n",
        "\n",
        "# サンプルデータ\n",
        "weights = np.array([0.2, 0.3, 0.5])\n",
        "data = np.array([10, 20, 30])\n",
        "\n",
        "# 重み付き合計を計算\n",
        "result = weighted_sum(weights, data)\n",
        "print(f'Weighted Sum: {result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1fb56a",
      "metadata": {
        "id": "9d1fb56a"
      },
      "source": [
        "### ランダムサンプリング\n",
        "\n",
        "ランダムサンプリングは、母集団から無作為にサンプルを抽出する手法です。この手法の目的は、母集団全体を代表するようなサンプルを得ることです。ランダムサンプリングは、例えばアンケート調査や品質管理など、各種統計分析の基礎となる手法です。理論的には、各要素がサンプルに選ばれる確率が等しいことから、『公平性』が担保されます。\n",
        "\n",
        "#### 理論と数式\n",
        "\n",
        "統計的には、サンプルの平均 \\( \\bar{x} \\) は母平均 \\( \\mu \\) の推定値として使用され、サンプル標準偏差 \\( s \\) は母標準偏差 \\( \\sigma \\) の推定値として役立ちます。\n",
        "\n",
        "\\[\n",
        "    \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "    s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
        "\\]\n",
        "\n",
        "ここで、\\( x_i \\) はサンプルの要素、\\( n \\) はサンプルサイズです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cafbd80",
      "metadata": {
        "id": "4cafbd80"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# 母集団を定義\n",
        "population = [i for i in range(1, 101)]  # 1から100までの整数\n",
        "\n",
        "# サンプルサイズ\n",
        "sample_size = 10\n",
        "\n",
        "# ランダムサンプリング\n",
        "sample = random.sample(population, sample_size)\n",
        "\n",
        "# 結果の表示\n",
        "print(\"ランダムサンプル:\", sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ddfaf1",
      "metadata": {
        "id": "f7ddfaf1"
      },
      "source": [
        "SHAP (SHapley Additive exPlanations) 値は、機械学習モデルの予測を説明するためのツールで、ゲーム理論に基づく手法です。SHAP値は各特徴量の寄与度を公正に割り当てることを目的としており、予測値への影響を定量化します。理論的には、SHAP値は特徴量の組み合わせ全体を考慮し、各特徴量が入ることで予測値がどれだけ変化するかを全特徴量で平均化して評価します。数式的には、\\( v(S) \\)を特徴量群\\( S \\)についての予測値とし、特徴量\\( i \\)の寄与を計算するためのSHAP値\\( \\phi_i \\)は以下の式で表されます。\n",
        "\n",
        "$$ \\phi_i = \\sum_{S \\subseteq N \\ \\{ i \\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} (v(S \\cup \\{ i \\}) - v(S)) $$\n",
        "\n",
        "ここで、\\( N \\)はすべての特徴量の集合、\\( |S| \\)は特徴量群\\( S \\)の要素数です。SHAP値の利用用途には、モデルの透明性を高めるための可視化、特定の予測における主要なドライバーの特定、特徴量の重要度評価などがあります。SHAP値は累積的な影響を観察できるため、個々の予測に対する特徴量の効果を深く理解できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a85025",
      "metadata": {
        "id": "61a85025"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# データを生成または読み込み\n",
        "X, y = shap.datasets.boston()\n",
        "\n",
        "# データを学習用とテスト用に分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# モデルをトレーニング\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# SHAP explainerの作成\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "\n",
        "# SHAP値を計算\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# 予測に対するSHAP値の可視化\n",
        "shap.summary_plot(shap_values, X_test, plot_type='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea13d319",
      "metadata": {
        "id": "ea13d319"
      },
      "source": [
        "### ハイパーパラメータの最適化\n",
        "\n",
        "ハイパーパラメータの最適化とは、機械学習モデルのパフォーマンスを最大化するために、モデルのハイパーパラメータを調整するプロセスです。ハイパーパラメータとは、モデルのトレーニングプロセス前に設定する必要のあるパラメータであり、学習率やバッチサイズ、正則化パラメータなどが含まれます。\n",
        "\n",
        "#### 理論と数式\n",
        "\n",
        "ハイパーパラメータの最適化は、通常次のような問題として定式化されます：\n",
        "\n",
        "\\[\n",
        "\\text{minimize}_{\\theta \\in \\Theta} \\ L(\\theta, \\mathcal{D}_{\\text{train}})\n",
        "\\]\n",
        "\n",
        "ここで、\\(\\theta\\) はハイパーパラメータのセット、\\(\\Theta\\) は許容されるハイパーパラメータの空間、\\(L\\) は損失関数、\\(\\mathcal{D}_{\\text{train}}\\) はトレーニングデータです。最適化は損失 \\(L\\) を最小化するように \\(\\theta\\) を選ぶことです。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "ハイパーパラメータの最適化は、特にディープラーニングや複雑な機械学習モデルで重要です。適切に設定されたハイパーパラメータはモデルの精度を大幅に向上させることができます。\n",
        "\n",
        "最も一般的なハイパーパラメータ最適化手法には、グリッドサーチ、ランダムサーチ、ベイズ最適化などがあります。ベイズ最適化は特に有効で、ハイパーパラメータの空間を探索する際により少ない試行で良好な結果を得ることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea03259",
      "metadata": {
        "id": "9ea03259"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# データセットをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# トレーニングデータとテストデータに分ける\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVMモデルを定義\n",
        "model = SVC()\n",
        "\n",
        "# ハイパーパラメータの選択肢を定義\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# グリッドサーチによるハイパーパラメータの最適化\n",
        "grid = GridSearchCV(model, param_grid, refit=True, verbose=2)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 最良のパラメータと精度を出力\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", grid.best_score_)\n",
        "\n",
        "# テストデータでの性能を評価\n",
        "score = grid.score(X_test, y_test)\n",
        "print(\"Test Score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f7a50a8",
      "metadata": {
        "id": "0f7a50a8"
      },
      "source": [
        "### 予測確率のキャリブレーション\n",
        "\n",
        "予測確率のキャリブレーションとは、モデルが予測する確率の出力が現実の確率とどの程度一致しているかを評価し、調整するプロセスです。モデルが生成する予測確率がバイアスを持つ場合や、確率の分布が実際の分布と一致しない場合、キャリブレーションが必要となります。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "キャリブレーションを評価するための方法の一つに**キャリブレーションプロット**があり、これはモデルの予測確率をビンに分け、それぞれのビンにおける予測確率と実際の事象の発生頻度を比較する方法です。また、Brierスコアもキャリブレーションを評価するための指標として用いられます。\n",
        "\n",
        "キャリブレーションの理論的基盤は、特にロジスティック回帰やベイズ学習などの確率出力を持つモデルに関連します。これらのモデルでは、予測確率が単に確率密度の出力として利用されず、そのまま意思決定やリスク評価、ベイジアンネットワークの生成など、様々な応用に利用されます。\n",
        "\n",
        "#### 数式\n",
        "\n",
        "予測確率のキャリブレーションを数式で表すと、次のように定義できます。\n",
        "\n",
        "- **キャリブレーション関数（Calibration Function, CF）**:\n",
        "  \\[ CF(p) = P(Y=1 \\mid \\hat{P}=p) \\]\n",
        "  ここで、\\(\\hat{P}\\)はモデルの予測確率を表し、\\(P(Y=1 \\mid \\hat{P}=p)\\)は実際の事象が発生する確率を表します。\n",
        "\n",
        "- **Brierスコア**:\n",
        "  \\[ \\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (f_i - o_i)^2 \\]\n",
        "  ここで、\\(f_i\\)は予測確率、\\(o_i\\)は実際のイベント（0または1）です。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "1. **分類問題の評価**: 分類モデルがどの程度信頼できるかを評価する際に使用されます。\n",
        "2. **確率予測の信頼性**: AIが出力する確率を意思決定に利用する場合、その信頼性を確認するために重要です。\n",
        "3. **リスク管理**: 金融や医療分野におけるリスク評価において、モデルの予測確率を正確にキャリブレーションすることで、より正確なリスク評価が可能となります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e48a6a",
      "metadata": {
        "id": "70e48a6a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットの作成\n",
        "data = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                          random_state=42)\n",
        "X, y = data\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ロジスティック回帰モデルの訓練\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで予測確率の計算\n",
        "prob_pos = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# キャリブレーション曲線の取得\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n",
        "\n",
        "# キャリブレーションプロットの描画\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\")\n",
        "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
        "plt.xlabel(\"Mean predicted value\")\n",
        "plt.ylabel(\"Fraction of positives\")\n",
        "plt.title(\"Calibration curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8118f02d",
      "metadata": {
        "id": "8118f02d"
      },
      "source": [
        "### 分布適応型のバギング (Balanced Bagging) に関する解説\n",
        "\n",
        "バランスバギング（Balanced Bagging）は、特に不均衡なデータセットにおいてバギングの手法を改良して活用するための技法です。一般的なバギングは、複数のサブサンプルを用いてその平均で予測を行う手法で、モデルのバリアンスを減少させる目的で使われます。しかし、クラスの不均衡があるデータセットでは、サンプルの不均衡がそのまま影響するため、クラス不均衡を考慮したサンプリングが必要です。\n",
        "\n",
        "Balanced Baggingはこの問題を解決するため、各サブサンプル内でクラスの分布が均等になるようにサンプリングを行います。そのため、マイノリティクラス（少数派のクラス）がバギングされたサンプリングにおいて適切な代表を持つことができるようになります。具体的には、各クラスからランダムにサンプルを抽出し、クラス数が均等になるようにリサンプリングを行います。\n",
        "\n",
        "数式的には、従来のバギングがブートストラップサンプリングによって各サブセット $B_i$ を生成するのに対し、Balanced Baggingでは各クラス $c_j$ のサンプル数を均一化するリサンプリング手法を採用します。特に、以下のような式が用いられます：\n",
        "\n",
        "- $N_c$: 各クラスのサンプル数\n",
        "- $B_i^{(c_j)}$: クラス $c_j$ に属するサブセット $B_i$ のサンプル\n",
        "\n",
        "$B_i^{(c_j)} = \\text{resample}(D^{(c_j)}, n=N_{balanced})$\n",
        "\n",
        "ここで、$N_{balanced}$ は各クラスから選出される目標サンプル数を示します。このようにして生成されたサブサンプルを用いて、それぞれの個別のモデルをトレーニングし、最終的なアンサンブルを作ります。\n",
        "\n",
        "Balanced Baggingは、クラス間バランスが重要な影響を与える不均衡なデータセットで使用され、特にレアケースの重要性が高い医療診断などの領域で有用です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124fe254",
      "metadata": {
        "id": "124fe254"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# データセットの作成\n",
        "X, y = make_classification(n_classes=3, class_sep=2, weights=[0.1, 0.3, 0.6], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "# トレーニングデータセットとテストデータセットに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Balanced Baggingのモデルを設定\n",
        "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                sampling_strategy='auto',\n",
        "                                replacement=False,\n",
        "                                random_state=0)\n",
        "\n",
        "# モデルのトレーニング\n",
        "bbc.fit(X_train, y_train)\n",
        "\n",
        "# 予測とパフォーマンスの評価\n",
        "y_pred = bbc.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "このコードでは、`imblearn` パッケージの `BalancedBaggingClassifier` を使用しており、クラス間の不均衡を考慮しながらブートストラップサンプルを生成しています。この方法では、各クラスが平等に代表されるようにしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d970bd7",
      "metadata": {
        "id": "4d970bd7"
      },
      "source": [
        "### スパースデータの処理\n",
        "\n",
        "スパースデータとは、多くの要素がゼロでほとんどの情報がわずかに非ゼロな要素に集中しているデータのことを指します。典型的な例としては、テキストデータの単語出現頻度ベクトル、ユーザ行動ログ、ソーシャルネットワークの隣接行列などがあります。\n",
        "\n",
        "#### 理論\n",
        "スパースデータの処理では、メモリ効率や計算効率が重要な課題となります。そのため、スパースデータを効率的に扱うために、特別なデータ構造（例えば、疎行列形式）を利用します。\n",
        "\n",
        "スパースデータの処理では、以下のような手法を使用します。\n",
        "- **圧縮形式の利用**: CSR (Compressed Sparse Row)、CSC (Compressed Sparse Column)、LIL (List of Lists) など。\n",
        "- **次元削減**: 主成分分析(PCA)、特異値分解(SVD)などの手法を利用して次元を削減します。\n",
        "- **正則化**: L1正則化（Lasso回帰）などを用いてモデルを学習します。\n",
        "\n",
        "#### 数式\n",
        "スパースデータの次元削減に関して、特異値分解 (SVD) を考えます。対象とするデータ行列 \\( A \\) を以下のように表現します。\n",
        "\\[\n",
        "A = U \\Sigma V^T\n",
        "\\]\n",
        "ここで、\\( U \\) と \\( V \\) は直交行列であり、\\( \\Sigma \\) は対角行列です。この分解をもとにデータを圧縮し、計算量を大幅に削減できます。\n",
        "\n",
        "#### 使用用途\n",
        "スパースデータは推薦システム、ドキュメント分類、画像処理、ネットワーク解析など、さまざまな分野で利用されています。情報の少ない大規模なデータセットで効率よく計算を行うことが必要な場合に特に有用です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132076b1",
      "metadata": {
        "id": "132076b1"
      },
      "outputs": [],
      "source": [
        "# Pythonでのスパースデータの例: SVDを用いた次元削減\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "# ダミーデータとしてスパース行列を定義します（例えば、ユーザー-アイテム行列）\n",
        "data = [[0, 0, 3, 0, 4],\n",
        "        [0, 0, 0, 0, 5],\n",
        "        [0, 7, 0, 0, 0],\n",
        "        [1, 0, 0, 0, 0]]\n",
        "\n",
        "# サンプル行列をCSRフォーマットに変換します\n",
        "sparse_matrix = csr_matrix(data)\n",
        "\n",
        "# 特異値分解を行います\n",
        "U, sigma, Vt = svds(sparse_matrix, k=2)  # 次元削減のため、kを小さくします\n",
        "\n",
        "# 特異ベクトルと特異値を用いて再構築\n",
        "sigma = np.diag(sigma)\n",
        "approximation = U @ sigma @ Vt\n",
        "\n",
        "print('Original Sparse Matrix:\\n', sparse_matrix.toarray())\n",
        "print('\\nApproximated Matrix after SVD:\\n', approximation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c71af7",
      "metadata": {
        "id": "a3c71af7"
      },
      "source": [
        "学習曲線（Learning Curve）とは、機械学習モデルのパフォーマンスを可視化するためのプロットです。通常、横軸にはトレーニングデータ（トレーニングセットのサイズ）を、縦軸には誤差（例えば、損失やエラーレート）を配置します。これにより、トレーニングセットサイズに対するモデルのパフォーマンスを観察でき、以下のような問題の診断に役立ちます。\n",
        "\n",
        "1. **過学習（Overfitting）**：トレーニングデータに対しては良好なパフォーマンスを示すものの、検証データや未知のデータに対してはパフォーマンスが低下している状態。\n",
        "2. **学習不足（Underfitting）**：トレーニングデータでも検証データでもパフォーマンスが低い状態。\n",
        "\n",
        "理論的には、学習曲線を描くための基本式は誤差関数 \\(L\\) の変化として表現されます。トレーニング誤差と検証誤差の間のギャップを観察することで、モデルのバイアス・バリアンスのトレードオフを理解する助けとなります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03578e59",
      "metadata": {
        "id": "03578e59"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# データセットのロード\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# 学習曲線の計算\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    SVC(), X, y, cv=5, scoring='accuracy',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# 平均と標準偏差を計算\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plt.figure()\n",
        "plt.title('Learning Curves (SVM)')\n",
        "plt.xlabel('Training examples')\n",
        "plt.ylabel('Score')\n",
        "plt.grid()\n",
        "\n",
        "# トレーニングスコア\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                 color=\"r\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score\")\n",
        "\n",
        "# テストスコア\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b02132",
      "metadata": {
        "id": "a1b02132"
      },
      "source": [
        "適応的学習率調整（Adaptive Learning Rate）は、最適化アルゴリズムにおいて、学習率を動的に調整する手法です。従来の勾配降下法では、固定の学習率を設定しますが、適応的学習率を使用することで学習率を各パラメータや更新ステップに応じて動的に調整できます。これにより、勾配が大きい場合は学習率を小さくし、勾配が小さい場合は学習率を大きくすることで、最適解に効率的に到達します。\n",
        "\n",
        "代表的なアルゴリズムにはAdagrad、RMSprop、Adamなどがあります。これらのアルゴリズムは学習率の調整方法においてそれぞれ独特の特徴を持っており、\n",
        "\n",
        "1. **Adagrad**: 二乗勾配の累積和で学習率を調整します。特に、疎なデータセットやオンライン学習に適しています。\n",
        "\n",
        "   更新式は以下の通りです:\n",
        "   \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\, g_t \\]\n",
        "   ここで、\\(G_t\\)は過去の勾配の二乗和、\\(\\epsilon\\)は数値の安定性を確保するための小さい定数です。\n",
        "\n",
        "2. **RMSprop**: Adagradの問題点である学習率の極端な減少を防ぐため、勾配の移動平均を利用します。\n",
        "\n",
        "   更新式は以下の通りです:\n",
        "   \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\, g_t \\]\n",
        "   ここで、\\(E[g^2]_t\\)は過去の勾配の二乗の指数移動平均です。\n",
        "\n",
        "3. **Adam**: RMSpropを基にしながら、勾配の一階モーメントも考慮する。\n",
        "\n",
        "   更新式は以下の通りです:\n",
        "   \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t \\]\n",
        "   \\[ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 \\]\n",
        "   \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\]\n",
        "   \\[ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\]\n",
        "   \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t + \\epsilon}} \\, \\hat{m}_t \\]\n",
        "   ここで、\\(m_t\\)と\\(v_t\\)は一階および二階のモーメントの移動平均を保持します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba32a46d",
      "metadata": {
        "id": "ba32a46d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def adagrad(gradients, learning_rate=0.01, epsilon=1e-8):\n",
        "    cache = np.zeros_like(gradients[0])\n",
        "    for g in gradients:\n",
        "        cache += g ** 2\n",
        "        adjusted_grad = learning_rate * g / (np.sqrt(cache) + epsilon)\n",
        "        print(f'Adagrad adjusted grad: {adjusted_grad}')\n",
        "\n",
        "# Example gradient\n",
        "example_gradients = [np.array([0.1, 0.2, 0.3]), np.array([0.2, 0.1, 0.4])]\n",
        "\n",
        "# Run Adagrad\n",
        "adagrad(example_gradients)\n",
        "\n",
        "def rmsprop(gradients, learning_rate=0.01, rho=0.9, epsilon=1e-8):\n",
        "    cache = np.zeros_like(gradients[0])\n",
        "    for g in gradients:\n",
        "        cache = rho * cache + (1 - rho) * g ** 2\n",
        "        adjusted_grad = learning_rate * g / (np.sqrt(cache) + epsilon)\n",
        "        print(f'RMSprop adjusted grad: {adjusted_grad}')\n",
        "\n",
        "# Run RMSprop\n",
        "rmsprop(example_gradients)\n",
        "\n",
        "def adam(gradients, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    m = np.zeros_like(gradients[0])\n",
        "    v = np.zeros_like(gradients[0])\n",
        "    t = 0\n",
        "    for g in gradients:\n",
        "        t += 1\n",
        "        m = beta1 * m + (1 - beta1) * g\n",
        "        v = beta2 * v + (1 - beta2) * np.square(g)\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "        adjusted_grad = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "        print(f'Adam adjusted grad: {adjusted_grad}')\n",
        "\n",
        "# Run Adam\n",
        "adam(example_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b933b330",
      "metadata": {
        "id": "b933b330"
      },
      "source": [
        "### 対数尤度の計算について  \n",
        "\n",
        "#### 理論  \n",
        "対数尤度（Log-Likelihood）は、統計モデルが与えられたデータを説明する適合度を測るための尺度です。尤度は確率や確率密度の値を表しており、モデルによってデータがどれだけ生成されやすいかを示します。  \n",
        "多くのパラメータ推定問題では、尤度関数を最大化することによってモデルのパラメータを推定します。この際、計算の簡略化と数値安定性のために、対数を用いて尤度関数を最大化するのが一般的です。  \n",
        "\n",
        "#### 数式  \n",
        "あるモデル $M$ とパラメータ $\theta$ の下でデータ $x$ が観測される確率を $P(x|\theta)$ としたとき、尤度 $L(\theta|x)$ は次のように定義されます。  \n",
        "\n",
        "\\[ L(\theta|x) = P(x|\theta) \\]  \n",
        "\n",
        "対数尤度はこの尤度の対数を取ったもので、次の式で表されます：  \n",
        "\n",
        "\\[ \\ell(\theta|x) = \\log L(\theta|x) = \\log P(x|\theta) \\]  \n",
        "\n",
        "#### 使用用途  \n",
        "対数尤度は主に以下のような状況で使用されます：  \n",
        "- **推定**：パラメータ推定のために、対数尤度を最大化することにより最尤推定を行います。  \n",
        "- **モデル比較**：異なるモデルの適合度を比較するために対数尤度比検定を用います。  \n",
        "- **仮説検定**：対数尤度比を用いてモデルの適合度の違いを検証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be817228",
      "metadata": {
        "id": "be817228"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 簡単な例として、正規分布のデータを扱います\n",
        "np.random.seed(42)\n",
        "# 平均0、標準偏差1の正規分布に従う1000個のデータ\n",
        "sample_data = np.random.normal(0, 1, 1000)\n",
        "\n",
        "# 正規分布の対数尤度を計算する関数\n",
        "def log_likelihood_normal(data, mu, sigma):\n",
        "    n = len(data)\n",
        "    log_likelihood = -n/2 * np.log(2 * np.pi) - n/2 * np.log(sigma**2) - 0.5 * np.sum((data - mu)**2) / sigma**2\n",
        "    return log_likelihood\n",
        "\n",
        "# 平均0、標準偏差1で計算\n",
        "e_mu, e_sigma = 0, 1\n",
        "log_likelihood = log_likelihood_normal(sample_data, e_mu, e_sigma)\n",
        "print(f'Log-Likelihood: {log_likelihood}')\n",
        "```\n",
        "このコードでは、正規分布に基づくデータに対して与えられた平均と標準偏差に基づいて対数尤度を計算しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b195bb",
      "metadata": {
        "id": "54b195bb"
      },
      "source": [
        "## 勾配ブースティング決定木 (GBDT) の解説\n",
        "\n",
        "### 理論\n",
        "勾配ブースティング決定木 (Gradient Boosting Decision Trees; GBDT) は、アンサンブル学習の一種で、特に回帰問題や分類問題における予測精度の向上を目的としています。GBDT は、弱学習器を逐次的に構築し、それぞれの弱学習器が前のものの誤差を修正することで最終的な強学習器を構成します。\n",
        "\n",
        "具体的には、最初に簡単なモデル（通常は決定木）がデータに適合されます。その後、モデル全体の誤差を減少させることを目的として、この最初のモデルの誤差を説明する新たなモデルが追加されます。これが繰り返されることで、モデルの集合が作られます。\n",
        "\n",
        "GBDTは以下の３つの要素に基づいています：\n",
        "- **損失関数** \\( L(y, F(x)) \\): モデルがどれだけ良くデータを予測するかを評価します。一般的に二乗誤差や対数誤差が用いられます。\n",
        "- **弱学習器**: 通常は決定木が使われます。\n",
        "- **勾配降下法**: 誤差を最小化するための反復最適化手法です。\n",
        "\n",
        "### 数式\n",
        "モデルの更新において、損失関数の勾配を計算し、それに基づいてモデルを改善します。\n",
        "\n",
        "1. 初期モデル: \\(F_0(x) = \\arg\\min_\\rho \\sum_{i=1}^N L(y_i, \\rho)\\)\n",
        "2. 次の反復でのモデル更新: \\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n",
        "   - ここで、\\(h_m(x)\\) は第m番目の弱学習器\n",
        "   - \\(\\gamma_m\\) は学習率\n",
        "3. \\(h_m(x)\\) は以下のようにして得られる：\\(h_m(x) = -g_m(x)\\)\n",
        "   - \\(g_m(x)\\) = 損失関数の勾配 = \\(\\left[\\frac{\\partial L(y, F(x))}{\\partial F(x)}\\right]_{F(x) = F_{m-1}(x)}\\)\n",
        "\n",
        "### 使用用途\n",
        "GBDTはさまざまなデータ分析のタスクにおいて広く使用されています。特に複雑な非線形関係を持つデータセットにおいて、高い予測精度を達成するために用いられます。また、GBDTは特徴選択や外れ値の処理にも強い特性を持っています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e72feff",
      "metadata": {
        "id": "0e72feff"
      },
      "outputs": [],
      "source": [
        "# PythonコードによるGBDTの基本的な手作り実装\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.initial_model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # 初期モデルの適合\n",
        "        self.initial_model = np.mean(y)\n",
        "        Fm = np.full(y.shape, self.initial_model)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # 勾配(残差)の計算\n",
        "            residuals = y - Fm\n",
        "\n",
        "            # 決定木のモデルを適合\n",
        "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            model.fit(X, residuals)\n",
        "\n",
        "            # モデルをリストに追加\n",
        "            self.models.append(model)\n",
        "\n",
        "            # モデルの予測を更新\n",
        "            Fm += self.learning_rate * model.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # 初期モデルの予測から開始\n",
        "        Fm = np.full((X.shape[0],), self.initial_model)\n",
        "\n",
        "        for model in self.models:\n",
        "            Fm += self.learning_rate * model.predict(X)\n",
        "\n",
        "        return Fm\n",
        "\n",
        "# 使用例:\n",
        "# X, yに適当なデータをセットして実行\n",
        "# gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "# gbr.fit(X, y)\n",
        "# predictions = gbr.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b578fa15",
      "metadata": {
        "id": "b578fa15"
      },
      "source": [
        "### 推論速度の最適化とは\n",
        "\n",
        "推論速度の最適化とは、機械学習モデルが新しいデータを使って予測を行う際の時間を短縮するための手法やプロセスを指します。これにより、リアルタイム処理が求められるシステムやリソースが限られているデバイス上での効率的な運用が可能になります。\n",
        "\n",
        "#### 理論\n",
        "推論速度の最適化は以下のような側面からアプローチされます。\n",
        "\n",
        "1. **モデルの簡素化**: モデルパラメータの削減や、モデルの軽量化（例えば知識蒸留（Knowledge Distillation）やプルーニング（Pruning））を行います。\n",
        "   - 知識蒸留では、より大きなモデルが与えた予測を小さなモデルが模倣することで、モデルのサイズを小さくします。\n",
        "   - プルーニングでは、寄与の小さいモデル重みやニューロンを削除し、計算量を削減します。\n",
        "\n",
        "2. **数値表現の変更**: モデルのパラメータや計算を固定小数点や半精度浮動小数点にすることにより、精度を若干犠牲にして計算速度を向上させます。\n",
        "   - 特に`INT8`量子化はよく使われ、通常の`FP32`の重みを8ビット整数に変換し、計算量を削減します。\n",
        "\n",
        "3. **並列化**: GPUやTPUの使用、バッチ処理による並列化など、ハードウェアを活用した並列計算により推論を高速化します。\n",
        "\n",
        "4. **効率的なアルゴリズム**: より効率的なアルゴリズムを設計することにより、同じ作業をより少ない計算で実現します。\n",
        "\n",
        "#### 使用用途\n",
        "- **リアルタイム応答**: 自動運転車、音声アシスタント、リアルタイム翻訳など。\n",
        "- **エッジデバイス**: スマートフォンやIoTデバイスなど、計算資源が限られるデバイス上での動作に必要。\n",
        "- **大規模デプロイメント**: サーバーコストを削減しつつ、多くのリクエストに応答するシステムで必要。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a771f22",
      "metadata": {
        "id": "5a771f22"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# モデルの定義例\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 元のモデルのサイズと速度の確認\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ダミーデータで計測\n",
        "import numpy as np\n",
        "data = np.random.rand(1000, 784).astype(np.float32)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "model.predict(data)\n",
        "end_time = time.time()\n",
        "print(\"Original Model Inference Time: {}s\".format(end_time - start_time))\n",
        "\n",
        "# TensorFlowのモデル量子化\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# 量子化後のモデルの推論速度の確認\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    interpreter.set_tensor(input_details[0]['index'], data[:1])\n",
        "    interpreter.invoke()\n",
        "result = interpreter.get_tensor(output_details[0]['index'])\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Quantized Model Inference Time: {}s\".format((end_time - start_time) / 100))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea14431",
      "metadata": {
        "id": "3ea14431"
      },
      "source": [
        "クラスタリングは、データセットをいくつかのグループ（クラスタ）に分けることで、同じクラスタ内のデータポイントが互いに似ている度合いを最大化し、異なるクラスタ間では異なる度合いを最大化する手法です。このプロセスは、探索的データ解析やパターン認識、データ圧縮などの用途で用いられます。代表的なクラスタリング手法には、k-meansクラスタリング、階層的クラスタリング、DBSCANなどがあります。クラスタリングとの組み合わせという概念は、クラスタリング技法と他の機械学習手法を組み合わせて、より強力なデータ分析を行います。たとえば、クラスタ解析を前処理として用いて、分類や回帰モデルの性能を向上させることがあります。数式として、k-meansクラスタリングは次のように表されます。与えられたn個のデータポイントをk個のクラスタに分けるために、各データポイント\\(x_i\\)に対するクラスタ中心\\(\\mu_j\\)の割り当てを繰り返し最小化します。\\[ \\sum_{i=1}^{n} \\sum_{j=1}^{k} ||x_i - \\mu_j||^2 \\] ここで、データポイント\\(x_i\\)とクラスタ中心\\(\\mu_j\\)のユークリッド距離が計算され、全体の誤差が最小化されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d004cd",
      "metadata": {
        "id": "38d004cd"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# サンプルデータの作成\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# k-meansによるクラスタリング\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# クラスタ割り当てのプロット\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
        "plt.title('クラスタリング結果')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}