{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koheikobayashi/machine-learning/blob/main/LightGBM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc4b0a5",
      "metadata": {
        "id": "8bc4b0a5"
      },
      "source": [
        "### Gini Impurityの解説\n",
        "\n",
        "Gini Impurity（ジニ不純度）は、決定木アルゴリズムにおけるノードの純度を計算するための尺度です。これは分類問題において、あるノードに分類されたデータサンプルの不純度を示します。\n",
        "\n",
        "インライン数式で表現すると、ジニ不純度$G$は次のように計算されます：$G = 1 - \\sum_{i=1}^{n} p_{i}^2$。ここで、$p_i$はそのノードにおけるクラス$i$の出現確率を意味します。\n",
        "\n",
        "ブロック数式で表現すると次の通りです：\n",
        "$$ G = 1 - \\sum_{i=1}^{n} p_{i}^2 $$\n",
        "\n",
        "ジニ不純度は0から1の間の値を取り、値が0に近いほどノード内のデータの純度が高く、1に近いほど不純度が高いことを示します。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMはGradient Boosting Decision Treeを用いたフレームワークであり、ジニ不純度はこの決定木アルゴリズムにおけるスプリット基準の一つとして使用されます。ジニ不純度を最小化するようにデータをスプリットすることで、ノードの純度を高め、モデルの精度を向上させます。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "ジニ不純度は主に決定木やランダムフォレストといったアルゴリズムで利用され、ノードの最適な分割を探す際に用いられます。この指標を用いることで、データの異なるクラスがどの程度混ざり合っているかを定量的に評価することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c923cec6",
      "metadata": {
        "id": "c923cec6"
      },
      "outputs": [],
      "source": [
        "# Gini Impurityを計算するPythonコード\n",
        "\n",
        "def gini_impurity(labels):\n",
        "    \"\"\"\n",
        "    与えられたラベルのリストに基づいてジニ不純度を計算する関数。\n",
        "    :param labels: 各データポイントのクラスラベルのリスト。\n",
        "    :return: ラベルのジニ不純度を返す。\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    # クラスラベルの頻度を計算\n",
        "    label_counts = Counter(labels)\n",
        "    total_count = len(labels)\n",
        "\n",
        "    # ジニ不純度の計算\n",
        "    impurity = 1.0\n",
        "    for label in label_counts:\n",
        "        # クラスiの確率を計算\n",
        "        prob_of_label = label_counts[label] / total_count\n",
        "        # 不純度を更新\n",
        "        impurity -= prob_of_label ** 2\n",
        "\n",
        "    return impurity\n",
        "\n",
        "# Example usage\n",
        "labels = [\"A\", \"A\", \"B\", \"B\", \"B\", \"C\"]  # ラベルの例\n",
        "print(\"Gini Impurity:\", gini_impurity(labels))  # ジニ不純度を出力"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66923646",
      "metadata": {
        "id": "66923646"
      },
      "source": [
        "エントロピーは情報理論における重要な概念であり、情報量や不確実性を定量化するために使用されます。エントロピーは、与えられた確率分布において、どれだけの情報が存在するかを計測します。具体的には、エントロピー $H(X)$ は次のように定義されます。\n",
        "\n",
        "$$ H(X) = -\\sum_{i} p(x_i) \\log(p(x_i)) $$\n",
        "\n",
        "ここで、$p(x_i)$ は事象 $x_i$ が発生する確率を表します。エントロピーは不確実性の尺度としての性質を持ち、確率分布が均一であるほどエントロピーは大きくなります。\n",
        "\n",
        "LightGBMでは、エントロピーが不純度の指標として使用されることがあります。決定木ベースのモデルでは、分岐を行う際にデータの不純度を測定するために情報ゲインを計算します。この情報ゲインの計算にはエントロピーが利用されます。具体的には、情報ゲインは分岐前後のエントロピーの差として求められ、データをどのように分割するかを決定する際の指標として用いられます。分岐後のエントロピーが低いほど、より純度の高い（つまり一方的なクラス分類が可能な）データセットに分けられることを意味します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfb5bf5",
      "metadata": {
        "id": "cbfb5bf5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_entropy(probabilities):\n",
        "    # エントロピーを計算する関数\n",
        "    # -p(x) * log(p(x)) の計算を行い、それらを足し合わせる\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "# 例として、それぞれの事象が発生する確率\n",
        "probabilities = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "# エントロピーの計算\n",
        "entropy = calculate_entropy(probabilities)\n",
        "\n",
        "# 計算結果を出力\n",
        "print('Entropy:', entropy)\n",
        "\n",
        "# 上記のコードはエントロピーを計算するためのシンプルな例です。\n",
        "# エントロピーが最大になるのはすべての事象の確率が均一なときです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c79f4e",
      "metadata": {
        "id": "79c79f4e"
      },
      "source": [
        "### MSE（Mean Squared Error）について\n",
        "\n",
        "MSE（Mean Squared Error）は、平均二乗誤差と呼ばれ、予測モデルの精度を評価するための指標です。特に、回帰モデルの性能を評価するためによく使用されます。MSEは、予測値と実際の値との差の二乗和の平均を求めたものです。\n",
        "\n",
        "インライン数式で表現すると、$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ となります。\n",
        "\n",
        "ブロック数式で表現すると、\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "です。\n",
        "\n",
        "ここで、$n$ はデータの数、$y_i$ は実際の値、$\\hat{y}_i$ は予測値です。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは勾配ブースティング決定木（GBDT）の実装の一つで、回帰問題においてはMSEを目的関数として最適化できます。MSEは、誤差を平方しているため、誤差が大きいデータポイントにより大きなペナルティを与える特徴があります。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "- **モデル評価**: 回帰モデルの精度を評価する標準的な指標。\n",
        "- **モデル比較**: 複数のモデルを比較する際に、MSEを用いてどのモデルがより精度が高いかを判断します。\n",
        "\n",
        "MSEは、モデルの予測の質を数値化することで、モデルのチューニングや改善の指針を与えてくれます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e73e97",
      "metadata": {
        "id": "a4e73e97"
      },
      "outputs": [],
      "source": [
        "# MSEを計算するためのPythonコード\n",
        "\n",
        "# 必要なライブラリをインポートします\n",
        "import numpy as np\n",
        "\n",
        "# 実際の値 (例)\n",
        "y_true = np.array([3.0, -0.5, 2.0, 7.0])\n",
        "\n",
        "# 予測値 (例)\n",
        "y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n",
        "\n",
        "# MSEを計算する関数を定義します\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    # 誤差を計算します\n",
        "    error = y_true - y_pred\n",
        "\n",
        "    # 誤差を二乗して平均値をとります\n",
        "    mse = np.mean(error ** 2)\n",
        "\n",
        "    # MSEを返します\n",
        "    return mse\n",
        "\n",
        "# 関数を呼び出して、MSEを計算します\n",
        "mse_result = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# MSEを出力します\n",
        "print(f'Mean Squared Error: {mse_result}') # 結果は0.375になります"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e5f32c",
      "metadata": {
        "id": "07e5f32c"
      },
      "source": [
        "### 情報利得 (Information Gain)\n",
        "\n",
        "情報利得は、機械学習や統計学においてデータの分類に関する指標として広く使用されます。決定木アルゴリズム（例: ID3、C4.5、CART）でノードを分割するための重要な基準の1つです。LightGBMは、勾配ブースティングフレームワークであり、高速で高精度なモデル構築を可能にするアルゴリズムです。LightGBMでも、決定木の分割基準として情報利得が使用されます。情報利得は、分割することによって得られる不純度の減少量を測定します。不純度が低いほどデータは分類されやすいため、情報利得はこの不純度の指標となります。\n",
        "\n",
        "#### 数式\n",
        "情報利得は、親ノードと子ノードのエントロピーの差として表されます。エントロピー$E(S)$は以下のように定義されます。\n",
        "\n",
        "インライン数式:\n",
        "$E(S) = - \\sum_{i=1}^{n} p_i \\log_2 p_i$\n",
        "\n",
        "ブロック数式:\n",
        "$$E(S) = - \\sum_{i=1}^{n} p_i \\log_2 p_i$$\n",
        "\n",
        "ここで、$p_i$はクラス$i$に属するデータ点の割合を示します。\n",
        "\n",
        "情報利得$IG(T, a)$は次のように定義されます。\n",
        "\n",
        "インライン数式:\n",
        "$IG(T, a) = E(T) - \\sum_{v \\in \\,Values(a)} \\frac{|T_v|}{|T|} E(T_v)$\n",
        "\n",
        "ブロック数式:\n",
        "$$IG(T, a) = E(T) - \\sum_{v \\in \\,Values(a)} \\frac{|T_v|}{|T|} E(T_v)$$\n",
        "\n",
        "ここで、$T$はデータセット、$a$は属性、$T_v$は属性$a$の値$v$に基づいたデータセットのサブセットです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45c090ff",
      "metadata": {
        "id": "45c090ff"
      },
      "outputs": [],
      "source": [
        "# 必要なモジュールのインポート\n",
        "import numpy as np\n",
        "\n",
        "# クラスの確率分布からエントロピーを計算する関数\n",
        "def entropy(prob_dist):\n",
        "    # エントロピーの計算\n",
        "    return -np.sum([p * np.log2(p) for p in prob_dist if p > 0])\n",
        "\n",
        "# 親ノードと子ノードから情報利得を計算する関数\n",
        "def information_gain(parent, children):\n",
        "    # 親ノードのエントロピー\n",
        "    parent_entropy = entropy(parent)\n",
        "    # 子ノードの合計エントロピー\n",
        "    children_entropy = sum((len(child) / sum(len(c) for c in children)) * entropy(child) for child in children)\n",
        "    # 情報利得\n",
        "    return parent_entropy - children_entropy\n",
        "\n",
        "# 使用例: クラスの確率分布を親と子に分けた場合の情報利得を計算\n",
        "parent_distribution = [0.5, 0.5]  # 親ノードのクラス分布\n",
        "child_distribution_1 = [0.8, 0.2] # 子ノード1のクラス分布\n",
        "child_distribution_2 = [0.2, 0.8] # 子ノード2のクラス分布\n",
        "\n",
        "# 情報利得の計算\n",
        "ig = information_gain(parent_distribution, [child_distribution_1, child_distribution_2])\n",
        "print(\"情報利得:\", ig)\n",
        "\n",
        "# このコードで計算される情報利得は、分割によってどれだけ不純度が減少したかを示します。\n",
        "# 親ノードや子ノードのクラス確率分布を変更することで、異なる情報利得を得ることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e468d0d6",
      "metadata": {
        "id": "e468d0d6"
      },
      "source": [
        "勾配 (Gradient) は、多変量微分の概念であり、関数の出力がどの方向に最も増加するかを示します。数式的には、勾配はベクトルで表され、一つの関数 $f(x_1, x_2, \\ldots, x_n)$ の勾配は次のように定義されます。$$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)$$  \n",
        "ここで、$\\frac{\\partial f}{\\partial x_i}$ は関数の $x_i$ に対する偏微分を示します。\n",
        "\n",
        "LightGBMにおいて、勾配は損失関数の最適化のために使用されます。勾配が示す方向に沿ってパラメータを更新することで、モデルがデータに対する誤差を最小化するように調整されます。具体的には、Boosting の枠組みで、木の追加によるモデルの改善を行うために勾配が利用されます。\n",
        "\n",
        "勾配の主な用途は、最適化問題における目的関数の勾配降下法 (Gradient Descent) による解法です。勾配降下法では、反復的に勾配を利用して関数の最小値または最大値を求める方法が取られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac36b6fb",
      "metadata": {
        "id": "ac36b6fb"
      },
      "outputs": [],
      "source": [
        "# Pythonでの勾配の計算例\n",
        "import numpy as np\n",
        "\n",
        "def function(x1, x2):\n",
        "    # 例として二変数関数 f(x1, x2) = x1^2 + x2^2\n",
        "    return x1**2 + x2**2\n",
        "\n",
        "def gradient(x1, x2):\n",
        "    # 関数 f の勾配を計算する\n",
        "    df_dx1 = 2 * x1  # x1 に関する偏微分\n",
        "    df_dx2 = 2 * x2  # x2 に関する偏微分\n",
        "    return np.array([df_dx1, df_dx2])\n",
        "\n",
        "# 点(1, 2)における勾配を計算\n",
        "x1, x2 = 1.0, 2.0\n",
        "grad = gradient(x1, x2)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"Function gradient at ({x1}, {x2}): {grad}\")\n",
        "# 出力: Function gradient at (1.0, 2.0): [2. 4.]\n",
        "# これは点 (1, 2) での勾配を示し、x1 については 2、x2 については 4 です。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790d6ced",
      "metadata": {
        "id": "790d6ced"
      },
      "source": [
        "### ヘッシアン (Hessian) について\n",
        "\n",
        "ヘッシアンは、2次微分の行列であり、多変数関数の2次偏微分の情報をまとめたものです。具体的には、多変数関数 \\( f(x_1, x_2, \\ldots, x_n) \\) のヘッシアンは以下のように定義されます：\n",
        "\n",
        "$$\n",
        "H(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "LightGBMにおいて、ヘッシアンは勾配ブースティング決定木 (GBDT) を使用する際に重要な役割を果たします。具体的には、各ステップで新しい決定木を構築するための損失関数の二次情報を提供します。この情報は、最適な分割や値の更新に役立つため、収束速度やモデルの精度に影響を与えます。\n",
        "\n",
        "使用用途としては、最適化問題におけるニュートン法や擬似ニュートン法、また機械学習におけるパラメータ更新など、様々な場面で利用されます。この行列が正定値である場合、関数は凹または凸であることが保証され、最適化問題の解への収束を助けます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39fbebd",
      "metadata": {
        "id": "f39fbebd"
      },
      "outputs": [],
      "source": [
        "# ヘッシアン行列を計算するためのPythonコード\n",
        "import sympy as sp\n",
        "\n",
        "# 変数を定義\n",
        "x1, x2 = sp.symbols('x1 x2')\n",
        "\n",
        "# 解析する関数を定義（例として二変数の関数）\n",
        "f = x1**2 + 3*x1*x2 + x2**2\n",
        "\n",
        "# 関数のヘッシアンを計算\n",
        "hessian_matrix = sp.hessian(f, (x1, x2))\n",
        "\n",
        "# ヘッシアン行列を表示\n",
        "print(\"Hessian Matrix:\")\n",
        "display(hessian_matrix)\n",
        "\n",
        "# 以下は計算処理の詳細（コメント）\n",
        "# - sympyライブラリを使用して関数のシンボリックな微分を行います。\n",
        "# - 二つの変数、x1とx2を定義します。\n",
        "# - 解析対象となる二変数の関数f(x)を定義します。\n",
        "# - sympy.hessian関数を利用して、関数fのヘッシアン行列を計算します。\n",
        "# - 結果のヘッシアン行列を表示します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b687ac2a",
      "metadata": {
        "id": "b687ac2a"
      },
      "source": [
        "Leaf-wise Growthとは、LightGBM（Light Gradient Boosting Machine）の決定木の成長アルゴリズムのひとつで、葉の数が最も多く情報利得を増加させるように成長させる方法です。これは、レベルごとに成長するdepth-wise growthとは対照的です。Leaf-wise Growthの利点は、特徴量が多いデータセットや均一でないデータセットにおいてモデルの性能を著しく向上させることです。数式的には、分割後の情報利得を最大化する葉ノードを選んで分割を行います。具体的には、分割による利得を $Gain =\n",
        "rac{1}{2} \\left(\n",
        "rac{(G_L^2)}{H_L + \\lambda} +\n",
        "rac{(G_R^2)}{H_R + \\lambda} -\n",
        "rac{(G^2)}{H + \\lambda}\n",
        "ight) - \\gamma$ と表現し、最大となる葉ノードを選びます。ここで $G$ は勾配の合計、$H$ はヘッセの合計、$\\lambda$ は正則化項、$\\gamma$ は分割のペナルティです。Leaf-wise Growthは特にバイナリ分類や回帰問題において有効で、より少ないメモリで計算できることから大規模データに適しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44b27c3",
      "metadata": {
        "id": "b44b27c3"
      },
      "outputs": [],
      "source": [
        "# Leaf-wise Growthのデモンストレーションを行うためのコード\n",
        "from lightgbm import LGBMRegressor\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ダミーデータの作成\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# トレーニングデータとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# モデルの作成、デフォルトではleaf-wise growthを使用\n",
        "model = LGBMRegressor(learning_rate=0.1, n_estimators=100)\n",
        "\n",
        "# モデルのトレーニング\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# テストデータでの予測\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 平均二乗誤差を計算して表示\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# このコードはLightGBMにおけるLeaf-wise Growthの基本的な使用例を示し、\n",
        "# デフォルトでleaf-wiseな成長戦略を採用している点に注目してください。\n",
        "# make_regression関数で生成した回帰問題を使い、\n",
        "# 訓練データとテストデータに分割後、\n",
        "# 単純な回帰モデルをLightGBMで構築します。\n",
        "# 学習済みモデルをテストデータで評価し、\n",
        "# 平均二乗誤差を出力することでモデルの性能を示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0e7516",
      "metadata": {
        "id": "ab0e7516"
      },
      "source": [
        "## Histogram-based Splittingについて\n",
        "\n",
        "Histogram-based Splitting（ヒストグラムベースの分割）とは、LightGBMなどの勾配ブースティング決定木 (GBDT) で使用される技術です。この技術は、データを効率的に処理し、モデルのトレーニングを高速化するために設計されています。\n",
        "\n",
        "### 理論\n",
        "従来の決定木ベースのモデルでは、各特徴量のすべての候補分割点に対して反復的に損失を計算することで、最適な分割点を探します。しかし、これは計算量が多く、特に大規模データセットでは処理が遅くなります。ヒストグラムベースの分割では、以下の手順を取ります：\n",
        "\n",
        "1. **特徴量のビン化**: まず各特徴量の連続値を有限のビン（区間）に分割します。これは階級値のヒストグラムを作ることに相当します。この過程では、連続値が指定されたビン数（例えば 255 ビンなど）にまとめられます。\n",
        "\n",
        "2. **統計量の計算**: 各ビンに対して、目的関数の勾配とヘッセ行列などの統計量を計算します。\n",
        "\n",
        "3. **最適なビンの選択**: ビンごとに計算した統計量を用いて、ノードの分割基準（情報利得やジニ不純度など）を評価します。最も基準を満たすようなビンを選び、その境界を用いてノードを分割します。\n",
        "\n",
        "これにより、計算する候補分割点は元の特徴量数からビンの数だけに削減され、計算効率が大幅に向上します。\n",
        "\n",
        "### 数式\n",
        "この手法の数式化は以下のように行われます。ここでは、ビン化を $b(x)$ とし、$I_j$ はビン $j$ に含まれるインスタンスのセットとします。目的関数は一例として二乗誤差を用いた勾配ブースティングを考えます。\n",
        "\n",
        "1. **勾配の総和**をビンごとに計算：\n",
        "$$ G_j = \\sum_{i \\in I_j} g_i $$\n",
        "\n",
        "2. **ヘッシアンの総和**をビンごとに計算：\n",
        "$$ H_j = \\sum_{i \\in I_j} h_i $$\n",
        "\n",
        "3. **分割ゲイン**: 各ビンの分割ゲインを計算して、最適な分割点を選択。\n",
        "\n",
        "### 使用用途\n",
        "ヒストグラムベースの分割は、特に大規模データセットや高次元データに適しています。この手法により、モデルの学習が効率的に行え、リソース消費を抑えつつ高精度なモデルが構築可能です。LightGBMはその高速・高性能な特性を支える核となる技術の一つとしてこの手法を採用しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506cc852",
      "metadata": {
        "id": "506cc852"
      },
      "outputs": [],
      "source": [
        "# ヒストグラムベースの分割を示すPythonコード例\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# データセットにおける特徴量の配列\n",
        "feature_values = np.array([2.3, 1.9, 3.1, 4.2, 5.0])\n",
        "\n",
        "# ビンの数を指定\n",
        "num_bins = 3\n",
        "\n",
        "# 最小値から最大値まで均等にビンを作成\n",
        "bins = np.linspace(np.min(feature_values), np.max(feature_values), num_bins + 1)\n",
        "\n",
        "# ビン化 - 各値を対応するビンに割り当て\n",
        "binned_values = np.digitize(feature_values, bins) - 1 # np.digitizeの結果は1始まりなので-1\n",
        "\n",
        "# 各ビンの勾配とヘッシアンの総和を計算するためのダミー勾配とヘッシアン\n",
        "fake_gradients = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n",
        "fake_hessians = np.array([0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "# ビンごとの勾配とヘッシアンの総和を初期化\n",
        "G = np.zeros(num_bins)\n",
        "H = np.zeros(num_bins)\n",
        "\n",
        "# 各サンプルのビンに対して勾配とヘッシアンを合計\n",
        "for i in range(len(feature_values)):\n",
        "    bin_idx = binned_values[i]\n",
        "    G[bin_idx] += fake_gradients[i]\n",
        "    H[bin_idx] += fake_hessians[i]\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Bins:\", bins)\n",
        "print(\"Binned Values:\", binned_values)\n",
        "print(\"Gradient Sums G:\", G)\n",
        "print(\"Hessian Sums H:\", H)\n",
        "\n",
        "# 各ビンごとに適切な分割ゲインを計算し、最適な分割基準を決めるために使用可能\n",
        "# この処理は、実際のモデルではより詳細な計算によって続きます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb2783e",
      "metadata": {
        "id": "5bb2783e"
      },
      "source": [
        "GOSS (Gradient-based One-Side Sampling) は、LightGBM において計算効率を向上させるためのサンプリング手法です。従来の勾配ブースティングマシンでは、すべてのデータを使用してモデルを構築するため、計算コストが高くなることがあります。GOSS は、重要度に基づいてデータのサンプリングを行い、パフォーマンスと精度を維持しつつ計算コストを削減します。GOSS のアイデアは、勾配が大きいサンプルは重要であるため、必ず使用し、勾配が小さいサンプルの中から一部をランダムに選ぶというものです。まず、最初のステップとして、大きな勾配を持つ上位a%のインスタンスを保持します。そして、残りの小さな勾配を持つインスタンスからランダムにb%をサンプリングします。この手法によって、モデルの精度を犠牲にすることなく、計算リソースの使用を効果的に削減できます。\n",
        "\n",
        "数式としては次のように表されます。インライン数式で示すと、重要度に基づいたインスタンスの選択は\\( G_i \\)に基づいています。例えば、\\( a \\)が選択されたインスタンスの割合であり、上位のインスタンスを選択し、\\( b \\)はランダムに選ばれる割合を表します。\n",
        "\n",
        "ブロック数式としては以下のようになります：\n",
        "\n",
        "$$\n",
        "S = \\{(x_i, y_i) | x_i \\text{ s.t. } G_i \\text{ is top } a\\%\\} + \\{(x_i, y_i) | x_i \\text{ randomly selected with probability } b\\%\\}\n",
        "$$\n",
        "\n",
        "この手法は特に大規模データセットに適しており、LightGBM のような大規模なデータに対する高効率のトレーニングを可能にしています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81bb3d0",
      "metadata": {
        "id": "a81bb3d0"
      },
      "outputs": [],
      "source": [
        "# GOSS (Gradient-based One-Side Sampling)を理解するためのPythonコード\n",
        "# 必要なライブラリをインポート\n",
        "import numpy as np\n",
        "\n",
        "# 仮の損失勾配を生成する\n",
        "np.random.seed(0)\n",
        "gradients = np.random.rand(1000)  # 1000個のサンプルの勾配\n",
        "\n",
        "# 勾配によるしきい値の割合を決める\n",
        "a = 0.2  # 勾配が大きい方の20%を選択\n",
        "b = 0.1  # 残りのサンプルからランダムに10%選択\n",
        "\n",
        "# インデックスと共に勾配をソートする\n",
        "sorted_indices = np.argsort(-gradients)  # 勾配が大きい順にソート\n",
        "\n",
        "# 上位a%のサンプルを選ぶ\n",
        "n_a = int(a * len(gradients))\n",
        "selected_large_gradients = sorted_indices[:n_a]\n",
        "\n",
        "# 残りのサンプルからb%をランダムに選ぶ\n",
        "remaining_indices = sorted_indices[n_a:]\n",
        "np.random.shuffle(remaining_indices)\n",
        "n_b = int(b * len(gradients))\n",
        "selected_small_gradients = remaining_indices[:n_b]\n",
        "\n",
        "# 最終的なサンプルのインデックスを取得\n",
        "final_sample_indices = np.concatenate([selected_large_gradients, selected_small_gradients])\n",
        "\n",
        "# 選択されたインデックスを出力\n",
        "print(f'Selected sample indices ({len(final_sample_indices)} samples): {final_sample_indices}')\n",
        "\n",
        "# このコードはGOSSのサンプリング方法をシミュレートしています。\n",
        "# 最初に大きな勾配を持つ上位a%のインデックスを選択し、\n",
        "# 次に残りからランダムにb%のインデックスを選びます。\n",
        "# 最終的に選択されたインデックスを出力します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1de9c8b",
      "metadata": {
        "id": "a1de9c8b"
      },
      "source": [
        "### Exclusive Feature Bundling (EFB)\n",
        "\n",
        "**Exclusive Feature Bundling (EFB)** は、カテゴリカルやワンホットエンコードされたデータにおいて、特徴量の冗長性を削減するための技術です。EFBの狙いは、**相互に排他的である特徴量を一つの特徴量にまとめること**にあります。一般に、機械学習モデルで扱うデータセットでは多くの特徴量がワンホットエンコードされたカテゴリカルデータとして存在しますが、これが理由で次元が膨れ上がってしまうことがあります。EFBはこの問題を効果的に解消します。\n",
        "\n",
        "#### 理論\n",
        "EFBは次のような特徴に依存しています: 特徴量集合$F_1, F_2, \\ldots, F_n$において、ある2つの特徴量集合$F_i$と$F_j$が同時に1を持たない（つまり、$F_i \\cap F_j = \\emptyset$）場合、それらをまとめて一つに束ねることが可能です。具体的には、これらをrebundlingすることで、新しい特徴量$b_k = f_i + f_j$のように表現できます。ただし、この場合、束ねた後の特徴量における特異な特徴情報が失われることはありません。\n",
        "\n",
        "#### 数式\n",
        "以下のように2つの特徴量$f_1$と$f_2$が排他的である場合を考えます:\n",
        "- $f_1 \\times f_2 = 0$ (すなわち、同時に1にならない)\n",
        "\n",
        "これらを束ねることで単一の特徴量 $b$ に集約できます。\n",
        "$$ b = f_1 + f_2 $$\n",
        "この新しい特徴量$b$は$f_1$と$f_2$の情報を完全に保存しています。\n",
        "\n",
        "#### 使用用途\n",
        "EFBはLightGBM（Microsoftが開発した勾配ブースティングフレームワーク）において重要な役割を果たします。特に大規模データセットでの学習速度を向上させ、メモリの使用量を削減します。LightGBMは内部的にこの技術を利用してデータ前処理の圧縮を行い、より効率的なモデル学習を実現しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1859ed",
      "metadata": {
        "id": "0d1859ed"
      },
      "outputs": [],
      "source": [
        "# PythonでExclusive Feature Bundlingを実装する例\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# サンプルデータ作成: ワンホットエンコードされたデータを模倣\n",
        "# 0-1のカテゴリカルデータが2つ、それぞれの特徴が排他的であると仮定\n",
        "np.random.seed(0)\n",
        "data = {\n",
        "    'feature_1': np.random.choice([0, 1], size=10),\n",
        "    'feature_2': np.random.choice([0, 1], size=10)\n",
        "}\n",
        "# 特徴量1と2が排他的であるように調整\n",
        "for i in range(len(data['feature_1'])):\n",
        "    if data['feature_1'][i] == 1:\n",
        "        data['feature_2'][i] = 0\n",
        "\n",
        "# データフレームに変換\n",
        "original_df = pd.DataFrame(data)\n",
        "\n",
        "# EFBを適用して単一の特徴量に束ねる\n",
        "original_df['bundled_feature'] = original_df['feature_1'] + original_df['feature_2']\n",
        "\n",
        "print(\"オリジナルデータ:\\n\", original_df)\n",
        "\n",
        "# 出力の説明\n",
        "# 'feature_1'と'feature_2'は相互排他的であるため、\n",
        "# 'bundled_feature'に和を取ることで情報損失なく統合されている。\n",
        "# これにより特徴量空間の次元が削減されている。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b392bdf",
      "metadata": {
        "id": "8b392bdf"
      },
      "source": [
        "Early Stoppingは、モデルのトレーニング過程において、過学習を防止するためのテクニックです。通常、機械学習モデルのトレーニングを行う際、トレーニングデータに対する誤差はエポック数が増えるごとに減少していきますが、検証データに対する誤差はある時点で増加に転じることがあります。この兆候はモデルがトレーニングデータに過剰に適応し、汎化能力が低下していることを示しています。\n",
        "\n",
        "具体的には、定義された回数（N回）にわたり検証データの誤差が改善されない場合にトレーニングを打ち切ります。例えば、あるエポックにおいて検証用の損失関数を\\( L \\)とし、最善の検証損失を\\( L_{best} \\)とすると、\\( L < L_{best} \\)ならば、\\( L_{best} = L \\)と更新し、改善が見られないエポック数をリセットします。それ以外の場合は改善がないエポック数をカウントし、定義された回数\\( N \\)に達した時点でトレーニングを終了します。\n",
        "\n",
        "LightGBMはこのEarly Stoppingの機能を標準でサポートしており、ハイパーパラメータとして\\( \\text{early\\_stopping\\_rounds} \\)を設定することで自動的にこの機能を適用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dab65a",
      "metadata": {
        "id": "40dab65a"
      },
      "outputs": [],
      "source": [
        "# Early Stoppingの機能を使用し、学習曲線を可視化するPythonコードの例です。\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットをロード\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# トレーニングデータと検証データに分割\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMデータセットの作成\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
        "\n",
        "# パラメータ設定\n",
        "params = {\n",
        "    'objective': 'binary',  # 二値分類問題の設定\n",
        "    'metric': 'binary_logloss',  # 評価指標\n",
        "    'verbose': -1  # ログ出力をオフ\n",
        "}\n",
        "\n",
        "# Early Stoppingの設定\n",
        "early_stopping_rounds = 10\n",
        "\n",
        "# モデルのトレーニング\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, valid_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=1000,\n",
        "    early_stopping_rounds=early_stopping_rounds  # Early Stoppingの指定\n",
        ")\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(model.evals_result_['train']['binary_logloss'], label='Train')\n",
        "plt.plot(model.evals_result_['valid']['binary_logloss'], label='Valid')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.title('Training with Early Stopping')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()  # Early Stoppingの結果、検証データの損失が減少しなくなったら学習を停止"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc25d0b0",
      "metadata": {
        "id": "cc25d0b0"
      },
      "source": [
        "Learning Rate Decay（学習率減衰）とは、機械学習モデルのトレーニングにおいて、エポックが進むにつれて学習率を徐々に減少させる手法のことです。これにより、初期段階で速い収束を実現しつつ、後半では学習率を減らすことで損失関数の微細な最小化が可能になります。LightGBMでもこの手法が利用されており、learning_rate_decayを設定することでエポックごとに学習率を減らすことができます。\n",
        "\n",
        "理論的には、学習率αを変化させることで、勾配降下法の更新式\n",
        "\n",
        "$$ \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t) $$\n",
        "\n",
        "における学習率αを、時間変数（エポック数）に応じた関数として定義します。例えば、指数関数的に減衰させる場合には\n",
        "\n",
        "$$ \\alpha_t = \\alpha_0 \\times \\gamma^t $$\n",
        "\n",
        "という形で学習率を設定します。この場合、\\(\\alpha_0\\) は初期学習率、\\(\\gamma\\) は減衰率、\\(t\\) はエポックを表します。\n",
        "\n",
        "学習率減衰の使用用途としては、訓練の初期段階で大きくステップを取り、探査を行うことでパラメータ空間を広く探索し、その後、ステップを小さくすることで収束へと導く際に使われます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6decad",
      "metadata": {
        "id": "0f6decad"
      },
      "outputs": [],
      "source": [
        "# PythonでのLearning Rate Decayの実装例\n",
        "# このコードでは、指数関数的な学習率減衰を実装します。\n",
        "\n",
        "# 初期学習率\n",
        "initial_learning_rate = 0.1\n",
        "\n",
        "# 減衰率（0 < decay_rate < 1）\n",
        "decay_rate = 0.96\n",
        "\n",
        "# 学習エポック数\n",
        "epochs = 10\n",
        "\n",
        "# 各エポックにおける学習率の計算\n",
        "learning_rates = []\n",
        "for epoch in range(epochs):\n",
        "    # 学習率を計算\n",
        "    current_learning_rate = initial_learning_rate * (decay_rate ** epoch)\n",
        "    learning_rates.append(current_learning_rate)\n",
        "    # ここでcurrent_learning_rateを用いてモデルの学習を実施\n",
        "    # model.fit(X, y, learning_rate=current_learning_rate)\n",
        "\n",
        "# 学習率の推移を表示\n",
        "print(\"Learning rates over epochs:\", learning_rates)\n",
        "\n",
        "# このコードでは、初期学習率が0.1で、徐々に減少します。\n",
        "# 例えば、3エポック目の学習率は 0.1 * 0.96^3 になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc715bbc",
      "metadata": {
        "id": "dc715bbc"
      },
      "source": [
        "### L1正則化（Lasso）について\n",
        "\n",
        "L1正則化は、モデルの重み（係数）に対してL1ノルム（絶対値の総和）を罰則として加える方法です。L1正則化を用いることで、不必要な特徴量の重みをゼロにし、モデルのスパース性（疎性）を引き出すことができます。数式としては、損失関数に$\\lambda\\sum_{i=1}^{n}|w_i|$を加えます。\n",
        "\n",
        "L1正則化の利点は、特徴選択として機能し、モデルの解釈性を向上させることです。また、高次元データセットでも有効に機能するため、特徴量が多い場合に適しています。ただし、計算コストが高くなる可能性があるという欠点もあります。\n",
        "\n",
        "$$\n",
        "\text{L1正則化を用いた最小化問題} = \text{損失関数} + \\lambda \\sum_{i=1}^{n} |w_i|\n",
        "$$\n",
        "\n",
        "### L2正則化（Ridge）について\n",
        "\n",
        "L2正則化は、モデルの重み（係数）に対してL2ノルム（二乗の総和）を罰則として加える方法です。数式としては、損失関数に$\\lambda\\sum_{i=1}^{n}w_i^2$を加えます。\n",
        "\n",
        "L2正則化は全ての重みを小さく均等化する傾向があり、過学習を防ぐ効果があります。特に、多重共線性の問題を緩和するため、特徴量間の相関が高い場合に有効です。しかし、全ての特徴量に対して重みをゼロにすることはないため、モデルのスパース性は低くなります。\n",
        "\n",
        "$$\n",
        "\text{L2正則化を用いた最小化問題} = \text{損失関数} + \\lambda \\sum_{i=1}^{n} w_i^2\n",
        "$$\n",
        "\n",
        "### LightGBMにおけるL1/L2正則化\n",
        "\n",
        "LightGBMは勾配ブースティングフレームワークの一つであり、高速に計算できることが特徴です。L1/L2正則化はLightGBMにおいて`lambda_l1`および`lambda_l2`のハイパーパラメータで指定できます。これにより、過学習を防ぎながらモデルの汎化性能を高めることが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a221705a",
      "metadata": {
        "id": "a221705a"
      },
      "outputs": [],
      "source": [
        "# L1/L2正則化を理解するためのPythonコード例\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ランダムなデータの生成\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 10)  # 特徴量10つを100件分\n",
        "true_coef = np.array([1.5, -2, 0, 0, 2.5, 0, 1.2, 0, 0, 0])  # 真の係数の設定\n",
        "noise = np.random.normal(size=100)  # ノイズの追加\n",
        "y = X.dot(true_coef) + noise\n",
        "\n",
        "# データの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# L1正則化（Lasso）を用いた回帰モデルの構築\n",
        "lasso = Lasso(alpha=0.1)  # alphaが正則化の強さに相当\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Lassoの予測結果と重み\n",
        "lasso_pred = lasso.predict(X_test)\n",
        "lasso_coef = lasso.coef_\n",
        "print('Lasso 重み:', lasso_coef)\n",
        "print('Lasso MSE:', mean_squared_error(y_test, lasso_pred))\n",
        "\n",
        "# L2正則化（Ridge）を用いた回帰モデルの構築\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Ridgeの予測結果と重み\n",
        "ridge_pred = ridge.predict(X_test)\n",
        "ridge_coef = ridge.coef_\n",
        "print('Ridge 重み:', ridge_coef)\n",
        "print('Ridge MSE:', mean_squared_error(y_test, ridge_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b178a3",
      "metadata": {
        "id": "35b178a3"
      },
      "source": [
        "SHAP値（SHapley Additive exPlanations）は、モデルの予測を個々の特徴の貢献度に分解するための手法です。この手法は、ゲーム理論のシャープレイ値（Shapley value）に基づいています。Shapley値は、協力ゲームにおける各参加者の貢献度を公平に測定する方法として提案されたものであり、MLモデルで特徴量の寄与度を計算する際に理論的裏付けがあります。\\n\\nインライン数式によると、特徴量$i$のShapley値$\\phi_i$は以下で計算されます。\\n$\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S \\cup \\{i\\}) - v(S))$\\n\\nブロック数式では、\\n\\n$$\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S \\cup \\{i\\}) - v(S))$$\\n\\nとなります。ここで、$N$は全ての特徴の集合、$S$は部分集合、$v(S)$は部分集合$S$を含む場合のモデルの予測です。\\n\\nLightGBMとSHAPの関係は、LightGBMがツリーベースのモデルであり、SHAPを用いてツリー構造を持つモデルから複雑な依存関係を解析するのに有効である点です。\\n\\nSHAPを使うことで、個々のデータポイントにおける特徴量の影響を可視化し、モデルの可説明性を高めることができます。これは特にビジネス上の重要な意思決定で合意を得るためや、モデルの公平性を検証する際に役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d7f4cc",
      "metadata": {
        "id": "25d7f4cc"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "\n",
        "# データを作成します\n",
        "data = pd.DataFrame({'feature1': np.random.rand(100), 'feature2': np.random.rand(100), 'feature3': np.random.rand(100)})\n",
        "labels = np.random.randint(0, 2, 100)  # バイナリラベルを生成します\n",
        "\n",
        "# LightGBMのデータセットを作成します\n",
        "dataset = lgb.Dataset(data, label=labels)\n",
        "\n",
        "# モデルのハイパーパラメータを設定します\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss'\n",
        "}\n",
        "\n",
        "# モデルをトレーニングします\n",
        "model = lgb.train(params, dataset, num_boost_round=10)\n",
        "\n",
        "# 予測を計算します\n",
        "preds = model.predict(data)\n",
        "\n",
        "# SHAP値を計算します\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(data)\n",
        "\n",
        "# SHAP値の要約プロットを表示します\n",
        "shap.summary_plot(shap_values, data, plot_type=\"bar\")\n",
        "\n",
        "# 注:\n",
        "# このコードはLightGBMモデルにおける各特徴量のSHAP値を計算し、\n",
        "# 特徴量の重要度を可視化するための要約プロットを表示します。\n",
        "# shap.summary_plot関数を使用して、どの特徴量が予測に最も影響を与えているかを視覚的に確認します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878f694b",
      "metadata": {
        "id": "878f694b"
      },
      "source": [
        "### Permutation Importanceの解説\n",
        "\n",
        "Permutation Importance（置換による重要度）は、特徴量の重要性を評価する方法の一つです。この方法は、モデルがどの程度特定の特徴量に依存しているかを測定するために使用されます。\n",
        "\n",
        "理論的には、Permutation Importanceは以下のように計算されます。\n",
        "1. 学習済みモデルを用いて、トレーニングデータまたはテストデータに対するベースラインの性能（例：精度やRMSEなど）を評価します。\n",
        "2. 調査する特徴量の値をシャッフルすることで、その特徴量の情報を破壊します。\n",
        "3. シャッフルしたデータを用いて再度モデルの性能を評価します。\n",
        "4. ベースラインの性能とシャッフル後の性能との差を計算します。この差が大きいほど、モデルはその特徴量に依存していると言えます。\n",
        "\n",
        "数式としては、ある特徴量\\( X_j \\)の重要度は次のように定義されます：\n",
        "\n",
        "- インライン数式: $ Importance(X_j) = Performance_{baseline} - Performance_{shuffled}(X_j) $\n",
        "\n",
        "- ブロック数式:\n",
        "$$\n",
        "Importance(X_j) = Performance_{baseline} - Performance_{shuffled}(X_j)\n",
        "$$\n",
        "\n",
        "この手法はモデルとは独立しているため、**LightGBM**のような勾配ブースティングモデルにも適用することが可能です。\n",
        "\n",
        "#### 使用用途\n",
        "Permutation Importanceは以下のような場合に利用されます：\n",
        "- 特徴量の選択:\n",
        "  重要でない特徴量を削除することで、よりシンプルで解釈しやすいモデルを作成します。\n",
        "- モデルの解釈:\n",
        "  モデルの予測に寄与する特徴量を理解することで、ビジネス上の意思決定をサポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d52126",
      "metadata": {
        "id": "c0d52126"
      },
      "outputs": [],
      "source": [
        "# Permutation Importanceを計算するPythonコード例\n",
        "\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# データセットの読み込み\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# データセットの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMモデルの訓練\n",
        "dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error',\n",
        "    'verbose': -1\n",
        "}\n",
        "model = lgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# 基本的なモデルの性能を計算\n",
        "baseline_preds = model.predict(X_test)\n",
        "baseline_preds = np.round(baseline_preds)  # 予測を0/1に変換\n",
        "baseline_performance = accuracy_score(y_test, baseline_preds)\n",
        "\n",
        "# Permutation Importanceを計算する関数\n",
        "def calculate_permutation_importance(model, X_test, y_test, baseline_performance, n_repeats=5):\n",
        "    feature_importances = np.zeros(X_test.shape[1])\n",
        "\n",
        "    for col in range(X_test.shape[1]):\n",
        "        permuted_performance = []\n",
        "        for _ in range(n_repeats):\n",
        "            # 特徴量をシャッフル\n",
        "            X_permuted = X_test.copy()\n",
        "            X_permuted[:, col] = shuffle(X_permuted[:, col], random_state=42)\n",
        "\n",
        "            # シャッフル後の性能を計算\n",
        "            permuted_preds = model.predict(X_permuted)\n",
        "            permuted_preds = np.round(permuted_preds)  # 予測を0/1に変換\n",
        "            performance = accuracy_score(y_test, permuted_preds)\n",
        "            permuted_performance.append(performance)\n",
        "\n",
        "        # 平均をとって重要度を計算\n",
        "        mean_permuted_performance = np.mean(permuted_performance)\n",
        "        feature_importances[col] = baseline_performance - mean_permuted_performance\n",
        "\n",
        "    return feature_importances\n",
        "\n",
        "# Permutation Importanceの計算と表示\n",
        "feature_importances = calculate_permutation_importance(model, X_test, y_test, baseline_performance)\n",
        "print('Permutation Importances:', feature_importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c3de3a",
      "metadata": {
        "id": "c4c3de3a"
      },
      "source": [
        "### 決定木の構築について\n",
        "\n",
        "決定木は、データを分類または回帰するための非線形の予測モデルとして広く使われています。決定木は木構造を持ち、各内部ノードはデータセット内の特徴に関連し、エッジは特徴の値に関連します。葉ノードはラベルまたは予測値を持ちます。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "決定木の構築は、データセットを再帰的に二分することで行われます。分割の選択は通常、情報利得、ジニ不純度、または分散減少といった基準を最大化または最小化することで行います。\n",
        "\n",
        "- **情報利得**: エントロピーの減少量を表し、情報理論に基づく分割基準です。$$\\text{情報利得} = \\text{エントロピー}（親）- \\sum_{i}\\frac{|子_i|}{|親|}\\times\\text{エントロピー}（子_i）$$\n",
        "\n",
        "- **ジニ不純度**: この基準は、ノード内でランダムに選ばれた2つのデータポイントが異なるクラスに属する確率を最小化します。$$\\text{ジニ不純度} = 1 - \\sum_{k} (p_k)^2$$\n",
        "\n",
        "- **分散減少**: 回帰問題で使われる分割基準で、ノード内のターゲットの分散を減らす方向に分割を行います。\n",
        "\n",
        "#### LightGBMとの関係\n",
        "LightGBM（Light Gradient Boosting Machine）は、勾配ブースティングの一つであり、決定木を逐次的に構築して高精度の予測モデルを作成します。LightGBMは、学習時に高速性と高効率を実現するために、特に多くの特徴量とデータを持つデータセットに対して効果的です。\n",
        "\n",
        "#### 使用用途\n",
        "決定木は、分類（例: スパムメールフィルタリング）、回帰（例: 家の価格予測）、特徴選択、データの可視化、データ構造の理解など、さまざまな用途で利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098219c4",
      "metadata": {
        "id": "098219c4"
      },
      "outputs": [],
      "source": [
        "# 決定木を用いた分類問題の例をPythonで示します\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Irisデータセットをロード\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 決定木分類器を初期化\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "\n",
        "# モデルを訓練\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで精度を評価\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# 決定木をプロット\n",
        "plt.figure(figsize=(20,10))\n",
        "_ = tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.show()\n",
        "\n",
        "# このコードは、Irisデータセットを使用して決定木をトレーニングし、\n",
        "# テストセットに対する精度を評価します。\n",
        "# また、決定木の構造を可視化して表示します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f27840",
      "metadata": {
        "id": "91f27840"
      },
      "source": [
        "### インラインおよびブロック数式とブートストラップサンプリングの解説\n",
        "\n",
        "ブートストラップサンプリングは、統計学の技法で、推定の不確実性を測るために使用されます。主にモデルの精度を評価するために広く使用されています。具体的には、元のデータセットからランダムにサンプルを抽出して、新しいデータセットを生成します。この手法は、以下のプロセスを含みます。\n",
        "\n",
        "1. **元のデータセットからサンプルを復元抽出する**: 元のデータセットからランダムにサンプリングを行い、新しいサンプルを作成します。サンプル数は元のデータセットのサイズと同じです。\n",
        "2. **データセットの再構築**: 上記の手順を複数回繰り返し、各回で異なるサンプルを収集します。\n",
        "\n",
        "数式で表すと、データセット $ X = \\{x_1, x_2, \\dots, x_n\\} $ に対して、$ X^* $ というブートストラップサンプルを $ T $ 回生成する場合、それぞれの $ X^*_t \\quad (t = 1, 2, \\dots, T) $ は $ n $ 個の要素を持ち、それぞれがデータセット $ X $ からのランダムな復元抽出によって得られます。\n",
        "\n",
        "$$ X^*_t = \\{x^*_{t,1}, x^*_{t,2}, \\dots, x^*_{t,n}\\} \\quad \\text{where } x^*_{t,i} \\sim X $$\n",
        "\n",
        "LightGBMにおいて、ブートストラップサンプリングはバギングの一部として活用されます。この手法により、LightGBMはデータの多様性を活かし、モデルが過学習するのを防ぎつつ、精度を高めることができます。\n",
        "\n",
        "### 使用用途\n",
        "- **モデルの精度向上**: 複数のモデルでアンサンブル学習を行う際に、データセットの多様性を提供します。\n",
        "- **過学習防止**: データセットのランダム性を利用してモデルの汎化能力を増加させます。\n",
        "- **統計的推論**: 各サンプルに基づく推定値の平均や分散を計算し、信頼区間の構築などに活用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4efe7c52",
      "metadata": {
        "id": "4efe7c52"
      },
      "outputs": [],
      "source": [
        "# Pythonを用いたブートストラップサンプリング実装の例\n",
        "import numpy as np\n",
        "\n",
        "# 元のデータセット\n",
        "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# ブートストラップサンプルを生成する関数\n",
        "# sample_size: 各ブートストラップサンプル内のデータの個数\n",
        "def bootstrap_sampling(data, n_samples, sample_size):\n",
        "    bootstrap_samples = []  # ブートストラップサンプルを格納するリスト\n",
        "    for _ in range(n_samples):\n",
        "        # np.random.choiceを用いてデータを復元抽出し、サンプルを作成\n",
        "        sample = np.random.choice(data, size=sample_size, replace=True)\n",
        "        bootstrap_samples.append(sample)\n",
        "    return bootstrap_samples\n",
        "\n",
        "# ブートストラップサンプルを10個生成し、それぞれに元データセットのサイズと同じ数のデータを含む\n",
        "n_samples = 10\n",
        "sample_size = len(original_data)\n",
        "bootstrap_samples = bootstrap_sampling(original_data, n_samples, sample_size)\n",
        "\n",
        "# 結果の表示\n",
        "# 各サンプルを表示し、ブートストラップ法が元のデータからどのようにサンプリングするかを確認\n",
        "for i, sample in enumerate(bootstrap_samples):\n",
        "    print(f\"Sample {i+1}: {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5a8813",
      "metadata": {
        "id": "4f5a8813"
      },
      "source": [
        "### ランダムフォレストの解説\n",
        "\n",
        "ランダムフォレストは、複数の決定木を用いて分類および回帰を行うアンサンブル学習の手法の一つです。Breimanによって提案され、バギングとランダムな特徴選択を組み合わせることで汎化性能を向上させています。各決定木は、訓練データのランダムサブセットを用いて構築されます。\n",
        "\n",
        " インライン数式として、各決定木が予測するクラスを$\\hat{Y}_i(x)$とすると、ランダムフォレストの最終予測$\\hat{Y}(x)$は多数決により与えられます。\n",
        "\n",
        " - バギング：ブートストラップ法でデータをランダムにサンプリングし、それぞれのサンプルで決定木を構築します。\n",
        "\n",
        "- ランダムな特徴選択：決定木の各分岐で使用する特徴をランダムに選びます。これにより、決定木間の相関を減らし、多様性を生むことができます。\n",
        "\n",
        "#### 数式：\n",
        "\n",
        "ランダムフォレストの予測は以下の式で表されます：\n",
        "$$\n",
        "\\hat{Y}(x) = \\text{majority\\_vote}(\\hat{Y}_1(x), \\hat{Y}_2(x), \\ldots, \\hat{Y}_B(x))\n",
        "$$\n",
        "ここで、$B$は決定木の数です。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "ランダムフォレストは、特に次のような場合に適しています。\n",
        "- 高次元の非線形データ\n",
        "- クラス間の分布が不均衡な場合\n",
        "- 欠損値が多いデータ\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBMは決定木ベースのアルゴリズムとして、ランダムフォレストとは異なる勾配ブースティングを用いています。勾配ブースティングは決定木を逐次構築して誤差を最小化し、通常、性能面でランダムフォレストより優れています。ただし、パラメータのチューニングが比較的難しいです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f6f5ce",
      "metadata": {
        "id": "a4f6f5ce"
      },
      "outputs": [],
      "source": [
        "# ランダムフォレストを使った分類問題の解法\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Irisデータセットをロード\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルを初期化\n",
        "# n_estimatorsは決定木の数を意味します。\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# モデルを訓練データで訓練\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで予測\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# 精度を表示\n",
        "equals = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {equals:.2f}\")\n",
        "# 簡単な分類性能の評価を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d2357b",
      "metadata": {
        "id": "c2d2357b"
      },
      "source": [
        "アンサンブル学習は、複数のモデルを組み合わせてより高い予測性能を実現する機械学習手法です。アンサンブル手法には、バギング、ブースティング、スタッキングなどがあります。これらの手法は、異なるモデルの長所を組み合わせることで、個々のモデルよりも優れた結果を得ることができます。\\\n",
        "特にLightGBMは、勾配ブースティングフレームワークの一種であり、大規模データセットに対する高速なトレーニングと高い精度を提供します。\\\n",
        "数式的な観点から見ると、アンサンブル学習において最も一般的なアイデアは、複数の基礎モデルの予測結果を平均化または加重平均することです。例えば、\\\n",
        "インライン数式: $f(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$ はN個のモデルの予測を平均化したものです。\\\n",
        "ブロック数式:\n",
        "$$f(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$$\n",
        "\n",
        "アンサンブル学習の使用用途としては、分類、回帰など幅広い問題に対応できる点が挙げられます。また、モデルの精度を向上させ、過学習を防ぐためにも活用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065db8ce",
      "metadata": {
        "id": "065db8ce"
      },
      "outputs": [],
      "source": [
        "# ライブラリをインポート\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データセットを読み込む\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 訓練データとテストデータを分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMモデルをインスタンス化\n",
        "model1 = LGBMClassifier(boosting_type='gbdt', objective='multiclass', random_state=42)\n",
        "model2 = LGBMClassifier(boosting_type='dart', objective='multiclass', random_state=42)\n",
        "\n",
        "# アンサンブルモデルを構築（バギング手法の一例）\n",
        "voting_clf = VotingClassifier(estimators=[('gbdt', model1), ('dart', model2)], voting='soft')\n",
        "\n",
        "# モデルをトレーニング\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 予測を行う\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# 精度を計算\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 結果を出力\n",
        "print(f'アンサンブルモデルの精度: {accuracy:.2f}')\n",
        "\n",
        "# 上記のコードは、LightGBMを使ってバギングの方法でアンサンブル学習を行っています。\n",
        "# VotingClassifierを使い、異なるLightGBMモデルの予測を組み合わせて精度を向上させています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bacd31e",
      "metadata": {
        "id": "1bacd31e"
      },
      "source": [
        "交差検証（Cross Validation）は、モデルの評価に使用される強力な技法であり、特に過学習を防ぐのに役立ちます。データセットを複数の部分（フォールド）に分割し、そのうち一部をテストセットとして、残りをトレーニングセットとして使用します。このプロセスを何度も繰り返し、各フォールドが一度はテストセットとして使用されるようにします。\n",
        "\n",
        "**主な手法**:\n",
        "- **K-Fold Cross Validation**: データをK個のフォールドに均等に分け、そのうちの1つをテストセットにして、残りをトレーニングセットにします。これをK回繰り返し、すべてのフォールドが一度はテストセットとして使われるようにします。\n",
        "\n",
        "数式で言うと、K-Fold Cross Validationにおける平均評価指標(例えば、精度やRMSE)は次のように定義されます：\n",
        "\n",
        "インライン: $ CV_{mean} = \\frac{1}{K} \\sum_{i=1}^{K} CV_{i} $\n",
        "\n",
        "ブロック:\n",
        "$$\n",
        " CV_{mean} = \\frac{1}{K} \\sum_{i=1}^{K} CV_{i}\n",
        "$$\n",
        "\n",
        "- **LightGBMとの関係性**: LightGBMは学習を迅速に行うことができるため、大規模データセットでも交差検証を効率的に実行できます。交差検証を用いることで、適切なハイパーパラメータチューニングが可能になり、モデルの汎化性能を高めることができます。\n",
        "\n",
        "**使用用途**:\n",
        "- モデルの評価: 交差検証を用いて、モデルの信頼性と汎化性能を評価。\n",
        "- パラメータチューニング: ハイパーパラメータの最適化により、モデルの精度を向上。\n",
        "- 特徴選択: より良い特徴量を探索するための基準として使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e500bab5",
      "metadata": {
        "id": "e500bab5"
      },
      "outputs": [],
      "source": [
        "# PythonでK-Fold Cross Validationを実施する例\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Irisデータセットを読み込む\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# モデルのインスタンスを作成（例としてRandom Forestを使用）\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# KFoldクロスバリデーションを設定（ここではK=5を指定）\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "# 各フォールドごとの結果を保存するリスト\n",
        "scores = []\n",
        "\n",
        "# データをK個のフォールドに分割\n",
        "for train_index, test_index in kf.split(X):\n",
        "    # トレーニングデータとテストデータにスプリット\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # モデルをトレーニング\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # テストデータで予測\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # 精度を計算し保存\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "# 各フォールドの精度とその平均を表示\n",
        "print(\"Scores for each fold:\", scores)\n",
        "print(\"Mean accuracy:\", np.mean(scores))\n",
        "\n",
        "# このコードは、Irisデータセットに対して5フォールドのクロスバリデーションを実行し、\n",
        "# 各フォールドの精度と平均精度を計算するものです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff01e87",
      "metadata": {
        "id": "6ff01e87"
      },
      "source": [
        "### ハイパーパラメータチューニング (GridSearch)\n",
        "\n",
        "ハイパーパラメータチューニングは、機械学習モデルのパフォーマンスを最適化するためにモデルのハイパーパラメータを調整するプロセスです。これによりモデルの汎化能力を最大化し、テストデータ上での予測精度を向上させることができます。\n",
        "\n",
        "**GridSearch**は、指定したハイパーパラメータの範囲の中からすべての組み合わせを試して最適な組み合わせを見つける方法です。たとえば、学習率と決定木の最大深度という2つのハイパーパラメータがあるとしましょう。それぞれについて複数の候補値を設定し、それらの積の数だけモデルを学習させます。\n",
        "\n",
        "$$(\\text{optimal parameters} = \\arg\\max_{\\text{params}} \\text{Score(params)})$$\n",
        "\n",
        "GridSearchは計算コストが高くなることがあります。なぜなら、組み合わせが多いとそのすべてを試す必要があるからです。しかし、最適なハイパーパラメータを見つける手法としてはシンプルで効果的です。\n",
        "\n",
        "LightGBMのような勾配ブースティングライブラリでもGridSearchを用いることができます。LightGBMでは特に、多くのハイパーパラメータが存在し、モデルの性能に影響を与えるため、精度の高いチューニングが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0db2bf6",
      "metadata": {
        "id": "e0db2bf6"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Irisデータセットの読み込み\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ハイパーパラメータの候補を設定\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# LightGBMの分類器を初期化\n",
        "lgbm = LGBMClassifier(random_state=42)\n",
        "\n",
        "# GridSearchCVで最適なハイパーパラメータを探索\n",
        "# GridSearchCVの初期化\n",
        "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# グリッドサーチの実行\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 最適なハイパーパラメータとそのスコアを出力\n",
        "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best Accuracy: \", grid_search.best_score_)\n",
        "\n",
        "# PythonコードのこのセクションはGridSearchをLightGBMモデルで使用する方法を例示しています。\n",
        "# ここでは、学習率、決定木の深さ、及び決定木の数（n_estimators）をハイパーパラメータとして指定しています。\n",
        "# このコードはsklearnのGridSearchCVを活用して、それらのハイパーパラメータの組み合わせを探索し、\n",
        "# 最高のパフォーマンスを得るための最適な組み合わせを見つけます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c3f74a",
      "metadata": {
        "id": "23c3f74a"
      },
      "source": [
        "### バギング (Bagging)について\n",
        "\n",
        "バギング（Bootstrap Aggregating）はアンサンブル学習の手法の一つであり、特にランダムフォレストなどでよく使用される手法です。バギングは、元の学習データセットから複数のサブセットをランダムに生成し、それぞれのサブセットで学習したモデルを組み合わせることで、予測性能を向上させます。\n",
        "\n",
        "**理論的背景**:\n",
        "バギングは、元のデータセットに対してブートストラップサンプリング（復元抽出）を行った複数のデータサンプルを用いて、複数のモデルを作成します。それぞれのモデルが出力する予測結果を統合し（平均や多数決を取るなど）、最終的な予測を行います。これにより、過学習を抑えつつ、モデル全体のバリアンスを減らすことができます。\n",
        "\n",
        "インライン数式で表現すると、各モデルの予測を $\\hat{f}_i(x)$ とし、最終予測を $\\hat{f}(x)$ とした場合、それぞれの予測を平均する場合の最終予測は以下のように表現されます：\n",
        "\n",
        "$\\hat{f}(x) = \\frac{1}{B} \\sum_{i=1}^{B} \\hat{f}_i(x)$\n",
        "\n",
        "ここで、$B$ は使用するモデルの数です。\n",
        "\n",
        "**LightGBMとの関係性**:\n",
        "LightGBM（Light Gradient Boosting Machine）では、主に勾配ブースティングによる決定木アンサンブル手法を採用していますが、バギングを使用することでさらなるモデルの改善を図ることが可能です。LightGBMにおいては、バギングフラクションおよびバギング頻度といったパラメータを調整することで、バギング手法を取り入れることができます。\n",
        "\n",
        "**使用用途**:\n",
        "バギングは特にデータにバリアンスが高い場合に有効で、例えば小規模データセットやノイズが多いデータセットに対するアプローチとして有効です。ランダムフォレストのように、決定木をベースとしたモデルで一般的に使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3cb0e47",
      "metadata": {
        "id": "e3cb0e47"
      },
      "outputs": [],
      "source": [
        "# バギングを実際にPythonで実装してみる\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Irisデータセットの読み込み\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# データセットを訓練データとテストデータに分割する\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# バギングによる分類器の作成\n",
        "# 基礎モデルとして決定木を使用する\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "bagging_model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "# モデルの学習\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# テストデータでの予測\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# 正答率の計算\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# 各ステップが何をしているのかについてのコメント\n",
        "# 1. データセットを読み込み、トレーニングとテストに分割\n",
        "# 2. バギングモデルを作成するため、基礎モデル（この場合は決定木）を選択\n",
        "# 3. バギング分類器を初期化し、基礎モデルを複数用いて学習\n",
        "# 4. テストデータに対して予測を実施\n",
        "# 5. 正答率を計算して結果を出力\n",
        "\n",
        "# バギングによって、精度の向上と過剰適応（過学習）の抑制を実現することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d51244",
      "metadata": {
        "id": "95d51244"
      },
      "source": [
        "### ブースティング (Boosting) の解説\n",
        "\n",
        "ブースティングは、**アンサンブル学習**の一種であり、複数の弱学習器（通常は決定木）を逐次的に組み合わせて、より強力な学習器を作る手法です。各弱学習器は、前の学習器が作った誤差を補完するように訓練されます。オーバーフィッティングを抑えつつ精度を上げることが得意です。\n",
        "\n",
        "**理論**:\n",
        "ブースティングの基本的な考え方は、単一のモデルでは予測が難しい場合でも、複数のモデルの予測を組み合わせることで精度を上げるというものです。アダブースト（AdaBoost）や勾配ブースティング（Gradient Boosting）が代表的なアルゴリズムです。\n",
        "\n",
        "例えば、アダブーストでは、各データポイントに重みをつけ、その重みを調整しながら逐次的に学習を進めます。各イテレーションでデータの重みが更新され、誤分類されたデータの重みが増すことで、次のイテレーションでこれらのデータに対する学習が強化されます。\n",
        "- 各モデルの更新は次式に依存します：\n",
        "  $$\\alpha_m = \\log\\left(\\frac{1 - \\text{error}}{\\text{error}}\\right)$$\n",
        "  ここで、\\(\\alpha_m\\)はモデル\\(m\\)の重み、\\(\\text{error}\\)はそのモデルの誤分類率を表します。\n",
        "\n",
        "**LightGBMとの関係性**:\n",
        "LightGBM は、勾配ブースティングの一種である**Gradient Boosting Decision Tree (GBDT)**の実装です。LightGBMは、学習速度とスケーラビリティを大幅に改善するために、**リーフワイズの木の成長**および**ヒストグラムベースの学習**を採用しています。\n",
        "\n",
        "**使用用途**:\n",
        "ブースティングは多数のフィーチャや複雑なデータセットの分類や回帰に非常に効果的です。特に、\n",
        "- テーブルデータの分類\n",
        "- 回帰分析\n",
        "- ランキングタスク\n",
        "など幅広い機械学習タスクで使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64f6eda",
      "metadata": {
        "id": "f64f6eda"
      },
      "outputs": [],
      "source": [
        "# ライブラリのインポート\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# データのロードと分割\n",
        "boston = load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# データセットの生成\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'boosting_type': 'gbdt',  # 勾配ブースティング決定木\n",
        "    'metric': 'rmse',         # 損失の指標\n",
        "}\n",
        "\n",
        "# モデルのトレーニング\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,  # ブースティングラウンドの数\n",
        "    valid_sets=test_data,\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# テストデータでの予測\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# MSEを計算して出力\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# 各ステップについてのコメント\n",
        "# 1. 必要なライブラリをインポート。\n",
        "# 2. データセットをロードし、訓練データとテストデータに分けます。\n",
        "# 3. LightGBM用のデータセットを生成します。\n",
        "# 4. モデルの設定を行い、損失関数やブースティングタイプを指定します。\n",
        "# 5. モデルの学習を行い、学習過程での早期終了を設定します。\n",
        "# 6. 学習したモデルを用いて、テストデータで予測を行います。\n",
        "# 7. 予測結果と実際の値からMSEを求めます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e49ffd",
      "metadata": {
        "id": "c8e49ffd"
      },
      "source": [
        "Log Loss（対数損失関数）は、分類モデルの性能を評価するためによく使われる損失関数です。特に、バイナリ分類（0または1）のケースで頻繁に使用されます。モデルが予測した確率と実際のラベルとの間の誤差を数値化し、分類器のパフォーマンスを評価します。LightGBMは、勾配ブースティング決定木（GBDT）をベースにしており、このLog Lossを目的関数として使用し、モデルの最適化を図ります。\n",
        "\n",
        " **数式:**\n",
        "\n",
        "  Log Loss の数式は次の通りです：\n",
        "\n",
        " \\[ \\text{Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right] \\]\n",
        "\n",
        "  ここで、\\( n \\) はサンプル数、\\( y_i \\) は実際のラベル（0または1）、\\( \\hat{y}_i \\) は予測された確率値です。\n",
        "\n",
        " **使用用途:**\n",
        "\n",
        " - モデルのパフォーマンスを数値で評価したいとき。\n",
        " - 特に分類問題において、モデルが出力する確率の正確さを測定する場面で利用されます。\n",
        " - モデルの柔軟性を評価し、異なるモデルやハイパーパラメータの選択に対して客観的な基準を提供します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2dd95a",
      "metadata": {
        "id": "2e2dd95a"
      },
      "outputs": [],
      "source": [
        "# Log LossをPythonで計算するためのコード例\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# 実際のラベル（y_true）と予測確率の例（y_pred）\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0])  # 実際のラベル\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2, 0.9, 0.1])  # モデルからの予測確率\n",
        "\n",
        "# Log Loss（対数損失）を計算\n",
        "loss = log_loss(y_true, y_pred)\n",
        "print('Log Loss:', loss)  # 計算されたLog Lossを出力\n",
        "\n",
        "# 上記のコードでは、実際のラベルと予測確率を使って\n",
        "# sklearn.metricsのlog_loss関数を使用し、対数損失を計算します。\n",
        "# log_loss関数は、分類モデルの予測のパフォーマンスを数値で評価するために用いられます。\n",
        "# 出力として、モデルがどの程度の精度でラベルを予測しているかがわかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ccf5bc",
      "metadata": {
        "id": "04ccf5bc"
      },
      "source": [
        "### データの重み付けについて\n",
        "データの重み付けは、モデルの学習において特定のデータポイントに対して異なる重要度を持たせる手法です。LightGBMなどの機械学習モデルにおいてデータの重み付けを行うことで、例えば不均衡データセットにおけるモデルの性能改善を図ることができる。\n",
        "具体的には、損失関数の計算にデータの重み$w_i$を導入し、\n",
        "$$ L = \\sum_{i=1}^{n} w_i \\cdot l(y_i, f(x_i)) $$\n",
        "とすることで、各データポイント$i$の貢献度を調整する。\n",
        "\n",
        "#### 理論と数学的背景\n",
        "重み付けを使用することで、一般に次のようなケースで有効です：\n",
        "1. **不均衡データセット**：クラス間でサンプル数に差がある場合、重み付けにより少数クラスの重要度を高めることができます。\n",
        "\n",
        "2. **コストセンシティブな課題**：異なる誤りに異なるコストがある場面での最適化。\n",
        "\n",
        "損失関数の変形により、特定のデータポイントの影響を自動調整できます。特殊なケースとして等重みのとき、通常の学習と等価になる。\n",
        "\n",
        "3. **アウトライヤーの影響減少**：大きな誤差を持つデータの影響を軽減する。\n",
        "\n",
        "#### LightGBMにおける実装\n",
        "LightGBMでは、各学習サンプルに対して重みを設定することが可能です。この重みは、`weight`パラメータを設定することでモデルの学習プロセスに影響を与えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df3f309",
      "metadata": {
        "id": "4df3f309"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# データセットを生成します\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42)\n",
        "\n",
        "# 訓練とテストにデータを分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# サンプルに対する重みを定義（ここでは単純にクラス0に対して5倍の重みをつける）\n",
        "weights = np.where(y_train == 0, 5, 1)\n",
        "\n",
        "# データセットを作成\n",
        "train_data = lgb.Dataset(X_train, label=y_train, weight=weights)\n",
        "\n",
        "# パラメータ設定\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error',\n",
        "    'is_unbalance': True\n",
        "}\n",
        "\n",
        "# モデルの訓練\n",
        "bst = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# テストデータに対する予測\n",
        "y_pred = bst.predict(X_test)\n",
        "# 0.5を閾値にしてクラスを決定\n",
        "pred_labels = np.rint(y_pred)\n",
        "\n",
        "# 精度を計算し表示\n",
        "accuracy = accuracy_score(y_test, pred_labels)\n",
        "print(f'精度: {accuracy:.2f}')\n",
        "\n",
        "# コメントアウトされたセクションで説明：このコードは、LightGBMを使用してデータの重み付けを実装するプロセスを示す。\n",
        "# データセットを生成し、指定された重みを用いてモデルを訓練し、テストデータでその精度を評価する。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086baedb",
      "metadata": {
        "id": "086baedb"
      },
      "source": [
        "ランダムサンプリングはデータ分析や機械学習において広く使われる手法であり、全データセットからランダムに部分データセットを抽出します。ランダムサンプリングの基本的な考え方は、各データポイントが選ばれる確率が等しいことです。\n",
        "\n",
        "### 理論\n",
        "\n",
        "ランダムサンプリングを数学的に表現すると、$X_i$ をデータセット内のデータポイントとすると、ランダムサンプリングされたサブセット $S$ は次のように定義されます。\n",
        "\n",
        "$$ P(X_i \\in S) =\n",
        "rac{k}{n} $$\n",
        "\n",
        "ここで、$k$ はサンプリングされるデータの数、$n$ は元のデータセットのサイズです。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "\n",
        "LightGBM（Light Gradient Boosting Machine）は勾配ブースティングを使用した機械学習アルゴリズムで、効率的な学習を行うために初期段階のデータサンプリングを行います。LightGBMは非常に大きなデータセットに対してサンプリングを活用し、モデルの学習を高速化します。\n",
        "\n",
        "### 使用用途\n",
        "\n",
        "ランダムサンプリングは、計算資源が限られている場合や、全データを使用することが困難な場合にモデルの性能を評価する際に有効です。また、結果のバイアスを防ぐために異なるデータスプリットに対する一般化能力を検証したい場合にも使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a72ca5",
      "metadata": {
        "id": "29a72ca5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 元のデータセットを生成（0から99までの整数）\n",
        "original_data = np.arange(100)\n",
        "\n",
        "# ランダムサンプリングによるサブセットを抽出\n",
        "# サンプルサイズを設定\n",
        "sample_size = 10\n",
        "\n",
        "# NumPyのrandom.choiceを使用してランダムにサンプリング\n",
        "# replace=False とすることで重複なしでサンプリング\n",
        "sampled_data = np.random.choice(original_data, size=sample_size, replace=False)\n",
        "\n",
        "# 抽出されたサブセットの出力\n",
        "print(\"Sampled data:\", sampled_data)\n",
        "\n",
        "# 上記のコードは、0から99までの整数の中から10個を重複なしでランダムに抜き出す例です。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc350fee",
      "metadata": {
        "id": "fc350fee"
      },
      "source": [
        "SHAP値（Shapley Additive exPlanations）は、機械学習モデルの予測に対する各特徴量の貢献度を定量化するための手法です。理論的にはゲーム理論に基づいており、各特徴量が予測結果にどれだけ寄与しているかを公平に割り当てます。\n",
        "\n",
        "### SHAP値の数式\n",
        "インライン数式として、各特徴量のSHAP値は基礎予測値と各特徴量の寄与の合計として表されます: $f(x) = \\phi_0 + \\sum_{i=1}^{M} \\phi_i$, ここで $\\phi_0$ は基礎予測値、$\\phi_i$ は特徴量 $i$ のSHAP値です。\n",
        "\n",
        "ブロック数式表現では以下のようになります:\n",
        "$$ \\phi_i(x_i) = \\sum_{S \\subseteq N \\{i}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!}(v(S \\cup {i}) - v(S)) $$\n",
        "この式は、特徴量$i$がモデルのパフォーマンスに与えるマージナル貢献をすべての特徴量のサブセットについて平均化したものです。\n",
        "\n",
        "### LightGBMとの関係性\n",
        "LightGBMは決定木ベースの勾配ブースティング手法で、多くの特徴量を扱うことができるため、SHAP値を用いて特徴量の影響を視覚化するのは非常に有効です。SHAP値を用いることで、モデルの解釈可能性を高め、重要な特徴量を容易に特定することができます。\n",
        "\n",
        "### 使用用途\n",
        "- **モデル解釈**: SHAP値を用いることで、モデルの予測結果を詳細に解釈し、特徴量の重要度を評価できます。\n",
        "- **特徴量選択**: 特定の特徴量が予測にどの程度寄与しているかを判断する際に有用です。\n",
        "- **バイアス検出**: 特定の特徴量が不当に影響を与えていないかを確認します。\n",
        "\n",
        "SHAP値の視覚化は、これらの用途を支えるための強力なツールです。一般に、バー、ヒートマップ、決定プロットなどで表現されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5e54be",
      "metadata": {
        "id": "6d5e54be"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします。\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットの生成 (ここでは単純な例としてランダムデータを使用しています)\n",
        "X = np.random.rand(100, 5)  # 説明変数\n",
        "Y = np.random.rand(100)  # 目的変数\n",
        "\n",
        "# データフレームに変換します\n",
        "X_df = pd.DataFrame(X, columns=[f'feat_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "# LightGBMデータセットの作成\n",
        "train_data = lgb.Dataset(X_df, label=Y)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse'\n",
        "}\n",
        "\n",
        "# モデルのトレーニング\n",
        "model = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# SHAP値の計算\n",
        "explainer = shap.Explainer(model, X_df)\n",
        "shap_values = explainer(X_df)\n",
        "\n",
        "# イベント結果のプロット (SHAP値のサマリープロット)\n",
        "shap.summary_plot(shap_values, X_df)\n",
        "\n",
        "# プロットの生成 (1つの特徴量のSHAP値の例)\n",
        "shap.dependence_plot('feat_0', shap_values.values, X_df)\n",
        "\n",
        "# 終了\n",
        "plt.show()  # プロットを表示します\n",
        "\n",
        "# このコードは、LightGBMを使用してモデルをトレーニングし、SHAPライブラリを使用してSHAP値を計算およびプロットします。\n",
        "# 各プロットは、特徴量がモデルの予測に与える影響を視覚的に解釈するための手段として利用できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc79eab",
      "metadata": {
        "id": "dcc79eab"
      },
      "source": [
        "### ハイパーパラメータの最適化\n",
        "\n",
        "ハイパーパラメータの最適化とは、機械学習モデルのパフォーマンスを最大化するために、設定可能なパラメータ（ハイパーパラメータ）の最適な組み合わせを見つけるプロセスです。ハイパーパラメータは、モデル構築前に設定する必要がありますが、その設定はモデルの予測性能に大きな影響を与えます。\n",
        "\n",
        "#### 理論\n",
        "\n",
        "多くの機械学習アルゴリズムには複数のハイパーパラメータが存在し、それらはモデルの学習能力や予測精度を調整します。ハイパーパラメータの探索空間が大きい場合、単純に試行錯誤で最適化するのは効率が悪いため、ベイズ最適化やグリッドサーチ、ランダムサーチなどの手法が使われます。\n",
        "\n",
        "インライン数式の例として、ある目的関数 $f(x)$ を最大化または最小化する問題がハイパーパラメータの最適化と考えることができます。\n",
        "\n",
        "ブロック数式で表すと、目的は以下のように定式化されます。\n",
        "\n",
        "$$\n",
        "\\arg\\max_{\\theta} f(\\theta) \\quad \\text{または} \\quad \\arg\\min_{\\theta} f(\\theta)\n",
        "$$\n",
        "\n",
        "ここで、$\\theta$ は最適化の対象であるハイパーパラメータの集合を表します。\n",
        "\n",
        "#### LightGBMとの関係性\n",
        "\n",
        "LightGBMは非常に高性能な勾配ブースティングライブラリであり、多くのハイパーパラメータを持っています。これらのパラメータを適切に調整することで、モデルの予測精度を向上させることが可能です。LightGBMでは、新しい特徴としてHyperoptやOptunaを使った自動ハイパーパラメータ最適化がサポートされています。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "ハイパーパラメータ最適化は、機械学習モデルの性能を向上させるための重要なステップです。特に、LightGBMなどの強力なツールを用いることで、競争が激しいコンペティションや実務での予測精度を向上させることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fb170b",
      "metadata": {
        "id": "d6fb170b"
      },
      "outputs": [],
      "source": [
        "# ハイパーパラメータの最適化の例として、Optunaを使用したLightGBMモデルの最適化を示します。\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# データを準備します（ここでは例としてのデータです）\n",
        "X, y = load_your_data()  # ユーザーが自身のデータをロードする関数\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Optunaの目的関数を定義します\n",
        "def objective(trial):\n",
        "    # 試行ごとに選択できるハイパーパラメータを指定\n",
        "    param = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'n_jobs': -1,\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
        "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
        "    }\n",
        "\n",
        "    # LightGBMデータセットを作成\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dvalid = lgb.Dataset(X_valid, label=y_valid, reference=dtrain)\n",
        "\n",
        "    # モデルを学習\n",
        "    gbm = lgb.train(param, dtrain, valid_sets=[dvalid], early_stopping_rounds=100, verbose_eval=False)\n",
        "\n",
        "    # 検証セットでモデルを評価\n",
        "    preds = gbm.predict(X_valid)\n",
        "    rmse = mean_squared_error(y_valid, preds, squared=False)\n",
        "    return rmse\n",
        "\n",
        "# Optunaでハイパーパラメータを最適化\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最良のハイパーパラメータを出力\n",
        "print('Best parameters: ', study.best_params)\n",
        "print('Best RMSE: ', study.best_value)\n",
        "\n",
        "# ノート：このコードはロードするデータが必要です。load_your_data()を適切なデータ読み込みコードに置き換えてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d4243f",
      "metadata": {
        "id": "d3d4243f"
      },
      "source": [
        "予測確率のキャリブレーション（Calibration of Predictive Probabilities）は、機械学習モデルが出力する予測確率を実際の確率に一致させるためのプロセスです。キャリブレーションが良好なモデルでは、予測確率が0.8であるケースがあれば、80%のケースで実際に正解します。例えば、ロジスティック回帰やLightGBMのようなモデルは、時として過信や過小評価に繋がる非適切な確率を出力する可能性があります。\n",
        "\n",
        "確率のキャリブレーションの理論的な背景としては、Brierスコアという評価指標があります。Brierスコアは、モデルの予測確率と実際の結果の差の2乗の平均で定義されます。数式は次の通りです：\n",
        "\n",
        "$$ Brier = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{p}_i - y_i)^2 $$\n",
        "\n",
        "ここで、$N$はサンプルの総数、$\\hat{p}_i$は予測確率、$y_i$は実際のラベル（0または1）です。\n",
        "\n",
        "キャリブレーションの手法としては、Platt ScalingやIsotonic Regressionなどがあります。通常、これらの手法はモデルの後段に設置され、予測確率を調整します。\n",
        "\n",
        "LightGBMは勾配ブースティングモデルの一つで、通常は分類問題で予測確率を出力します。キャリブレーションを使用することにより、LightGBMが出力する確率を実際の確率に近づけることが可能です。これにより、確率に基づく意思決定の精度が向上します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970ed444",
      "metadata": {
        "id": "970ed444"
      },
      "outputs": [],
      "source": [
        "# Pythonで確率のキャリブレーションを行うコード例です。\n",
        "# ライブラリを読み込みます。\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "# データセットを生成します。\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# データを学習用とテスト用に分割します。\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ランダムフォレストモデルをインスタンス化します。\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# モデルを学習します。\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# キャリブレーションを行わない場合の予測確率を計算します。\n",
        "uncalibrated_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# キャリブレーションを行わない場合のBrierスコアを計算します。\n",
        "uncalibrated_brier = brier_score_loss(y_test, uncalibrated_probs)\n",
        "\n",
        "# CalibratedClassifierCVを用いてモデルのキャリブレーションを行います。\n",
        "calibrated_rf = CalibratedClassifierCV(rf, method='isotonic', cv='prefit')\n",
        "calibrated_rf.fit(X_train, y_train)\n",
        "\n",
        "# キャリブレーション後の予測確率を計算します。\n",
        "calibrated_probs = calibrated_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# キャリブレーション後のBrierスコアを計算します。\n",
        "calibrated_brier = brier_score_loss(y_test, calibrated_probs)\n",
        "\n",
        "# キャリブレーション前後のBrierスコアを出力します。\n",
        "print(f'Uncalibrated Brier Score: {uncalibrated_brier:.4f}')\n",
        "print(f'Calibrated Brier Score: {calibrated_brier:.4f}')\n",
        "\n",
        "# プログラムの実行により、キャリブレーション後のBrierスコアが改善していることが確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5ff0ea",
      "metadata": {
        "id": "eb5ff0ea"
      },
      "source": [
        "分布適応型のバギング（Balanced Bagging）は、データのクラス不均衡問題に対処するために使用される手法です。標準的なバギング手法では、各ブートストラップサンプルが同一のデータ分布に従いますが、分布適応型バギングでは、各クラスが均等にサンプリングされるようにデータが再サンプリングされます。\n",
        "\n",
        "理論的には、これは過少サンプルされているクラスのデータを多くサンプリングすることで、モデルがすべてのクラスに対してバランスの取れた予測を行うための助けとなります。分布適応型バギングが目的とするのは、不均衡なデータセットにおいても、少数派クラスを適切に予測できるようにすることです。\n",
        "\n",
        "LightGBMとの関係性としては、LightGBMは勾配ブースティング決定木（GBDT）を用いたライブラリであり、分類や回帰の問題に対して高いパフォーマンスを発揮します。LightGBMでもクラス不均衡に対処するためにBalanced Baggingが活用できます。具体的には、'is_unbalance' オプションや'scale_pos_weight'を調整することで、内部的にクラスのバランスを取るようなモデルを作ることができます。\n",
        "\n",
        "使用用途としては、医療データや金融の不正検出、欠陥検出など、少数派クラスを如何に正確に予測するかが重要になる分野で有用です。\n",
        "\n",
        "数式としてのサンプルウェイトは以下で示されます：\n",
        "\n",
        "$$W_i = \\frac{1}{n} \\times \\frac{N}{N_i}$$\n",
        "\n",
        "ここで、$W_i$はサンプリング時に負荷する重みです。$N$はデータセット全体のサイズ、$N_i$はクラス$i$のサイズ、$n$は全クラス数です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a76945",
      "metadata": {
        "id": "01a76945"
      },
      "outputs": [],
      "source": [
        "# 分布適応型のバギングを使用したデータセットの生成とLightGBMによるモデルの訓練\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 不均衡データの作成\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "                           weights=[0.1, 0.9], n_informative=3,\n",
        "                           n_redundant=1, flip_y=0,\n",
        "                           n_features=20, n_clusters_per_class=1,\n",
        "                           n_samples=1000, random_state=10)\n",
        "\n",
        "# データセットを訓練用とテスト用に分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# LightGBMモデルをBalanced Baggingを用いて訓練\n",
        "bbc = BalancedBaggingClassifier(base_estimator=LGBMClassifier(),\n",
        "                                sampling_strategy='auto',\n",
        "                                replacement=False,\n",
        "                                random_state=42)\n",
        "\n",
        "bbc.fit(X_train, y_train)\n",
        "\n",
        "# テストデータを用いて予測\n",
        "y_pred = bbc.predict(X_test)\n",
        "\n",
        "# 予測結果を評価\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# このコードは、BalancedBaggingClassifierを用いて、\n",
        "# 分布適応型バギングを行いながらLightGBMモデルを訓練しています。\n",
        "# 'make_classification' 関数で不均衡データを生成し、\n",
        "# 'BalancedBaggingClassifier' がそれを均等化しました。\n",
        "# 最後に、モデルのパフォーマンスを評価するためにclassification_reportを使用しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e75640",
      "metadata": {
        "id": "87e75640"
      },
      "source": [
        "### スパースデータの処理に関する解説\n",
        "\n",
        "スパースデータとは、データセット内の多くの要素がゼロであり、非ゼロ要素がごくわずかであるようなデータを指します。スパースデータは、特に自然言語処理やレコメンデーションシステムなどで頻繁に使用されます。LightGBMはスパースデータを効率的に処理する機能を備えており、多くのゼロを含むデータセットに対しても優れた性能を発揮します。\n",
        "\n",
        "#### 理論と数式\n",
        "\n",
        "スパースデータの効率的な処理のために、LightGBMは以下の手法を用いています。\n",
        "\n",
        "1. **スパースアウェアスプリッティング**:\n",
        "LightGBMはスプリット時にゼロ値を特別に扱うことで、計算効率を高めています。この手法により、ゼロ値が多い列でも効率よく処理が可能です。\n",
        "\n",
        "2. **ヒストグラムベースのアプローチ**:\n",
        "LightGBMはデータをいくつかのビンに分けて処理し、ヒストグラムカウントを使用して効率的にスコアを計算します。この手法は、スパースデータだけでなく、密データにも有効です。\n",
        "\n",
        "数式としては、スパースデータにおいて特に重要なのは、ヒストグラムベースの計算です。通常の計算では、\n",
        "$$ G_i = \\sum_{j=1}^{n} g_j $$\n",
        "$$ H_i = \\sum_{j=1}^{n} h_j $$\n",
        "が計算されますが、LightGBMではこれを非ゼロ要素に対してのみ計算し、ゼロ要素については追加の高速化技術を用います。\n",
        "\n",
        "#### 使用用途\n",
        "\n",
        "スパースデータの処理は、以下のような用途で非常に有効です。\n",
        "\n",
        "- **テキストデータの処理**: 単語を特徴量として使用する際には、単語の出現頻度は非常に低くなるため、スパースデータとして扱うことが多いです。\n",
        "- **レコメンデーションシステム**: ユーザーが多くのアイテムに対して評価を行う際、実際に評価を行うアイテムはごく僅かであるため、スパースデータであることが一般的です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70459e3",
      "metadata": {
        "id": "e70459e3"
      },
      "outputs": [],
      "source": [
        "# スパースデータの処理におけるPythonコード例\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "\n",
        "# サンプルデータの作成（スパース行列）\n",
        "data = scipy.sparse.csr_matrix([\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 2, 0, 0],\n",
        "    [3, 0, 0, 4]\n",
        "])\n",
        "\n",
        "# ターゲットラベル\n",
        "target = np.array([1, 0, 1])\n",
        "\n",
        "# LightGBMのデータセットを作成する際にスパースデータをそのまま使用できます。\n",
        "d_train = lgb.Dataset(data, label=target)\n",
        "\n",
        "# パラメータの設定\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    # 二値分類目的\n",
        "    'boosting_type': 'gbdt',\n",
        "    # gradient boosting決定木\n",
        "    'metric': 'binary_logloss',\n",
        "    # 損失関数としてbinary loglossを使用\n",
        "}\n",
        "\n",
        "# モデルのトレーニング\n",
        "bst = lgb.train(params, d_train, num_boost_round=10)\n",
        "\n",
        "# モデルを使用して新しいデータを予測\n",
        "# ここでは新しいスパースデータを予測する例です。\n",
        "new_data = scipy.sparse.csr_matrix([\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 1, 1, 0]\n",
        "])\n",
        "\n",
        "preds = bst.predict(new_data)\n",
        "print(\"Predictions: \", preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1189a8c4",
      "metadata": {
        "id": "1189a8c4"
      },
      "source": [
        "学習曲線（Learning Curve）は、機械学習モデルの学習プロセスを評価するためのグラフです。このグラフは一般的に、トレーニングデータに対するモデルのパフォーマンスと、検証データに対するパフォーマンスを比較し、データのサイズまたはイテレーションの数に対してプロットされます。これにより、オーバーフィッティングやアンダーフィッティングの存在を特定することができます。たとえば、トレーニングデータ上でのエラーが低く、検証データ上でのエラーが高い場合、モデルはオーバーフィッティングしている可能性があります。学習曲線は、ハイパーパラメータのチューニングやモデルの改善に役立ちます。LightGBM（Light Gradient Boosting Machine）は、スケーラブルな勾配ブースティングフレームワークであり、その学習曲線を描画することで、モデルの適切さを視覚的に評価できます。\n",
        "\n",
        "理論的には、学習曲線は次のような形式で表現されます:\n",
        "インライン数式: $ P_{train}(m) $ および $ P_{val}(m) $\n",
        "\n",
        "ブロック数式:\n",
        "$$\n",
        "P_{train}(m) = \\frac{1}{m} \\sum_{i=1}^m L(y_i, f(x_i)) \\\\\n",
        "P_{val}(m) = \\frac{1}{n} \\sum_{j=1}^n L(y_j, f(x_j))\n",
        "$$\n",
        "ここで、$P_{train}(m)$はトレーニングデータセットの平均損失、$P_{val}(m)$は検証データセットの平均損失です。$m$と$n$はそれぞれトレーニングと検証データのサンプル数です。$L$は損失関数であり、一般的には平均二乗誤差 (MSE) やクロスエントロピーなどが使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065faf1a",
      "metadata": {
        "id": "065faf1a"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポートします\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# データセットを生成します\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# データセットをトレーニングとテストに分割します\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# モデルを定義します\n",
        "model = LGBMClassifier(random_state=42)\n",
        "\n",
        "# 学習曲線を計算します\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model, X_train, y_train, cv=5, scoring='accuracy',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# トレーニングとテストの平均スコアを計算します\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "# 学習曲線をプロットします\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training score', color='r')\n",
        "plt.plot(train_sizes, test_scores_mean, label='Cross-validation score', color='g')\n",
        "\n",
        "# グラフのラベルを設定します\n",
        "plt.xlabel('Training size')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# このコードでは、sklearnのlearning_curve関数を用いて学習曲線を計算し、\n",
        "# LightGBMの分類器モデルでのトレーニングスコアと交差確認スコアをプロットしています。\n",
        "# トレーニングデータのサイズに対して、トレーニングとテストのスコアを可視化し、モデルの性能を評価します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b8c81e",
      "metadata": {
        "id": "e6b8c81e"
      },
      "source": [
        "適応的学習率調整(Adaptive Learning Rate)は、機械学習の最適化アルゴリズムであり、各重みの更新の度に学習率を調整する手法です。これは、勾配が緩やかになった領域では大きなステップを踏み、勾配が急な領域では小さなステップを踏むことで最適化を加速化します。これにより、学習の効率化や収束速度の向上を図ることができます。一般的な適応学習率手法には、AdaGrad、RMSprop、Adamなどがあります。\n",
        "\n",
        "LightGBMにおいても適応的学習率調整を利用することができます。LightGBMの主なアルゴリズム自体は決定木の集合学習ですが、その最適化ステップにおいて勾配降下法が応用されています。このとき、適応学習率調整を導入することで、学習の安定性や精度が向上する場合があります。\n",
        "\n",
        "例えば、AdaGradは学習率\\( \\eta \\)を以下のように定義します：\n",
        "$$ \\eta_t = \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} $$\n",
        "ここで、\\( G_t \\)は過去のすべての勾配の二乗和であり、\\( \\epsilon \\)はゼロ割りの防止のための微小定数です。\n",
        "\n",
        "適応的学習率調整の主な使用用途は、勾配降下法に基づく最適化アルゴリズムにおいて収束を早めることで、モデルの訓練をより効率的に行うことにあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8eaaf57",
      "metadata": {
        "id": "f8eaaf57"
      },
      "outputs": [],
      "source": [
        "# Pythonで適応的学習率用のサンプルコードを示します。\n",
        "import numpy as np\n",
        "\n",
        "# 初期学習率\n",
        "eta = 0.01\n",
        "# ゼロ割り回避の微小定数\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 仮の勾配計算のサンプル関数\n",
        "def compute_gradient(x):\n",
        "    return 2 * x  # 単純な勾配として2xを使用\n",
        "\n",
        "# AdaGradの適応的学習率\n",
        "class AdaGrad:\n",
        "    def __init__(self, eta=0.01, epsilon=1e-8):\n",
        "        self.eta = eta\n",
        "        self.epsilon = epsilon\n",
        "        self.grad_square_sum = 0\n",
        "\n",
        "    def update(self, gradient):\n",
        "        self.grad_square_sum += gradient ** 2\n",
        "        adaptive_lr = self.eta / (np.sqrt(self.grad_square_sum) + self.epsilon)\n",
        "        return adaptive_lr\n",
        "\n",
        "# パラメータの初期値\n",
        "x = 10\n",
        "adagrad = AdaGrad(eta, epsilon)\n",
        "\n",
        "# 10回の更新ステップを示します。\n",
        "for i in range(10):\n",
        "    grad = compute_gradient(x)\n",
        "    learning_rate = adagrad.update(grad)\n",
        "    x -= learning_rate * grad  # パラメータの更新\n",
        "    print(f'Step {i+1}: x = {x}, learning_rate = {learning_rate}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ac141a",
      "metadata": {
        "id": "a0ac141a"
      },
      "source": [
        "対数尤度は、統計モデルや機械学習モデルのパラメータを最適化する際に使用される重要な概念です。尤度は、あるデータが観測される確率を表します。一方で、対数尤度はその自然対数を取ったもので、数値計算での安定性と容易さからモデルの訓練時に多用されます。\n",
        "\n",
        "LightGBMのような勾配ブースティングフレームワークでは、各イテレーションで決定木を構築し、それを用いて目的関数を最小化します。この際、対数尤度が損失関数として使用されることが一般的です。特に、分類タスクではロジスティック回帰モデルに基づく対数尤度が利用されることがあります。\n",
        "\n",
        "対数尤度の数式は以下のように表されます。観測データ$x_{1}, x_{2}, ..., x_{n}$と、モデルによって生成される確率 $p(x)$を用いると、\n",
        "\n",
        "インライン数式としては、$\\log L(\\theta) = \\sum_{i=1}^{n} \\log p(x_i|\\theta)$ となります。\n",
        "\n",
        "ブロック数式としては、\n",
        "$$\n",
        "\\log L(\\theta) = \\sum_{i=1}^{n} \\log p(x_i|\\theta)\n",
        "$$\n",
        "です。\n",
        "\n",
        "この数式を用いて得た対数尤度を最大化することにより、モデルのパラメータ$\\theta$の最適化を行います。\n",
        "\n",
        "使用用途としては、モデルのフィット度を評価したり、モデル選択やハイパーパラメータの最適化に利用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad852d9",
      "metadata": {
        "id": "dad852d9"
      },
      "outputs": [],
      "source": [
        "# Pythonで対数尤度を計算する例\n",
        "import numpy as np\n",
        "\n",
        "# 簡単な例として、正規分布に従うデータを仮定する\n",
        "# 平均mu、標準偏差sigmaの正規分布とする\n",
        "mu = 0\n",
        "sigma = 1\n",
        "\n",
        "# 対数尤度を計算する関数\n",
        "def log_likelihood(data, mu, sigma):\n",
        "    # データの長さを取得\n",
        "    n = len(data)\n",
        "\n",
        "    # 正規分布の確率密度関数を用いて対数尤度を計算\n",
        "    # np.logで自然対数を取る\n",
        "    likelihood = -n/2 * np.log(2 * np.pi * sigma**2) - np.sum((data - mu)**2) / (2 * sigma**2)\n",
        "    return likelihood\n",
        "\n",
        "# サンプルデータを生成\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(mu, sigma, 100)\n",
        "\n",
        "# 対数尤度を計算\n",
        "log_likelihood_value = log_likelihood(data, mu, sigma)\n",
        "print(\"Log-Likelihood:\", log_likelihood_value)\n",
        "\n",
        "# このコードでは、与えられたデータが指定されたパラメータでの正規分布にどれだけ適合しているかを評価する\n",
        "# 対数尤度を用いると、この適合度を数値化できる"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b00869",
      "metadata": {
        "id": "e2b00869"
      },
      "source": [
        "勾配ブースティング決定木 (GBDT) は、予測モデルを構築するための強力なensemble学習手法です。この手法は回帰や分類問題に多く使われており、特に非線形性の高いデータに対して有効です。GBDTは、複数の決定木を段階的に追加し、それぞれの決定木を訓練することで前のモデルの誤差を補正します。\n",
        "\n",
        "GBDTの理論において、モデルは次のように更新されます。予測モデル \\( F_{m}(x) \\) は、前のモデル \\( F_{m-1}(x) \\) に新しい決定木 \\( h_m(x) \\) を加えることにより構築されます：\n",
        "\n",
        "$$ F_{m}(x) = F_{m-1}(x) +\n",
        "u \\cdot h_m(x) $$\n",
        "\n",
        "ここで、\\(\n",
        "u \\) は学習率（通常は0から1の間）で、モデルの更新をゆっくりと行うために用いられます。\n",
        "\n",
        "LightGBMは、GBDTの実装の一つで、特に大規模なデータセットに対する効率性を高めたものです。他のGBDT実装と比べて、メモリ使用量が少なく、高速に学習を進めることができます。これは主に、葉の成長をベースにした学習（leaf-wise growth）やHistogram-Based Learningを使ったテクニックによって実現されています。\n",
        "\n",
        "GBDTの使用用途としては、チャーン予測、詐欺検出、個人情報のレコメンデーションのようなフィールドで幅広く利用されており、その高精度な性能によって、しばしば他のモデルを凌ぐことがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2583df6",
      "metadata": {
        "id": "b2583df6"
      },
      "outputs": [],
      "source": [
        "# GBDTの手作り実装のPythonコード\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GBDT:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # 初期モデルF_0(x)は目標変数の平均値\n",
        "        F_m = np.full(y.shape, np.mean(y))\n",
        "        self.F_0 = F_m.copy()\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # 残差を計算\n",
        "            residual = y - F_m\n",
        "            # 決定木による学習\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residual)\n",
        "            self.trees.append(tree)\n",
        "            # モデルの更新\n",
        "            F_m += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # 初期モデルからスタート\n",
        "        F_m = np.full(X.shape[0], self.F_0)\n",
        "        # 各木からの予測を加算\n",
        "        for tree in self.trees:\n",
        "            F_m += self.learning_rate * tree.predict(X)\n",
        "        return F_m\n",
        "\n",
        "# データセット例\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 2, 1.5, 3.5, 2])\n",
        "\n",
        "# モデルの学習\n",
        "gbdt = GBDT(n_estimators=10, learning_rate=0.1, max_depth=2)\n",
        "gbdt.fit(X, y)\n",
        "\n",
        "# 予測\n",
        "predictions = gbdt.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# この例では、非常にシンプルなGBDTの手作り実装を示しています。\n",
        "# 実際のデータに対しては、より多くのエポックや深さを増やして汎化性能を高めることができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d859896",
      "metadata": {
        "id": "5d859896"
      },
      "source": [
        "推論速度の最適化とは、機械学習モデルの推論（予測）をより速く実行するための手法を指します。特にLightGBMは、その軽量さと高速さから広く使用されていますが、さらに推論速度を向上させるための最適化技法が存在します。\\n\\n一般的に、推論速度の最適化は以下の要素を考慮します：\\n\\n1. **モデルのサイズと複雑性の削減**: モデルをシンプルにし、木の深さを浅くすることで、各サンプルの計算時間を減少させます。\\n2. **特徴量の選択**: 不必要な特徴量を削除し、モデルの計算負荷を減らします。\\n3. **前処理の効率化**: データの前処理の段階で行列演算や整数計算を工夫し、計算量を最小化します。\\n\\n具体的に、LightGBMでは次のような最適化が使われます：\\n- **並列計算**: スレッド間の計算を並列化することで速度を向上させます。\\n- **HistTreeの使用**: ヒストグラムベースの決定木構築法を使用することで、より速いビン分割を実現します。\\n- **Early Stopping**: 適切なエポックで学習を停止することで、過学習を防げ、さらなる検証なしでの予測が高速化されます。\\n\\n\\(\n",
        " \\text{時間複雑度をO(n)}\n",
        " \\) で計算される処理を工夫することで、より高速化を図ることができます。\n",
        " \\( n \\) はデータポイントの数とします。 \\n\\n推論速度の最適化は特にリアルタイムアプリケーションや、大規模データを扱うシステムにおいて、その効果を発揮します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f5cd14",
      "metadata": {
        "id": "97f5cd14"
      },
      "outputs": [],
      "source": [
        "# 推論速度の最適化を行うPythonコードの例\\n\\nfrom lightgbm import LGBMClassifier\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\n\\n# ダミーデータを作成します\\nX, y = make_classification(n_samples=10000, n_features=20, random_state=42)\\n\\n# データセットをトレーニング用とテスト用に分割します\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# モデルを作成し、推論速度の最適化を行います\\nmodel = LGBMClassifier(n_estimators=100, max_depth=3, n_jobs=-1, device='cpu')\\n\\n# モデルを学習させます\\nmodel.fit(X_train, y_train)\\n\\n# 推論速度の確認\\nimport time\\nstart_time = time.time()\\npredictions = model.predict(X_test)\\nend_time = time.time()\\n\\nprint(f\"推論にかかった時間: {end_time - start_time:.4f} 秒\")\\n\\n# 上記のコードでは、max_depthを3に設定し、過剰に複雑でないモデルを生成することにより推論速度を最適化しています。また、n_jobsを-1に設定したことで可能な限り多くのスレッドを使用し並列化を進めています。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1885a662",
      "metadata": {
        "id": "1885a662"
      },
      "source": [
        "LightGBMは、強力な勾配ブースティングフレームワークとして知られています。クラスタリングと組み合わせることで、データの前処理や特徴量エンジニアリングを効果的に行うことができます。例えば、大規模なデータセットをクラスタリング手法を用いてグループに分け、各グループに対してLightGBMモデルを適用することで、より精度の高い予測を実現することが可能です。インライン数式としては、距離を測るためのユークリッド距離が使用され、これは $d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$ と表されます。ブロック数式として、クラスタ中心の更新式は次のようになります：\n",
        "\n",
        "$$C_i = \\frac{1}{|C_i|} \\sum_{x_j \\in C_i} x_j$$\n",
        "\n",
        "この式は、クラスタ内の全データ点の重心を計算します。これにより、クラスタリングによってデータの構造を理解し、それを強化学習アルゴリズムの初期段階として利用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79ef85a",
      "metadata": {
        "id": "a79ef85a"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインポート\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import lightgbm as lgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# クラスタリング用データ生成（2クラス分類問題）\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# K-Meansクラスタリングを適用\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "# クラスタリング結果を特徴量に追加\n",
        "X_with_clusters = np.hstack((X, clusters.reshape(-1, 1)))\n",
        "\n",
        "# データセットの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_with_clusters, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM用データセットに変換\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "# ハイパーパラメータ設定\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'binary_logloss',\n",
        "    'max_depth': -1,\n",
        "    'learning_rate': 0.1,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "# モデルのトレーニング\n",
        "bst = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# テストデータでの予測\n",
        "predictions = bst.predict(X_test)\n",
        "\n",
        "# 予測結果の表示（0.5を閾値にして二値化）\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "print(\"予測結果:\", predictions_binary)\n",
        "\n",
        "# このコードでは、scikit-learnのKMeansによるクラスタリングとLightGBMによるモデル\n",
        "# トレーニングを組み合わせています。クラスタリングの結果を特徴量として追加し、\n",
        "# モデルの性能向上を目指します。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}